{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e3e393",
   "metadata": {},
   "outputs": [],
   "source": [
    "###ORLANDO ALFONSO BENAVIDES PEREZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "715715d6-6ca4-4395-8459-5675609c3172",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Se importan las librerias\n",
    "import pandas as pd\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import timedelta\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "from sagemaker.inputs import TrainingInput\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e5ea66a-3d24-4349-870e-6a7bed50e4d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: 1.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    }
   ],
   "source": [
    "#Se inicializan los bucket\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "image_name = sagemaker.image_uris.retrieve(\"forecasting-deepar\", region)\n",
    "role='LabRole'\n",
    "s3_bucket = 'dollar-predictor274'\n",
    "s3_data_path = \"s3://{}/{}/data\".format(s3_bucket, 'deepar-model')\n",
    "s3_output_path = \"s3://{}/{}/output\".format(s3_bucket, 'deepar-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e244e93-1213-48bd-af38-6c61c34417aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Se carga el dataset\n",
    "df = pd.read_csv('df_dollar_10min.csv', parse_dates=['fecha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c5a5b10-d400-4300-88e2-1301cab8a7da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Se divididen los datos  del dataset en datos de entrenamiento y datos de test\n",
    "train_size = math.ceil(len(df) * 0.7)\n",
    "train_data = df[:train_size]\n",
    "test_data = df[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "95ba192f-79d9-4e8b-9164-5407ea099a6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "freq = '10min' # La serie tiene registros cada 10 min\n",
    "prediction_length = 1 # se predice para el siguiete periodo\n",
    "context_length = 12 # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "67733b03-4bc2-4192-abb7-6aef47d45ae4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18342/1049242397.py:1: FutureWarning: The 'freq' argument in Timestamp is deprecated and will be removed in a future version.\n",
      "  start_dataset = pd.Timestamp(\"2023-02-17 13:15:00\", freq=freq)\n",
      "/tmp/ipykernel_18342/1049242397.py:2: FutureWarning: The 'freq' argument in Timestamp is deprecated and will be removed in a future version.\n",
      "  end_training = pd.Timestamp(\"2023-03-22 17:30:00\", freq=freq)\n"
     ]
    }
   ],
   "source": [
    "start_dataset = pd.Timestamp(\"2023-02-17 13:15:00\", freq=freq)\n",
    "end_training = pd.Timestamp(\"2023-03-22 17:30:00\", freq=freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "70dd0f07-c228-4144-8c9a-b7238cbb7e34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_daily_groups_train_data = train_data.groupby(pd.Grouper(key='fecha', freq='D'))\n",
    "df_daily_groups_test_data = test_data.groupby(pd.Grouper(key='fecha', freq='D'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4925fa36-fa34-4bc3-8e83-2ef6902e434e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'start': '2023-02-17 13:10:00', 'target': [4946.8202, 4942.746043137256, 4937.931349152543, 4933.719009876543, 4931.0680764705885, 4931.236175, 4931.441344186046, 4932.41604, 4934.126216541354, 4934.819030555555, 4935.328091489361, 4934.921802380953, 4933.430054347826, 4931.38086, 4930.012629166667, 4928.255918181818, 4926.226331372549, 4924.88836, 4923.76967027027, 4922.9587833333335, 4922.482694444445, 4922.013855, 4921.494372222222, 4921.098806060606, 4920.75665]}, {'start': '2023-02-20 13:10:00', 'target': [4890.10752, 4892.943850000001, 4894.356809999999, 4898.778865517242, 4904.442151162791, 4908.378322222223, 4909.985377142857, 4911.573092307693, 4912.019033333333, 4912.238271428571, 4912.2645, 4912.2906, 4912.328875, 4912.37695, 4913.6919, 4913.73885, 4913.7861, 4913.925657142858, 4913.982333333333, 4914.142630769231, 4914.341209090909, 4914.4217, 4914.4267, 4914.4365, 4914.4387]}, {'start': '2023-02-21 13:10:00', 'target': [4919.366678571429, 4918.425358208955, 4922.934089130435, 4924.611131944444, 4927.972508333333, 4929.548433333333, 4930.3082, 4931.258598360656, 4931.698103921568, 4931.673280645161, 4933.024925, 4934.499493333334, 4935.434967241379, 4937.023423913044, 4937.872203389831, 4939.085456818182, 4940.221181578947, 4941.034526923077, 4942.105876190476, 4943.144959574468, 4944.069106122449, 4944.984039189189, 4946.182274074074, 4947.263340789474, 4947.963220833333, 4948.57458627451, 4949.121602631579, 4949.6221, 4949.970498181819]}, {'start': '2023-02-22 13:30:00', 'target': [4946.371267857143, 4943.401388235295, 4941.788611578948, 4939.668252631579, 4938.914001219512, 4938.041555056179, 4935.891584482759, 4933.682982608695, 4932.790420930233, 4932.379665, 4932.113713793104, 4931.730696153846, 4931.274614285714, 4930.762846067416, 4930.558549425287, 4930.228266101694, 4929.813754385966, 4929.626577999999, 4929.489023529412, 4929.374787951807, 4929.238944117647, 4928.834133962264, 4928.254313846153, 4927.71667, 4926.723563829787, 4925.822715217391, 4925.1856777777775, 4924.7666]}, {'start': '2023-02-23 13:00:00', 'target': [4904.133275, 4886.762972340425, 4878.96992972973, 4878.492119607843, 4878.279079545455, 4876.520833333333, 4872.300691489362, 4867.51221875, 4864.5914454545455, 4862.246543859649, 4860.297763414634, 4858.022093220338, 4856.34790967742, 4855.556100000001, 4854.89365, 4854.335795833334, 4854.216987755102, 4854.3296641025645, 4853.988536000001, 4853.962693548387, 4854.166145454546, 4854.360657142857, 4854.4303875, 4854.379956000001, 4854.375484, 4854.278819354839, 4854.004770833333, 4853.844272, 4853.898772222222, 4853.972535714286, 4853.9941]}]\n"
     ]
    }
   ],
   "source": [
    "training_data = [\n",
    "    {\n",
    "        \"start\": str(group.iloc[0]['fecha']),\n",
    "        \"target\": group['precio'].values.tolist(),\n",
    "    }\n",
    "    for name, group in df_daily_groups_train_data if len(group) > 0\n",
    "]\n",
    "print(training_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "71350b97-a62d-472f-a082-94c118779e2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'start': '2023-03-22 13:00:00', 'target': [4797.672655, 4796.560152173913, 4787.514345454546, 4784.140872093023, 4783.463295454546, 4781.217283333333, 4779.098413636363, 4777.875138888889, 4776.260331111111, 4774.896042, 4774.018496774193, 4773.509864705883, 4773.56224, 4774.0225, 4774.0232166666665, 4773.910484615385, 4773.944192857142, 4774.167964285714, 4774.378694871795, 4774.806668888888, 4775.166169230769, 4775.359740625, 4775.570693181819, 4775.654693333333, 4775.740647826087, 4775.853919148936, 4775.998655, 4776.132548, 4776.124059090909, 4776.21652972973, 4776.2492]}, {'start': '2023-03-23 13:00:00', 'target': [4764.457161290322, 4763.255968115942, 4761.08185730337, 4758.80404494382, 4757.429940983607, 4756.668765306123, 4756.584083333333, 4757.036705555556, 4757.1301875, 4757.3571125, 4757.8938777777785, 4758.14598490566, 4758.319503125, 4758.552785, 4758.865178947369, 4758.891453571428, 4758.531924, 4758.446760606061, 4758.310716666667, 4757.559637864078, 4756.936239215686, 4756.543766666667, 4756.042917543859, 4755.658164102564, 4755.572773684211, 4755.568461538462, 4755.740675510205, 4756.026557317073, 4755.915122115384, 4755.362242168675, 4755.0706]}, {'start': '2023-03-27 13:00:00', 'target': [4711.900004545454, 4711.277950769231, 4703.588514736842, 4699.222055238095, 4697.2303833333335, 4695.7808125, 4694.3454, 4693.7085777777775, 4693.639316666667, 4693.608910344827, 4693.471287096774, 4693.3412285714285, 4693.183338297872, 4692.51095, 4691.713002222222, 4690.338265079365, 4689.230082, 4688.753063333334, 4688.37335, 4688.009997727273, 4687.657099999999, 4687.492457142857, 4687.355627586207, 4687.250590476191, 4687.0774369565215, 4687.065918181818, 4687.071217647059, 4687.094721212122, 4687.120247368422, 4686.972829545454]}]\n"
     ]
    }
   ],
   "source": [
    "testing_data = [\n",
    "    {\n",
    "        \"start\": str(group.iloc[0]['fecha']),\n",
    "        \"target\": group['precio'].values.tolist(),\n",
    "    }\n",
    "    for name, group in df_daily_groups_test_data if len(group) > 0\n",
    "]\n",
    "print(testing_data[5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "40b87417-7389-4866-8031-325e55c529f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Guardo el archivo json para ser usado luego en el entranamiento\n",
    "def write_dicts_to_file(path, data):\n",
    "    with open(path, \"wb\") as fp:\n",
    "        for d in data:\n",
    "            fp.write(json.dumps(d).encode(\"utf-8\"))\n",
    "            fp.write(\"\\n\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c8fbf8bb-0d4c-4564-b682-cd447cbb6a61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 2.54 ms, total: 2.54 ms\n",
      "Wall time: 2.89 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "write_dicts_to_file(\"train.json\", training_data)\n",
    "write_dicts_to_file(\"test.json\", testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3de1a53e-89fc-44bf-9c63-75e8c16805f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3 = boto3.resource(\"s3\")\n",
    "\n",
    "def copy_to_s3(local_file, s3_path, override=True):\n",
    "    assert s3_path.startswith(\"s3://\")\n",
    "    split = s3_path.split(\"/\")\n",
    "    bucket = split[2]\n",
    "    path = \"/\".join(split[3:])\n",
    "    buk = s3.Bucket(bucket)\n",
    "\n",
    "    if len(list(buk.objects.filter(Prefix=path))) > 0:\n",
    "        if not override:\n",
    "            print(\n",
    "                \"File s3://{}/{} already exists.\\nSet override to upload anyway.\\n\".format(\n",
    "                    s3_bucket, s3_path\n",
    "                )\n",
    "            )\n",
    "            return\n",
    "        else:\n",
    "            print(\"Overwriting existing file\")\n",
    "    with open(local_file, \"rb\") as data:\n",
    "        print(\"Uploading file to {}\".format(s3_path))\n",
    "        buk.put_object(Key=path, Body=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d3fb1ce6-dd3b-437c-9a1f-f9dd63b40b28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting existing file\n",
      "Uploading file to s3://dollar-predictor274/deepar-model/data/train/train.json\n",
      "Overwriting existing file\n",
      "Uploading file to s3://dollar-predictor274/deepar-model/data/test/test.json\n",
      "CPU times: user 60 ms, sys: 2.17 ms, total: 62.2 ms\n",
      "Wall time: 341 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "copy_to_s3(\"train.json\", s3_data_path + \"/train/train.json\")\n",
    "copy_to_s3(\"test.json\", s3_data_path + \"/test/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6754aa66-5d9f-4ebc-a443-60f766f5461e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri=image_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    base_job_name=\"dollar-predictor\",\n",
    "    output_path=s3_output_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1444c5a2-50e9-4cb6-969b-c9baa217e721",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"epochs\": \"400\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"5E-4\",\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5313abfe-00d9-45d4-b90f-27f5dd181209",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fd5b8d76-c697-4087-8b16-749c46c14b95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 0 ns, total: 5 µs\n",
      "Wall time: 8.58 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_channels = {\"train\": \"{}/train/\".format(s3_data_path), \"test\": \"{}/test/\".format(s3_data_path)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d7364fc3-4fd1-4f65-aabd-2a8c1bc70799",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: dollar-predictor-2023-04-10-20-31-00-048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-10 20:31:00 Starting - Starting the training job...\n",
      "2023-04-10 20:31:18 Starting - Preparing the instances for training......\n",
      "2023-04-10 20:32:22 Downloading - Downloading input data\n",
      "2023-04-10 20:32:22 Training - Downloading the training image........\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.8/site-packages/mxnet/model.py:78: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if num_device is 1 and 'dist' not in kvstore:\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.8/site-packages/jsonref.py:8: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  from collections import Mapping, MutableMapping, Sequence\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] Reading default configuration from /opt/amazon/lib/python3.8/site-packages/algorithm/resources/default-input.json: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'student-t', 'mini_batch_size': '128', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]'}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'context_length': '12', 'early_stopping_patience': '40', 'epochs': '400', 'learning_rate': '5E-4', 'mini_batch_size': '64', 'prediction_length': '1', 'time_freq': '10min'}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] Final configuration: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '40', 'embedding_dimension': '10', 'learning_rate': '5E-4', 'likelihood': 'student-t', 'mini_batch_size': '64', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', 'context_length': '12', 'epochs': '400', 'prediction_length': '1', 'time_freq': '10min'}\u001b[0m\n",
      "\u001b[34mProcess 7 is a worker.\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] random_seed is None\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] Training set statistics:\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] Real time series\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] number of time series: 17\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] number of observations: 500\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] mean target length: 29.41176470588235\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] min/mean/max target: 4676.54052734375/4823.015328125/4949.970703125\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] mean abs(target): 4823.015328125\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] contains missing values: no\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] Small number of time series. Doing 38 passes over dataset with prob 0.9907120743034056 per epoch.\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] Test set statistics:\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] Real time series\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] number of time series: 8\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] number of observations: 214\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] mean target length: 26.75\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] min/mean/max target: 4686.97265625/4790.41727739851/4876.41943359375\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] mean abs(target): 4790.41727739851\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] contains missing values: no\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] #memory_usage::<batchbuffer> = 0.40283203125 mb\u001b[0m\n",
      "\u001b[34m/opt/amazon/python3.8/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] nvidia-smi: took 0.030 seconds to run.\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158824.35928, \"EndTime\": 1681158824.3923025, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 31.995773315429688, \"count\": 1, \"min\": 31.995773315429688, \"max\": 31.995773315429688}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] #memory_usage::<model> = 5 mb\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158824.3923771, \"EndTime\": 1681158824.439673, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 80.25431632995605, \"count\": 1, \"min\": 80.25431632995605, \"max\": 80.25431632995605}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] Epoch[0] Batch[0] avg_epoch_loss=9.724931\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=9.724930763244629\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] Epoch[0] Batch[5] avg_epoch_loss=8.935590\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=8.935589790344238\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] Epoch[0] Batch [5]#011Speed: 2871.93 samples/sec#011loss=8.935590\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] Epoch[0] Batch[10] avg_epoch_loss=8.635173\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] #quality_metric: host=algo-1, epoch=0, batch=10 train loss <loss>=8.274672508239746\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] Epoch[0] Batch [10]#011Speed: 2700.70 samples/sec#011loss=8.274673\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] processed a total of 676 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158824.4397297, \"EndTime\": 1681158824.8866796, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 400.0, \"count\": 1, \"min\": 400, \"max\": 400}, \"update.time\": {\"sum\": 446.8681812286377, \"count\": 1, \"min\": 446.8681812286377, \"max\": 446.8681812286377}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1512.3848270226388 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] #progress_metric: host=algo-1, completed 0.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] #quality_metric: host=algo-1, epoch=0, train loss <loss>=8.635172843933105\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:44 INFO 139919854057280] Saved checkpoint to \"/opt/ml/model/state_082e0f21-ee6a-42f1-99c1-fd62765e4ad0-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158824.8867562, \"EndTime\": 1681158824.8971763, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 9.97614860534668, \"count\": 1, \"min\": 9.97614860534668, \"max\": 9.97614860534668}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:45 INFO 139919854057280] Epoch[1] Batch[0] avg_epoch_loss=7.340539\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:45 INFO 139919854057280] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=7.340538501739502\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:45 INFO 139919854057280] Epoch[1] Batch[5] avg_epoch_loss=7.573680\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:45 INFO 139919854057280] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=7.573680321375529\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:45 INFO 139919854057280] Epoch[1] Batch [5]#011Speed: 2756.66 samples/sec#011loss=7.573680\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:45 INFO 139919854057280] Epoch[1] Batch[10] avg_epoch_loss=7.079542\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:45 INFO 139919854057280] #quality_metric: host=algo-1, epoch=1, batch=10 train loss <loss>=6.48657636642456\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:45 INFO 139919854057280] Epoch[1] Batch [10]#011Speed: 2823.17 samples/sec#011loss=6.486576\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:45 INFO 139919854057280] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158824.8972447, \"EndTime\": 1681158825.2857287, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 388.43226432800293, \"count\": 1, \"min\": 388.43226432800293, \"max\": 388.43226432800293}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:45 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1706.4559502696998 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:45 INFO 139919854057280] #progress_metric: host=algo-1, completed 0.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:45 INFO 139919854057280] #quality_metric: host=algo-1, epoch=1, train loss <loss>=7.07954216003418\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:45 INFO 139919854057280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:45 INFO 139919854057280] Saved checkpoint to \"/opt/ml/model/state_1eb9d23a-451c-41b1-bfcb-478efbe238ad-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158825.2857926, \"EndTime\": 1681158825.2961936, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 10.029315948486328, \"count\": 1, \"min\": 10.029315948486328, \"max\": 10.029315948486328}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:45 INFO 139919854057280] Epoch[2] Batch[0] avg_epoch_loss=7.416363\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:45 INFO 139919854057280] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=7.41636323928833\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:45 INFO 139919854057280] Epoch[2] Batch[5] avg_epoch_loss=7.225015\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:45 INFO 139919854057280] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=7.225015163421631\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:45 INFO 139919854057280] Epoch[2] Batch [5]#011Speed: 2932.62 samples/sec#011loss=7.225015\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:45 INFO 139919854057280] Epoch[2] Batch[10] avg_epoch_loss=7.405990\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:45 INFO 139919854057280] #quality_metric: host=algo-1, epoch=2, batch=10 train loss <loss>=7.623160457611084\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:45 INFO 139919854057280] Epoch[2] Batch [10]#011Speed: 2885.76 samples/sec#011loss=7.623160\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:45 INFO 139919854057280] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158825.2962558, \"EndTime\": 1681158825.64822, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 351.9115447998047, \"count\": 1, \"min\": 351.9115447998047, \"max\": 351.9115447998047}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:45 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1909.0236818524472 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:45 INFO 139919854057280] #progress_metric: host=algo-1, completed 0.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:45 INFO 139919854057280] #quality_metric: host=algo-1, epoch=2, train loss <loss>=7.405990297144109\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:45 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:45 INFO 139919854057280] Epoch[3] Batch[0] avg_epoch_loss=7.435078\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:45 INFO 139919854057280] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=7.435078144073486\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:45 INFO 139919854057280] Epoch[3] Batch[5] avg_epoch_loss=7.212782\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:45 INFO 139919854057280] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=7.212782382965088\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:45 INFO 139919854057280] Epoch[3] Batch [5]#011Speed: 2557.69 samples/sec#011loss=7.212782\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:46 INFO 139919854057280] Epoch[3] Batch[10] avg_epoch_loss=6.559616\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:46 INFO 139919854057280] #quality_metric: host=algo-1, epoch=3, batch=10 train loss <loss>=5.775816321372986\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:46 INFO 139919854057280] Epoch[3] Batch [10]#011Speed: 2808.23 samples/sec#011loss=5.775816\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:46 INFO 139919854057280] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158825.6482913, \"EndTime\": 1681158826.0130858, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 364.3782138824463, \"count\": 1, \"min\": 364.3782138824463, \"max\": 364.3782138824463}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:46 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1769.691796750047 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:46 INFO 139919854057280] #progress_metric: host=algo-1, completed 1.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:46 INFO 139919854057280] #quality_metric: host=algo-1, epoch=3, train loss <loss>=6.559615991332314\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:46 INFO 139919854057280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:46 INFO 139919854057280] Saved checkpoint to \"/opt/ml/model/state_63061661-ead4-4bc7-8bec-d5620b8832be-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158826.0131435, \"EndTime\": 1681158826.0233634, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 9.607553482055664, \"count\": 1, \"min\": 9.607553482055664, \"max\": 9.607553482055664}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:46 INFO 139919854057280] Epoch[4] Batch[0] avg_epoch_loss=7.369681\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:46 INFO 139919854057280] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=7.369681358337402\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:46 INFO 139919854057280] Epoch[4] Batch[5] avg_epoch_loss=6.887222\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:46 INFO 139919854057280] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=6.887221574783325\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:46 INFO 139919854057280] Epoch[4] Batch [5]#011Speed: 2277.97 samples/sec#011loss=6.887222\u001b[0m\n",
      "\n",
      "2023-04-10 20:33:33 Training - Training image download completed. Training in progress.\u001b[34m[04/10/2023 20:33:46 INFO 139919854057280] Epoch[4] Batch[10] avg_epoch_loss=6.876058\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:46 INFO 139919854057280] #quality_metric: host=algo-1, epoch=4, batch=10 train loss <loss>=6.862661933898925\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:46 INFO 139919854057280] Epoch[4] Batch [10]#011Speed: 2112.14 samples/sec#011loss=6.862662\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:46 INFO 139919854057280] processed a total of 696 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158826.0234797, \"EndTime\": 1681158826.4470704, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 423.5386848449707, \"count\": 1, \"min\": 423.5386848449707, \"max\": 423.5386848449707}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:46 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1642.7295719231272 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:46 INFO 139919854057280] #progress_metric: host=algo-1, completed 1.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:46 INFO 139919854057280] #quality_metric: host=algo-1, epoch=4, train loss <loss>=6.876058101654053\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:46 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:46 INFO 139919854057280] Epoch[5] Batch[0] avg_epoch_loss=6.796571\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:46 INFO 139919854057280] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=6.796571254730225\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:46 INFO 139919854057280] Epoch[5] Batch[5] avg_epoch_loss=6.271861\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:46 INFO 139919854057280] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=6.271861394246419\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:46 INFO 139919854057280] Epoch[5] Batch [5]#011Speed: 2225.62 samples/sec#011loss=6.271861\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:46 INFO 139919854057280] Epoch[5] Batch[10] avg_epoch_loss=6.496443\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:46 INFO 139919854057280] #quality_metric: host=algo-1, epoch=5, batch=10 train loss <loss>=6.765940952301025\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:46 INFO 139919854057280] Epoch[5] Batch [10]#011Speed: 2061.69 samples/sec#011loss=6.765941\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:46 INFO 139919854057280] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158826.4471896, \"EndTime\": 1681158826.93305, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 485.4087829589844, \"count\": 1, \"min\": 485.4087829589844, \"max\": 485.4087829589844}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:46 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1369.6658348138742 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:46 INFO 139919854057280] #progress_metric: host=algo-1, completed 1.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:46 INFO 139919854057280] #quality_metric: host=algo-1, epoch=5, train loss <loss>=6.496443011543968\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:46 INFO 139919854057280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:46 INFO 139919854057280] Saved checkpoint to \"/opt/ml/model/state_b6e7eaad-0b14-4ebf-b348-d630568b1179-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158826.9331267, \"EndTime\": 1681158826.945041, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 11.452198028564453, \"count\": 1, \"min\": 11.452198028564453, \"max\": 11.452198028564453}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:47 INFO 139919854057280] Epoch[6] Batch[0] avg_epoch_loss=6.135828\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:47 INFO 139919854057280] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=6.135828495025635\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:47 INFO 139919854057280] Epoch[6] Batch[5] avg_epoch_loss=5.751726\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:47 INFO 139919854057280] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=5.751726309458415\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:47 INFO 139919854057280] Epoch[6] Batch [5]#011Speed: 2244.51 samples/sec#011loss=5.751726\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:47 INFO 139919854057280] Epoch[6] Batch[10] avg_epoch_loss=5.725518\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:47 INFO 139919854057280] #quality_metric: host=algo-1, epoch=6, batch=10 train loss <loss>=5.694068813323975\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:47 INFO 139919854057280] Epoch[6] Batch [10]#011Speed: 2052.56 samples/sec#011loss=5.694069\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:47 INFO 139919854057280] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158826.9451106, \"EndTime\": 1681158827.443512, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 498.34227561950684, \"count\": 1, \"min\": 498.34227561950684, \"max\": 498.34227561950684}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:47 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1334.1408169639676 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:47 INFO 139919854057280] #progress_metric: host=algo-1, completed 1.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:47 INFO 139919854057280] #quality_metric: host=algo-1, epoch=6, train loss <loss>=5.725518356670033\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:47 INFO 139919854057280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:47 INFO 139919854057280] Saved checkpoint to \"/opt/ml/model/state_ecc17f6a-f6bd-41f3-b739-121629d47c0a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158827.4435854, \"EndTime\": 1681158827.4533753, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 9.379386901855469, \"count\": 1, \"min\": 9.379386901855469, \"max\": 9.379386901855469}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:47 INFO 139919854057280] Epoch[7] Batch[0] avg_epoch_loss=5.913690\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:47 INFO 139919854057280] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=5.913690090179443\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:47 INFO 139919854057280] Epoch[7] Batch[5] avg_epoch_loss=5.688140\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:47 INFO 139919854057280] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=5.688140074412028\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:47 INFO 139919854057280] Epoch[7] Batch [5]#011Speed: 2155.28 samples/sec#011loss=5.688140\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:47 INFO 139919854057280] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158827.4534326, \"EndTime\": 1681158827.876552, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 423.0635166168213, \"count\": 1, \"min\": 423.0635166168213, \"max\": 423.0635166168213}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:47 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1512.415528044447 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:47 INFO 139919854057280] #progress_metric: host=algo-1, completed 2.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:47 INFO 139919854057280] #quality_metric: host=algo-1, epoch=7, train loss <loss>=5.72152304649353\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:47 INFO 139919854057280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:47 INFO 139919854057280] Saved checkpoint to \"/opt/ml/model/state_52abfce3-cd0a-419f-85c5-7cb7c1f9dfb0-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158827.8766224, \"EndTime\": 1681158827.8870668, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 9.996175765991211, \"count\": 1, \"min\": 9.996175765991211, \"max\": 9.996175765991211}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:48 INFO 139919854057280] Epoch[8] Batch[0] avg_epoch_loss=5.842690\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:48 INFO 139919854057280] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=5.842690467834473\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:48 INFO 139919854057280] Epoch[8] Batch[5] avg_epoch_loss=5.446462\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:48 INFO 139919854057280] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=5.44646151860555\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:48 INFO 139919854057280] Epoch[8] Batch [5]#011Speed: 2984.52 samples/sec#011loss=5.446462\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:48 INFO 139919854057280] Epoch[8] Batch[10] avg_epoch_loss=5.510561\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:48 INFO 139919854057280] #quality_metric: host=algo-1, epoch=8, batch=10 train loss <loss>=5.587480640411377\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:48 INFO 139919854057280] Epoch[8] Batch [10]#011Speed: 2554.70 samples/sec#011loss=5.587481\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:48 INFO 139919854057280] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158827.8871276, \"EndTime\": 1681158828.2678792, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 380.70154190063477, \"count\": 1, \"min\": 380.70154190063477, \"max\": 380.70154190063477}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:48 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1733.221451759837 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:48 INFO 139919854057280] #progress_metric: host=algo-1, completed 2.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:48 INFO 139919854057280] #quality_metric: host=algo-1, epoch=8, train loss <loss>=5.510561119426381\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:48 INFO 139919854057280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:48 INFO 139919854057280] Saved checkpoint to \"/opt/ml/model/state_51561fbe-abac-4102-8b67-20e050167ca5-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158828.2679436, \"EndTime\": 1681158828.2756405, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 7.272481918334961, \"count\": 1, \"min\": 7.272481918334961, \"max\": 7.272481918334961}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:48 INFO 139919854057280] Epoch[9] Batch[0] avg_epoch_loss=5.052765\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:48 INFO 139919854057280] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=5.052764892578125\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:48 INFO 139919854057280] Epoch[9] Batch[5] avg_epoch_loss=5.123994\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:48 INFO 139919854057280] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=5.12399419148763\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:48 INFO 139919854057280] Epoch[9] Batch [5]#011Speed: 2704.96 samples/sec#011loss=5.123994\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:48 INFO 139919854057280] Epoch[9] Batch[10] avg_epoch_loss=4.793555\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:48 INFO 139919854057280] #quality_metric: host=algo-1, epoch=9, batch=10 train loss <loss>=4.397027158737183\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:48 INFO 139919854057280] Epoch[9] Batch [10]#011Speed: 2952.66 samples/sec#011loss=4.397027\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:48 INFO 139919854057280] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158828.2756925, \"EndTime\": 1681158828.6549897, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 379.2450428009033, \"count\": 1, \"min\": 379.2450428009033, \"max\": 379.2450428009033}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:48 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1692.350679958721 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:48 INFO 139919854057280] #progress_metric: host=algo-1, completed 2.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:48 INFO 139919854057280] #quality_metric: host=algo-1, epoch=9, train loss <loss>=4.793554631146518\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:48 INFO 139919854057280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:48 INFO 139919854057280] Saved checkpoint to \"/opt/ml/model/state_4fc402f5-cece-4eaa-bdfd-1b96c8a8d70c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158828.655072, \"EndTime\": 1681158828.665155, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 9.74893569946289, \"count\": 1, \"min\": 9.74893569946289, \"max\": 9.74893569946289}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:48 INFO 139919854057280] Epoch[10] Batch[0] avg_epoch_loss=5.301409\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:48 INFO 139919854057280] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=5.301408767700195\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:48 INFO 139919854057280] Epoch[10] Batch[5] avg_epoch_loss=5.185513\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:48 INFO 139919854057280] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=5.185512860616048\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:48 INFO 139919854057280] Epoch[10] Batch [5]#011Speed: 2989.91 samples/sec#011loss=5.185513\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:49 INFO 139919854057280] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158828.665218, \"EndTime\": 1681158829.0184445, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 353.17516326904297, \"count\": 1, \"min\": 353.17516326904297, \"max\": 353.17516326904297}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:49 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1794.6331764237436 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:49 INFO 139919854057280] #progress_metric: host=algo-1, completed 2.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:49 INFO 139919854057280] #quality_metric: host=algo-1, epoch=10, train loss <loss>=5.176511430740357\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:49 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:49 INFO 139919854057280] Epoch[11] Batch[0] avg_epoch_loss=5.301753\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:49 INFO 139919854057280] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=5.301753044128418\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:49 INFO 139919854057280] Epoch[11] Batch[5] avg_epoch_loss=5.100469\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:49 INFO 139919854057280] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=5.100468715031941\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:49 INFO 139919854057280] Epoch[11] Batch [5]#011Speed: 2844.30 samples/sec#011loss=5.100469\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:49 INFO 139919854057280] Epoch[11] Batch[10] avg_epoch_loss=4.950151\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:49 INFO 139919854057280] #quality_metric: host=algo-1, epoch=11, batch=10 train loss <loss>=4.769769763946533\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:49 INFO 139919854057280] Epoch[11] Batch [10]#011Speed: 2513.29 samples/sec#011loss=4.769770\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:49 INFO 139919854057280] processed a total of 673 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158829.0185163, \"EndTime\": 1681158829.4127948, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 393.89681816101074, \"count\": 1, \"min\": 393.89681816101074, \"max\": 393.89681816101074}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:49 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1708.1122933137274 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:49 INFO 139919854057280] #progress_metric: host=algo-1, completed 3.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:49 INFO 139919854057280] #quality_metric: host=algo-1, epoch=11, train loss <loss>=4.95015100999312\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:49 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:49 INFO 139919854057280] Epoch[12] Batch[0] avg_epoch_loss=4.828574\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:49 INFO 139919854057280] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=4.828574180603027\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:49 INFO 139919854057280] Epoch[12] Batch[5] avg_epoch_loss=4.885735\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:49 INFO 139919854057280] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=4.885734875996907\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:49 INFO 139919854057280] Epoch[12] Batch [5]#011Speed: 2549.85 samples/sec#011loss=4.885735\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:49 INFO 139919854057280] Epoch[12] Batch[10] avg_epoch_loss=4.627303\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:49 INFO 139919854057280] #quality_metric: host=algo-1, epoch=12, batch=10 train loss <loss>=4.317183971405029\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:49 INFO 139919854057280] Epoch[12] Batch [10]#011Speed: 2723.62 samples/sec#011loss=4.317184\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:49 INFO 139919854057280] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158829.412869, \"EndTime\": 1681158829.7913384, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 377.99906730651855, \"count\": 1, \"min\": 377.99906730651855, \"max\": 377.99906730651855}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:49 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1729.6829690462664 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:49 INFO 139919854057280] #progress_metric: host=algo-1, completed 3.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:49 INFO 139919854057280] #quality_metric: host=algo-1, epoch=12, train loss <loss>=4.627302646636963\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:49 INFO 139919854057280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:49 INFO 139919854057280] Saved checkpoint to \"/opt/ml/model/state_8a8c319c-95c1-4c27-818b-d235a0777e81-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158829.7914135, \"EndTime\": 1681158829.799122, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 7.294178009033203, \"count\": 1, \"min\": 7.294178009033203, \"max\": 7.294178009033203}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:49 INFO 139919854057280] Epoch[13] Batch[0] avg_epoch_loss=4.781801\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:49 INFO 139919854057280] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=4.781800746917725\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:50 INFO 139919854057280] Epoch[13] Batch[5] avg_epoch_loss=4.851944\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:50 INFO 139919854057280] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=4.851944049199422\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:50 INFO 139919854057280] Epoch[13] Batch [5]#011Speed: 2909.67 samples/sec#011loss=4.851944\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:50 INFO 139919854057280] Epoch[13] Batch[10] avg_epoch_loss=4.833870\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:50 INFO 139919854057280] #quality_metric: host=algo-1, epoch=13, batch=10 train loss <loss>=4.8121819496154785\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:50 INFO 139919854057280] Epoch[13] Batch [10]#011Speed: 2800.00 samples/sec#011loss=4.812182\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:50 INFO 139919854057280] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158829.799278, \"EndTime\": 1681158830.1514459, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 352.1089553833008, \"count\": 1, \"min\": 352.1089553833008, \"max\": 352.1089553833008}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:50 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1865.4285758770668 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:50 INFO 139919854057280] #progress_metric: host=algo-1, completed 3.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:50 INFO 139919854057280] #quality_metric: host=algo-1, epoch=13, train loss <loss>=4.8338703675703565\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:50 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:50 INFO 139919854057280] Epoch[14] Batch[0] avg_epoch_loss=4.685458\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:50 INFO 139919854057280] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=4.685458183288574\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:50 INFO 139919854057280] Epoch[14] Batch[5] avg_epoch_loss=4.801187\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:50 INFO 139919854057280] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=4.801187038421631\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:50 INFO 139919854057280] Epoch[14] Batch [5]#011Speed: 2675.59 samples/sec#011loss=4.801187\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:50 INFO 139919854057280] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158830.1515038, \"EndTime\": 1681158830.487966, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 335.72959899902344, \"count\": 1, \"min\": 335.72959899902344, \"max\": 335.72959899902344}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:50 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1890.645836350265 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:50 INFO 139919854057280] #progress_metric: host=algo-1, completed 3.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:50 INFO 139919854057280] #quality_metric: host=algo-1, epoch=14, train loss <loss>=4.762334966659546\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:50 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:50 INFO 139919854057280] Epoch[15] Batch[0] avg_epoch_loss=4.624826\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:50 INFO 139919854057280] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=4.624825954437256\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:50 INFO 139919854057280] Epoch[15] Batch[5] avg_epoch_loss=4.686292\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:50 INFO 139919854057280] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=4.686292250951131\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:50 INFO 139919854057280] Epoch[15] Batch [5]#011Speed: 3065.76 samples/sec#011loss=4.686292\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:50 INFO 139919854057280] Epoch[15] Batch[10] avg_epoch_loss=4.785079\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:50 INFO 139919854057280] #quality_metric: host=algo-1, epoch=15, batch=10 train loss <loss>=4.903623867034912\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:50 INFO 139919854057280] Epoch[15] Batch [10]#011Speed: 2880.09 samples/sec#011loss=4.903624\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:50 INFO 139919854057280] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158830.4880385, \"EndTime\": 1681158830.8587594, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 370.07832527160645, \"count\": 1, \"min\": 370.07832527160645, \"max\": 370.07832527160645}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:50 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1772.1513320778147 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:50 INFO 139919854057280] #progress_metric: host=algo-1, completed 4.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:50 INFO 139919854057280] #quality_metric: host=algo-1, epoch=15, train loss <loss>=4.785079349171031\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:50 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:50 INFO 139919854057280] Epoch[16] Batch[0] avg_epoch_loss=4.630353\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:50 INFO 139919854057280] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=4.630352973937988\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:51 INFO 139919854057280] Epoch[16] Batch[5] avg_epoch_loss=4.637294\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:51 INFO 139919854057280] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=4.637294212977092\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:51 INFO 139919854057280] Epoch[16] Batch [5]#011Speed: 3029.16 samples/sec#011loss=4.637294\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:51 INFO 139919854057280] Epoch[16] Batch[10] avg_epoch_loss=4.704233\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:51 INFO 139919854057280] #quality_metric: host=algo-1, epoch=16, batch=10 train loss <loss>=4.7845603942871096\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:51 INFO 139919854057280] Epoch[16] Batch [10]#011Speed: 2726.06 samples/sec#011loss=4.784560\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:51 INFO 139919854057280] processed a total of 685 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158830.858825, \"EndTime\": 1681158831.214456, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 355.21912574768066, \"count\": 1, \"min\": 355.21912574768066, \"max\": 355.21912574768066}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:51 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1927.8850958038227 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:51 INFO 139919854057280] #progress_metric: host=algo-1, completed 4.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:51 INFO 139919854057280] #quality_metric: host=algo-1, epoch=16, train loss <loss>=4.704233386299827\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:51 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:51 INFO 139919854057280] Epoch[17] Batch[0] avg_epoch_loss=4.479425\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:51 INFO 139919854057280] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=4.479425430297852\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:51 INFO 139919854057280] Epoch[17] Batch[5] avg_epoch_loss=4.591465\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:51 INFO 139919854057280] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=4.591464519500732\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:51 INFO 139919854057280] Epoch[17] Batch [5]#011Speed: 2634.94 samples/sec#011loss=4.591465\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:51 INFO 139919854057280] Epoch[17] Batch[10] avg_epoch_loss=4.620695\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:51 INFO 139919854057280] #quality_metric: host=algo-1, epoch=17, batch=10 train loss <loss>=4.655770778656006\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:51 INFO 139919854057280] Epoch[17] Batch [10]#011Speed: 2534.51 samples/sec#011loss=4.655771\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:51 INFO 139919854057280] processed a total of 697 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158831.2145216, \"EndTime\": 1681158831.607601, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 392.7156925201416, \"count\": 1, \"min\": 392.7156925201416, \"max\": 392.7156925201416}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:51 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1774.4190363584055 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:51 INFO 139919854057280] #progress_metric: host=algo-1, completed 4.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:51 INFO 139919854057280] #quality_metric: host=algo-1, epoch=17, train loss <loss>=4.620694637298584\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:51 INFO 139919854057280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:51 INFO 139919854057280] Saved checkpoint to \"/opt/ml/model/state_0cb21dcc-00b4-41c0-bf24-a202bf839379-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158831.6076639, \"EndTime\": 1681158831.6175358, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 9.546279907226562, \"count\": 1, \"min\": 9.546279907226562, \"max\": 9.546279907226562}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:51 INFO 139919854057280] Epoch[18] Batch[0] avg_epoch_loss=4.071756\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:51 INFO 139919854057280] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=4.071755886077881\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:51 INFO 139919854057280] Epoch[18] Batch[5] avg_epoch_loss=4.574735\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:51 INFO 139919854057280] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=4.574735005696614\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:51 INFO 139919854057280] Epoch[18] Batch [5]#011Speed: 2949.49 samples/sec#011loss=4.574735\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:51 INFO 139919854057280] Epoch[18] Batch[10] avg_epoch_loss=4.744250\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:51 INFO 139919854057280] #quality_metric: host=algo-1, epoch=18, batch=10 train loss <loss>=4.947668552398682\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:51 INFO 139919854057280] Epoch[18] Batch [10]#011Speed: 2671.96 samples/sec#011loss=4.947669\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:51 INFO 139919854057280] processed a total of 678 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158831.61781, \"EndTime\": 1681158831.9779196, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 360.0633144378662, \"count\": 1, \"min\": 360.0633144378662, \"max\": 360.0633144378662}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:51 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1882.4985731014901 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:51 INFO 139919854057280] #progress_metric: host=algo-1, completed 4.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:51 INFO 139919854057280] #quality_metric: host=algo-1, epoch=18, train loss <loss>=4.744250254197554\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:51 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:52 INFO 139919854057280] Epoch[19] Batch[0] avg_epoch_loss=5.609025\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:52 INFO 139919854057280] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=5.609025001525879\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:52 INFO 139919854057280] Epoch[19] Batch[5] avg_epoch_loss=5.593959\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:52 INFO 139919854057280] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=5.593958695729573\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:52 INFO 139919854057280] Epoch[19] Batch [5]#011Speed: 2821.20 samples/sec#011loss=5.593959\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:52 INFO 139919854057280] Epoch[19] Batch[10] avg_epoch_loss=5.213247\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:52 INFO 139919854057280] #quality_metric: host=algo-1, epoch=19, batch=10 train loss <loss>=4.756392145156861\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:52 INFO 139919854057280] Epoch[19] Batch [10]#011Speed: 2837.94 samples/sec#011loss=4.756392\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:52 INFO 139919854057280] processed a total of 679 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158831.9779878, \"EndTime\": 1681158832.3436484, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 365.22483825683594, \"count\": 1, \"min\": 365.22483825683594, \"max\": 365.22483825683594}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:52 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1858.6082531213026 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:52 INFO 139919854057280] #progress_metric: host=algo-1, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:52 INFO 139919854057280] #quality_metric: host=algo-1, epoch=19, train loss <loss>=5.213246627287432\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:52 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:52 INFO 139919854057280] Epoch[20] Batch[0] avg_epoch_loss=5.665145\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:52 INFO 139919854057280] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=5.665144920349121\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:52 INFO 139919854057280] Epoch[20] Batch[5] avg_epoch_loss=4.858157\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:52 INFO 139919854057280] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=4.858156601587932\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:52 INFO 139919854057280] Epoch[20] Batch [5]#011Speed: 2802.77 samples/sec#011loss=4.858157\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:52 INFO 139919854057280] Epoch[20] Batch[10] avg_epoch_loss=4.793676\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:52 INFO 139919854057280] #quality_metric: host=algo-1, epoch=20, batch=10 train loss <loss>=4.716299152374267\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:52 INFO 139919854057280] Epoch[20] Batch [10]#011Speed: 2813.61 samples/sec#011loss=4.716299\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:52 INFO 139919854057280] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158832.343707, \"EndTime\": 1681158832.698742, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 354.5999526977539, \"count\": 1, \"min\": 354.5999526977539, \"max\": 354.5999526977539}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:52 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1900.0589422970934 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:52 INFO 139919854057280] #progress_metric: host=algo-1, completed 5.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:52 INFO 139919854057280] #quality_metric: host=algo-1, epoch=20, train loss <loss>=4.793675942854448\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:52 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:52 INFO 139919854057280] Epoch[21] Batch[0] avg_epoch_loss=4.812161\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:52 INFO 139919854057280] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=4.812161445617676\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:52 INFO 139919854057280] Epoch[21] Batch[5] avg_epoch_loss=4.854586\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:52 INFO 139919854057280] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=4.8545864423116045\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:52 INFO 139919854057280] Epoch[21] Batch [5]#011Speed: 3051.06 samples/sec#011loss=4.854586\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:53 INFO 139919854057280] Epoch[21] Batch[10] avg_epoch_loss=4.802892\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:53 INFO 139919854057280] #quality_metric: host=algo-1, epoch=21, batch=10 train loss <loss>=4.7408576011657715\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:53 INFO 139919854057280] Epoch[21] Batch [10]#011Speed: 2802.88 samples/sec#011loss=4.740858\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:53 INFO 139919854057280] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158832.698835, \"EndTime\": 1681158833.0614452, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 362.08200454711914, \"count\": 1, \"min\": 362.08200454711914, \"max\": 362.08200454711914}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:53 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1805.7165306436812 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:53 INFO 139919854057280] #progress_metric: host=algo-1, completed 5.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:53 INFO 139919854057280] #quality_metric: host=algo-1, epoch=21, train loss <loss>=4.802891514518044\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:53 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:53 INFO 139919854057280] Epoch[22] Batch[0] avg_epoch_loss=4.781146\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:53 INFO 139919854057280] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=4.781146049499512\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:53 INFO 139919854057280] Epoch[22] Batch[5] avg_epoch_loss=4.818048\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:53 INFO 139919854057280] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=4.818047523498535\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:53 INFO 139919854057280] Epoch[22] Batch [5]#011Speed: 2986.12 samples/sec#011loss=4.818048\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:53 INFO 139919854057280] Epoch[22] Batch[10] avg_epoch_loss=4.883267\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:53 INFO 139919854057280] #quality_metric: host=algo-1, epoch=22, batch=10 train loss <loss>=4.961530971527099\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:53 INFO 139919854057280] Epoch[22] Batch [10]#011Speed: 2868.86 samples/sec#011loss=4.961531\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:53 INFO 139919854057280] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158833.061514, \"EndTime\": 1681158833.4114285, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 349.5373725891113, \"count\": 1, \"min\": 349.5373725891113, \"max\": 349.5373725891113}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:53 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1881.954249694166 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:53 INFO 139919854057280] #progress_metric: host=algo-1, completed 5.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:53 INFO 139919854057280] #quality_metric: host=algo-1, epoch=22, train loss <loss>=4.883267272602428\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:53 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:53 INFO 139919854057280] Epoch[23] Batch[0] avg_epoch_loss=4.311441\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:53 INFO 139919854057280] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=4.311441421508789\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:53 INFO 139919854057280] Epoch[23] Batch[5] avg_epoch_loss=4.515102\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:53 INFO 139919854057280] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=4.515101671218872\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:53 INFO 139919854057280] Epoch[23] Batch [5]#011Speed: 2731.12 samples/sec#011loss=4.515102\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:53 INFO 139919854057280] Epoch[23] Batch[10] avg_epoch_loss=4.427203\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:53 INFO 139919854057280] #quality_metric: host=algo-1, epoch=23, batch=10 train loss <loss>=4.321725368499756\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:53 INFO 139919854057280] Epoch[23] Batch [10]#011Speed: 2754.76 samples/sec#011loss=4.321725\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:53 INFO 139919854057280] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158833.4114952, \"EndTime\": 1681158833.7871196, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 375.2474784851074, \"count\": 1, \"min\": 375.2474784851074, \"max\": 375.2474784851074}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:53 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1771.7315405205916 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:53 INFO 139919854057280] #progress_metric: host=algo-1, completed 6.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:53 INFO 139919854057280] #quality_metric: host=algo-1, epoch=23, train loss <loss>=4.427203351801092\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:53 INFO 139919854057280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:53 INFO 139919854057280] Saved checkpoint to \"/opt/ml/model/state_8cbdc8d6-19b7-4878-8c73-389d65242b67-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158833.7871828, \"EndTime\": 1681158833.7964187, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 8.868932723999023, \"count\": 1, \"min\": 8.868932723999023, \"max\": 8.868932723999023}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:53 INFO 139919854057280] Epoch[24] Batch[0] avg_epoch_loss=4.524770\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:53 INFO 139919854057280] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=4.5247697830200195\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:54 INFO 139919854057280] Epoch[24] Batch[5] avg_epoch_loss=4.584161\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:54 INFO 139919854057280] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=4.5841606458028155\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:54 INFO 139919854057280] Epoch[24] Batch [5]#011Speed: 2896.07 samples/sec#011loss=4.584161\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:54 INFO 139919854057280] Epoch[24] Batch[10] avg_epoch_loss=4.424483\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:54 INFO 139919854057280] #quality_metric: host=algo-1, epoch=24, batch=10 train loss <loss>=4.232869720458984\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:54 INFO 139919854057280] Epoch[24] Batch [10]#011Speed: 2783.78 samples/sec#011loss=4.232870\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:54 INFO 139919854057280] processed a total of 677 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158833.7964897, \"EndTime\": 1681158834.1534855, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 356.94360733032227, \"count\": 1, \"min\": 356.94360733032227, \"max\": 356.94360733032227}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:54 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1896.1819238119915 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:54 INFO 139919854057280] #progress_metric: host=algo-1, completed 6.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:54 INFO 139919854057280] #quality_metric: host=algo-1, epoch=24, train loss <loss>=4.424482952464711\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:54 INFO 139919854057280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:54 INFO 139919854057280] Saved checkpoint to \"/opt/ml/model/state_f5f6f345-690f-4a31-a178-ddfcfaed1f28-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158834.1535487, \"EndTime\": 1681158834.1616836, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 7.80797004699707, \"count\": 1, \"min\": 7.80797004699707, \"max\": 7.80797004699707}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:54 INFO 139919854057280] Epoch[25] Batch[0] avg_epoch_loss=4.352596\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:54 INFO 139919854057280] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=4.352596282958984\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:54 INFO 139919854057280] Epoch[25] Batch[5] avg_epoch_loss=4.318566\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:54 INFO 139919854057280] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=4.31856632232666\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:54 INFO 139919854057280] Epoch[25] Batch [5]#011Speed: 2935.07 samples/sec#011loss=4.318566\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:54 INFO 139919854057280] Epoch[25] Batch[10] avg_epoch_loss=4.437744\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:54 INFO 139919854057280] #quality_metric: host=algo-1, epoch=25, batch=10 train loss <loss>=4.580756759643554\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:54 INFO 139919854057280] Epoch[25] Batch [10]#011Speed: 2893.07 samples/sec#011loss=4.580757\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:54 INFO 139919854057280] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158834.1617455, \"EndTime\": 1681158834.5115757, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 349.7800827026367, \"count\": 1, \"min\": 349.7800827026367, \"max\": 349.7800827026367}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:54 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1837.776812265758 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:54 INFO 139919854057280] #progress_metric: host=algo-1, completed 6.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:54 INFO 139919854057280] #quality_metric: host=algo-1, epoch=25, train loss <loss>=4.43774379383434\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:54 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:54 INFO 139919854057280] Epoch[26] Batch[0] avg_epoch_loss=3.846016\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:54 INFO 139919854057280] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=3.8460159301757812\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:54 INFO 139919854057280] Epoch[26] Batch[5] avg_epoch_loss=4.360375\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:54 INFO 139919854057280] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=4.3603748480478925\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:54 INFO 139919854057280] Epoch[26] Batch [5]#011Speed: 2805.82 samples/sec#011loss=4.360375\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:54 INFO 139919854057280] Epoch[26] Batch[10] avg_epoch_loss=4.311941\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:54 INFO 139919854057280] #quality_metric: host=algo-1, epoch=26, batch=10 train loss <loss>=4.253819513320923\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:54 INFO 139919854057280] Epoch[26] Batch [10]#011Speed: 2941.07 samples/sec#011loss=4.253820\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:54 INFO 139919854057280] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158834.5116465, \"EndTime\": 1681158834.8722246, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 360.0175380706787, \"count\": 1, \"min\": 360.0175380706787, \"max\": 360.0175380706787}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:54 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1791.1191589573964 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:54 INFO 139919854057280] #progress_metric: host=algo-1, completed 6.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:54 INFO 139919854057280] #quality_metric: host=algo-1, epoch=26, train loss <loss>=4.311940604990179\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:54 INFO 139919854057280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:54 INFO 139919854057280] Saved checkpoint to \"/opt/ml/model/state_f38fa466-bdb4-48ee-8119-118524aa992d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158834.8722901, \"EndTime\": 1681158834.8809779, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 8.29005241394043, \"count\": 1, \"min\": 8.29005241394043, \"max\": 8.29005241394043}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:55 INFO 139919854057280] Epoch[27] Batch[0] avg_epoch_loss=4.200191\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:55 INFO 139919854057280] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=4.200190544128418\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:55 INFO 139919854057280] Epoch[27] Batch[5] avg_epoch_loss=4.475160\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:55 INFO 139919854057280] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=4.475160280863444\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:55 INFO 139919854057280] Epoch[27] Batch [5]#011Speed: 2731.86 samples/sec#011loss=4.475160\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:55 INFO 139919854057280] Epoch[27] Batch[10] avg_epoch_loss=4.589749\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:55 INFO 139919854057280] #quality_metric: host=algo-1, epoch=27, batch=10 train loss <loss>=4.727254915237427\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:55 INFO 139919854057280] Epoch[27] Batch [10]#011Speed: 2869.73 samples/sec#011loss=4.727255\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:55 INFO 139919854057280] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158834.881069, \"EndTime\": 1681158835.2415001, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 360.37516593933105, \"count\": 1, \"min\": 360.37516593933105, \"max\": 360.37516593933105}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:55 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1819.776216468615 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:55 INFO 139919854057280] #progress_metric: host=algo-1, completed 7.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:55 INFO 139919854057280] #quality_metric: host=algo-1, epoch=27, train loss <loss>=4.5897487510334365\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:55 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:55 INFO 139919854057280] Epoch[28] Batch[0] avg_epoch_loss=4.485487\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:55 INFO 139919854057280] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=4.485487461090088\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:55 INFO 139919854057280] Epoch[28] Batch[5] avg_epoch_loss=4.550331\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:55 INFO 139919854057280] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=4.550330797831218\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:55 INFO 139919854057280] Epoch[28] Batch [5]#011Speed: 2852.08 samples/sec#011loss=4.550331\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:55 INFO 139919854057280] Epoch[28] Batch[10] avg_epoch_loss=4.594756\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:55 INFO 139919854057280] #quality_metric: host=algo-1, epoch=28, batch=10 train loss <loss>=4.6480659484863285\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:55 INFO 139919854057280] Epoch[28] Batch [10]#011Speed: 2701.80 samples/sec#011loss=4.648066\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:55 INFO 139919854057280] processed a total of 693 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158835.2415774, \"EndTime\": 1681158835.6037917, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 361.66882514953613, \"count\": 1, \"min\": 361.66882514953613, \"max\": 361.66882514953613}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:55 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1915.6440364747411 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:55 INFO 139919854057280] #progress_metric: host=algo-1, completed 7.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:55 INFO 139919854057280] #quality_metric: host=algo-1, epoch=28, train loss <loss>=4.5947558663108135\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:55 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:55 INFO 139919854057280] Epoch[29] Batch[0] avg_epoch_loss=4.508345\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:55 INFO 139919854057280] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=4.508345127105713\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:55 INFO 139919854057280] Epoch[29] Batch[5] avg_epoch_loss=4.724985\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:55 INFO 139919854057280] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=4.724984804789226\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:55 INFO 139919854057280] Epoch[29] Batch [5]#011Speed: 3006.89 samples/sec#011loss=4.724985\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:55 INFO 139919854057280] Epoch[29] Batch[10] avg_epoch_loss=4.552677\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:55 INFO 139919854057280] #quality_metric: host=algo-1, epoch=29, batch=10 train loss <loss>=4.345906972885132\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:55 INFO 139919854057280] Epoch[29] Batch [10]#011Speed: 2863.07 samples/sec#011loss=4.345907\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:55 INFO 139919854057280] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158835.603854, \"EndTime\": 1681158835.9561572, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 351.99928283691406, \"count\": 1, \"min\": 351.99928283691406, \"max\": 351.99928283691406}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:55 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1877.3475480148404 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:55 INFO 139919854057280] #progress_metric: host=algo-1, completed 7.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:55 INFO 139919854057280] #quality_metric: host=algo-1, epoch=29, train loss <loss>=4.5526766993782735\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:55 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:56 INFO 139919854057280] Epoch[30] Batch[0] avg_epoch_loss=5.064450\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:56 INFO 139919854057280] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=5.064449787139893\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:56 INFO 139919854057280] Epoch[30] Batch[5] avg_epoch_loss=4.799187\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:56 INFO 139919854057280] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=4.799186627070109\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:56 INFO 139919854057280] Epoch[30] Batch [5]#011Speed: 3061.21 samples/sec#011loss=4.799187\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:56 INFO 139919854057280] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158835.9562228, \"EndTime\": 1681158836.2870681, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 330.45291900634766, \"count\": 1, \"min\": 330.45291900634766, \"max\": 330.45291900634766}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:56 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1848.4106791109637 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:56 INFO 139919854057280] #progress_metric: host=algo-1, completed 7.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:56 INFO 139919854057280] #quality_metric: host=algo-1, epoch=30, train loss <loss>=4.566842103004456\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:56 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:56 INFO 139919854057280] Epoch[31] Batch[0] avg_epoch_loss=4.789987\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:56 INFO 139919854057280] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=4.789987087249756\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:56 INFO 139919854057280] Epoch[31] Batch[5] avg_epoch_loss=4.706420\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:56 INFO 139919854057280] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=4.706419865290324\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:56 INFO 139919854057280] Epoch[31] Batch [5]#011Speed: 2714.28 samples/sec#011loss=4.706420\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:56 INFO 139919854057280] processed a total of 610 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158836.28714, \"EndTime\": 1681158836.66407, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 376.5759468078613, \"count\": 1, \"min\": 376.5759468078613, \"max\": 376.5759468078613}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:56 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1619.43879424059 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:56 INFO 139919854057280] #progress_metric: host=algo-1, completed 8.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:56 INFO 139919854057280] #quality_metric: host=algo-1, epoch=31, train loss <loss>=4.72578797340393\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:56 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:56 INFO 139919854057280] Epoch[32] Batch[0] avg_epoch_loss=4.846107\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:56 INFO 139919854057280] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=4.846107482910156\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:56 INFO 139919854057280] Epoch[32] Batch[5] avg_epoch_loss=4.653721\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:56 INFO 139919854057280] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=4.653721332550049\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:56 INFO 139919854057280] Epoch[32] Batch [5]#011Speed: 2852.08 samples/sec#011loss=4.653721\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:56 INFO 139919854057280] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158836.6641402, \"EndTime\": 1681158837.0001693, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 335.4785442352295, \"count\": 1, \"min\": 335.4785442352295, \"max\": 335.4785442352295}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:57 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1856.4418202007873 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:57 INFO 139919854057280] #progress_metric: host=algo-1, completed 8.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:57 INFO 139919854057280] #quality_metric: host=algo-1, epoch=32, train loss <loss>=4.587476825714111\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:57 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:57 INFO 139919854057280] Epoch[33] Batch[0] avg_epoch_loss=4.906759\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:57 INFO 139919854057280] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=4.906758785247803\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:57 INFO 139919854057280] Epoch[33] Batch[5] avg_epoch_loss=4.634148\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:57 INFO 139919854057280] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=4.634148359298706\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:57 INFO 139919854057280] Epoch[33] Batch [5]#011Speed: 2989.17 samples/sec#011loss=4.634148\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:57 INFO 139919854057280] Epoch[33] Batch[10] avg_epoch_loss=4.635267\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:57 INFO 139919854057280] #quality_metric: host=algo-1, epoch=33, batch=10 train loss <loss>=4.636608695983886\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:57 INFO 139919854057280] Epoch[33] Batch [10]#011Speed: 2699.73 samples/sec#011loss=4.636609\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:57 INFO 139919854057280] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158837.0002477, \"EndTime\": 1681158837.371296, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 370.6953525543213, \"count\": 1, \"min\": 370.6953525543213, \"max\": 370.6953525543213}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:57 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1798.8341068523628 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:57 INFO 139919854057280] #progress_metric: host=algo-1, completed 8.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:57 INFO 139919854057280] #quality_metric: host=algo-1, epoch=33, train loss <loss>=4.635266694155606\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:57 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:57 INFO 139919854057280] Epoch[34] Batch[0] avg_epoch_loss=4.704342\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:57 INFO 139919854057280] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=4.704341888427734\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:57 INFO 139919854057280] Epoch[34] Batch[5] avg_epoch_loss=4.578619\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:57 INFO 139919854057280] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=4.578619480133057\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:57 INFO 139919854057280] Epoch[34] Batch [5]#011Speed: 2889.90 samples/sec#011loss=4.578619\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:57 INFO 139919854057280] Epoch[34] Batch[10] avg_epoch_loss=4.668437\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:57 INFO 139919854057280] #quality_metric: host=algo-1, epoch=34, batch=10 train loss <loss>=4.776218128204346\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:57 INFO 139919854057280] Epoch[34] Batch [10]#011Speed: 2872.25 samples/sec#011loss=4.776218\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:57 INFO 139919854057280] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158837.3713672, \"EndTime\": 1681158837.7554402, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 383.47625732421875, \"count\": 1, \"min\": 383.47625732421875, \"max\": 383.47625732421875}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:57 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1673.7629680829639 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:57 INFO 139919854057280] #progress_metric: host=algo-1, completed 8.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:57 INFO 139919854057280] #quality_metric: host=algo-1, epoch=34, train loss <loss>=4.668437047438188\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:57 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:57 INFO 139919854057280] Epoch[35] Batch[0] avg_epoch_loss=4.448821\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:57 INFO 139919854057280] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=4.4488205909729\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:57 INFO 139919854057280] Epoch[35] Batch[5] avg_epoch_loss=4.312946\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:57 INFO 139919854057280] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=4.31294600168864\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:57 INFO 139919854057280] Epoch[35] Batch [5]#011Speed: 2957.48 samples/sec#011loss=4.312946\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:58 INFO 139919854057280] Epoch[35] Batch[10] avg_epoch_loss=4.487696\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:58 INFO 139919854057280] #quality_metric: host=algo-1, epoch=35, batch=10 train loss <loss>=4.697397041320801\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:58 INFO 139919854057280] Epoch[35] Batch [10]#011Speed: 2904.28 samples/sec#011loss=4.697397\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:58 INFO 139919854057280] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158837.7555013, \"EndTime\": 1681158838.108206, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 352.10251808166504, \"count\": 1, \"min\": 352.10251808166504, \"max\": 352.10251808166504}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:58 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1868.2577711166755 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:58 INFO 139919854057280] #progress_metric: host=algo-1, completed 9.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:58 INFO 139919854057280] #quality_metric: host=algo-1, epoch=35, train loss <loss>=4.487696474248713\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:58 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:58 INFO 139919854057280] Epoch[36] Batch[0] avg_epoch_loss=4.212984\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:58 INFO 139919854057280] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=4.21298360824585\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:58 INFO 139919854057280] Epoch[36] Batch[5] avg_epoch_loss=4.405099\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:58 INFO 139919854057280] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=4.405098517735799\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:58 INFO 139919854057280] Epoch[36] Batch [5]#011Speed: 2949.51 samples/sec#011loss=4.405099\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:58 INFO 139919854057280] Epoch[36] Batch[10] avg_epoch_loss=4.471891\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:58 INFO 139919854057280] #quality_metric: host=algo-1, epoch=36, batch=10 train loss <loss>=4.552042388916016\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:58 INFO 139919854057280] Epoch[36] Batch [10]#011Speed: 2672.17 samples/sec#011loss=4.552042\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:58 INFO 139919854057280] processed a total of 691 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158838.1082728, \"EndTime\": 1681158838.4633262, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 354.6478748321533, \"count\": 1, \"min\": 354.6478748321533, \"max\": 354.6478748321533}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:58 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1947.881159159248 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:58 INFO 139919854057280] #progress_metric: host=algo-1, completed 9.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:58 INFO 139919854057280] #quality_metric: host=algo-1, epoch=36, train loss <loss>=4.471891186454079\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:58 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:58 INFO 139919854057280] Epoch[37] Batch[0] avg_epoch_loss=4.216550\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:58 INFO 139919854057280] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=4.216549873352051\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:58 INFO 139919854057280] Epoch[37] Batch[5] avg_epoch_loss=4.240000\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:58 INFO 139919854057280] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=4.239999691645305\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:58 INFO 139919854057280] Epoch[37] Batch [5]#011Speed: 2997.68 samples/sec#011loss=4.240000\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:58 INFO 139919854057280] Epoch[37] Batch[10] avg_epoch_loss=4.269473\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:58 INFO 139919854057280] #quality_metric: host=algo-1, epoch=37, batch=10 train loss <loss>=4.304841756820679\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:58 INFO 139919854057280] Epoch[37] Batch [10]#011Speed: 2650.91 samples/sec#011loss=4.304842\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:58 INFO 139919854057280] processed a total of 693 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158838.4633942, \"EndTime\": 1681158838.8353043, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 371.55747413635254, \"count\": 1, \"min\": 371.55747413635254, \"max\": 371.55747413635254}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:58 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1864.6469398045447 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:58 INFO 139919854057280] #progress_metric: host=algo-1, completed 9.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:58 INFO 139919854057280] #quality_metric: host=algo-1, epoch=37, train loss <loss>=4.269473357634111\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:58 INFO 139919854057280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:58 INFO 139919854057280] Saved checkpoint to \"/opt/ml/model/state_1f9485b4-7739-45dd-bebd-409ae0ec35e6-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158838.835372, \"EndTime\": 1681158838.84562, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 9.845495223999023, \"count\": 1, \"min\": 9.845495223999023, \"max\": 9.845495223999023}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:59 INFO 139919854057280] Epoch[38] Batch[0] avg_epoch_loss=4.617793\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:59 INFO 139919854057280] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=4.617793083190918\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:59 INFO 139919854057280] Epoch[38] Batch[5] avg_epoch_loss=4.313630\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:59 INFO 139919854057280] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=4.313630024592082\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:59 INFO 139919854057280] Epoch[38] Batch [5]#011Speed: 2922.82 samples/sec#011loss=4.313630\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:59 INFO 139919854057280] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158838.8456821, \"EndTime\": 1681158839.2023554, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 356.62150382995605, \"count\": 1, \"min\": 356.62150382995605, \"max\": 356.62150382995605}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:59 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1749.2712806912482 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:59 INFO 139919854057280] #progress_metric: host=algo-1, completed 9.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:59 INFO 139919854057280] #quality_metric: host=algo-1, epoch=38, train loss <loss>=4.2407667398452755\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:59 INFO 139919854057280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:59 INFO 139919854057280] Saved checkpoint to \"/opt/ml/model/state_6b131d91-18de-4985-afe5-121d3b7098dc-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158839.202426, \"EndTime\": 1681158839.2124312, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 9.458780288696289, \"count\": 1, \"min\": 9.458780288696289, \"max\": 9.458780288696289}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:59 INFO 139919854057280] Epoch[39] Batch[0] avg_epoch_loss=4.320945\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:59 INFO 139919854057280] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=4.3209452629089355\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:59 INFO 139919854057280] Epoch[39] Batch[5] avg_epoch_loss=4.261473\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:59 INFO 139919854057280] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=4.2614734172821045\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:59 INFO 139919854057280] Epoch[39] Batch [5]#011Speed: 2844.33 samples/sec#011loss=4.261473\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:59 INFO 139919854057280] Epoch[39] Batch[10] avg_epoch_loss=4.393717\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:59 INFO 139919854057280] #quality_metric: host=algo-1, epoch=39, batch=10 train loss <loss>=4.5524087905883786\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:59 INFO 139919854057280] Epoch[39] Batch [10]#011Speed: 2464.35 samples/sec#011loss=4.552409\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:59 INFO 139919854057280] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158839.2125475, \"EndTime\": 1681158839.6209183, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 408.31971168518066, \"count\": 1, \"min\": 408.31971168518066, \"max\": 408.31971168518066}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:59 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1650.278015272434 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:59 INFO 139919854057280] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:59 INFO 139919854057280] #quality_metric: host=algo-1, epoch=39, train loss <loss>=4.393716768784956\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:59 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:59 INFO 139919854057280] Epoch[40] Batch[0] avg_epoch_loss=4.561308\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:59 INFO 139919854057280] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=4.561307907104492\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:59 INFO 139919854057280] Epoch[40] Batch[5] avg_epoch_loss=4.206056\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:59 INFO 139919854057280] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=4.206056316693624\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:33:59 INFO 139919854057280] Epoch[40] Batch [5]#011Speed: 2901.67 samples/sec#011loss=4.206056\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:00 INFO 139919854057280] Epoch[40] Batch[10] avg_epoch_loss=4.467446\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:00 INFO 139919854057280] #quality_metric: host=algo-1, epoch=40, batch=10 train loss <loss>=4.7811126708984375\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:00 INFO 139919854057280] Epoch[40] Batch [10]#011Speed: 2633.93 samples/sec#011loss=4.781113\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:00 INFO 139919854057280] processed a total of 673 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158839.6209877, \"EndTime\": 1681158840.0121825, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 390.7644748687744, \"count\": 1, \"min\": 390.7644748687744, \"max\": 390.7644748687744}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:00 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1721.8332800616813 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:00 INFO 139919854057280] #progress_metric: host=algo-1, completed 10.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:00 INFO 139919854057280] #quality_metric: host=algo-1, epoch=40, train loss <loss>=4.467445568604902\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:00 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:00 INFO 139919854057280] Epoch[41] Batch[0] avg_epoch_loss=4.318315\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:00 INFO 139919854057280] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=4.318315029144287\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:00 INFO 139919854057280] Epoch[41] Batch[5] avg_epoch_loss=4.641882\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:00 INFO 139919854057280] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=4.641881863276164\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:00 INFO 139919854057280] Epoch[41] Batch [5]#011Speed: 2679.71 samples/sec#011loss=4.641882\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:00 INFO 139919854057280] Epoch[41] Batch[10] avg_epoch_loss=4.723885\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:00 INFO 139919854057280] #quality_metric: host=algo-1, epoch=41, batch=10 train loss <loss>=4.822288894653321\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:00 INFO 139919854057280] Epoch[41] Batch [10]#011Speed: 2483.09 samples/sec#011loss=4.822289\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:00 INFO 139919854057280] processed a total of 678 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158840.0122507, \"EndTime\": 1681158840.4190936, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 406.5072536468506, \"count\": 1, \"min\": 406.5072536468506, \"max\": 406.5072536468506}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:00 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1667.4913682956305 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:00 INFO 139919854057280] #progress_metric: host=algo-1, completed 10.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:00 INFO 139919854057280] #quality_metric: host=algo-1, epoch=41, train loss <loss>=4.7238850593566895\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:00 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:00 INFO 139919854057280] Epoch[42] Batch[0] avg_epoch_loss=4.811607\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:00 INFO 139919854057280] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=4.811607360839844\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:00 INFO 139919854057280] Epoch[42] Batch[5] avg_epoch_loss=4.530038\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:00 INFO 139919854057280] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=4.530037959416707\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:00 INFO 139919854057280] Epoch[42] Batch [5]#011Speed: 2890.40 samples/sec#011loss=4.530038\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:00 INFO 139919854057280] Epoch[42] Batch[10] avg_epoch_loss=4.520868\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:00 INFO 139919854057280] #quality_metric: host=algo-1, epoch=42, batch=10 train loss <loss>=4.509864711761475\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:00 INFO 139919854057280] Epoch[42] Batch [10]#011Speed: 2863.99 samples/sec#011loss=4.509865\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:00 INFO 139919854057280] processed a total of 669 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158840.419158, \"EndTime\": 1681158840.7916493, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 372.161865234375, \"count\": 1, \"min\": 372.161865234375, \"max\": 372.161865234375}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:00 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1797.0753391143958 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:00 INFO 139919854057280] #progress_metric: host=algo-1, completed 10.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:00 INFO 139919854057280] #quality_metric: host=algo-1, epoch=42, train loss <loss>=4.520868301391602\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:00 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:00 INFO 139919854057280] Epoch[43] Batch[0] avg_epoch_loss=4.231976\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:00 INFO 139919854057280] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=4.23197603225708\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:01 INFO 139919854057280] Epoch[43] Batch[5] avg_epoch_loss=4.294934\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:01 INFO 139919854057280] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=4.294934352238973\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:01 INFO 139919854057280] Epoch[43] Batch [5]#011Speed: 3017.34 samples/sec#011loss=4.294934\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:01 INFO 139919854057280] Epoch[43] Batch[10] avg_epoch_loss=4.325867\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:01 INFO 139919854057280] #quality_metric: host=algo-1, epoch=43, batch=10 train loss <loss>=4.362985897064209\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:01 INFO 139919854057280] Epoch[43] Batch [10]#011Speed: 2947.15 samples/sec#011loss=4.362986\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:01 INFO 139919854057280] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158840.79173, \"EndTime\": 1681158841.1552246, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 363.13939094543457, \"count\": 1, \"min\": 363.13939094543457, \"max\": 363.13939094543457}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:01 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1781.2031723715138 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:01 INFO 139919854057280] #progress_metric: host=algo-1, completed 11.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:01 INFO 139919854057280] #quality_metric: host=algo-1, epoch=43, train loss <loss>=4.32586687261408\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:01 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:01 INFO 139919854057280] Epoch[44] Batch[0] avg_epoch_loss=4.604634\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:01 INFO 139919854057280] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=4.6046342849731445\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:01 INFO 139919854057280] Epoch[44] Batch[5] avg_epoch_loss=4.526075\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:01 INFO 139919854057280] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=4.526074727376302\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:01 INFO 139919854057280] Epoch[44] Batch [5]#011Speed: 2156.35 samples/sec#011loss=4.526075\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:01 INFO 139919854057280] Epoch[44] Batch[10] avg_epoch_loss=4.453227\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:01 INFO 139919854057280] #quality_metric: host=algo-1, epoch=44, batch=10 train loss <loss>=4.3658106327056885\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:01 INFO 139919854057280] Epoch[44] Batch [10]#011Speed: 2478.55 samples/sec#011loss=4.365811\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:01 INFO 139919854057280] processed a total of 682 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158841.1552916, \"EndTime\": 1681158841.569832, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 414.1583442687988, \"count\": 1, \"min\": 414.1583442687988, \"max\": 414.1583442687988}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:01 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1646.3966111059951 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:01 INFO 139919854057280] #progress_metric: host=algo-1, completed 11.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:01 INFO 139919854057280] #quality_metric: host=algo-1, epoch=44, train loss <loss>=4.453227411616933\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:01 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:01 INFO 139919854057280] Epoch[45] Batch[0] avg_epoch_loss=4.271695\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:01 INFO 139919854057280] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=4.271695137023926\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:01 INFO 139919854057280] Epoch[45] Batch[5] avg_epoch_loss=4.191153\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:01 INFO 139919854057280] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=4.1911531289418535\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:01 INFO 139919854057280] Epoch[45] Batch [5]#011Speed: 2339.18 samples/sec#011loss=4.191153\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:01 INFO 139919854057280] Epoch[45] Batch[10] avg_epoch_loss=4.174029\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:01 INFO 139919854057280] #quality_metric: host=algo-1, epoch=45, batch=10 train loss <loss>=4.153480195999146\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:01 INFO 139919854057280] Epoch[45] Batch [10]#011Speed: 2498.09 samples/sec#011loss=4.153480\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:01 INFO 139919854057280] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158841.5698824, \"EndTime\": 1681158841.9664478, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 396.2419033050537, \"count\": 1, \"min\": 396.2419033050537, \"max\": 396.2419033050537}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:01 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1700.0748091373518 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:01 INFO 139919854057280] #progress_metric: host=algo-1, completed 11.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:01 INFO 139919854057280] #quality_metric: host=algo-1, epoch=45, train loss <loss>=4.17402906851335\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:01 INFO 139919854057280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:01 INFO 139919854057280] Saved checkpoint to \"/opt/ml/model/state_e8f2bc99-6377-41eb-974a-26aea0366465-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158841.9665296, \"EndTime\": 1681158841.9745402, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 7.29060173034668, \"count\": 1, \"min\": 7.29060173034668, \"max\": 7.29060173034668}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:02 INFO 139919854057280] Epoch[46] Batch[0] avg_epoch_loss=3.962984\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:02 INFO 139919854057280] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=3.962984323501587\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:02 INFO 139919854057280] Epoch[46] Batch[5] avg_epoch_loss=4.153493\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:02 INFO 139919854057280] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=4.1534929275512695\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:02 INFO 139919854057280] Epoch[46] Batch [5]#011Speed: 2616.56 samples/sec#011loss=4.153493\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:02 INFO 139919854057280] Epoch[46] Batch[10] avg_epoch_loss=4.368727\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:02 INFO 139919854057280] #quality_metric: host=algo-1, epoch=46, batch=10 train loss <loss>=4.627008152008057\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:02 INFO 139919854057280] Epoch[46] Batch [10]#011Speed: 2500.55 samples/sec#011loss=4.627008\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:02 INFO 139919854057280] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158841.9748662, \"EndTime\": 1681158842.3575737, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 382.5674057006836, \"count\": 1, \"min\": 382.5674057006836, \"max\": 382.5674057006836}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:02 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1724.6584407776486 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:02 INFO 139919854057280] #progress_metric: host=algo-1, completed 11.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:02 INFO 139919854057280] #quality_metric: host=algo-1, epoch=46, train loss <loss>=4.368727120486173\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:02 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:02 INFO 139919854057280] Epoch[47] Batch[0] avg_epoch_loss=4.457183\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:02 INFO 139919854057280] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=4.457182884216309\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:02 INFO 139919854057280] Epoch[47] Batch[5] avg_epoch_loss=4.461228\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:02 INFO 139919854057280] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=4.461228211720784\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:02 INFO 139919854057280] Epoch[47] Batch [5]#011Speed: 2901.72 samples/sec#011loss=4.461228\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:02 INFO 139919854057280] Epoch[47] Batch[10] avg_epoch_loss=4.556655\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:02 INFO 139919854057280] #quality_metric: host=algo-1, epoch=47, batch=10 train loss <loss>=4.671166324615479\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:02 INFO 139919854057280] Epoch[47] Batch [10]#011Speed: 2217.84 samples/sec#011loss=4.671166\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:02 INFO 139919854057280] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158842.3576586, \"EndTime\": 1681158842.7497828, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 391.4320468902588, \"count\": 1, \"min\": 391.4320468902588, \"max\": 391.4320468902588}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:02 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1683.1413662811055 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:02 INFO 139919854057280] #progress_metric: host=algo-1, completed 12.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:02 INFO 139919854057280] #quality_metric: host=algo-1, epoch=47, train loss <loss>=4.556654626672918\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:02 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:02 INFO 139919854057280] Epoch[48] Batch[0] avg_epoch_loss=4.485804\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:02 INFO 139919854057280] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=4.485803604125977\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:03 INFO 139919854057280] Epoch[48] Batch[5] avg_epoch_loss=4.383242\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:03 INFO 139919854057280] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=4.383242050806682\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:03 INFO 139919854057280] Epoch[48] Batch [5]#011Speed: 2185.00 samples/sec#011loss=4.383242\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:03 INFO 139919854057280] Epoch[48] Batch[10] avg_epoch_loss=4.348495\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:03 INFO 139919854057280] #quality_metric: host=algo-1, epoch=48, batch=10 train loss <loss>=4.306799125671387\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:03 INFO 139919854057280] Epoch[48] Batch [10]#011Speed: 2045.20 samples/sec#011loss=4.306799\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:03 INFO 139919854057280] processed a total of 704 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158842.7498434, \"EndTime\": 1681158843.235575, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 485.4238033294678, \"count\": 1, \"min\": 485.4238033294678, \"max\": 485.4238033294678}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:03 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1449.9529411880246 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:03 INFO 139919854057280] #progress_metric: host=algo-1, completed 12.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:03 INFO 139919854057280] #quality_metric: host=algo-1, epoch=48, train loss <loss>=4.348495266654274\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:03 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:03 INFO 139919854057280] Epoch[49] Batch[0] avg_epoch_loss=4.301731\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:03 INFO 139919854057280] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=4.301731109619141\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:03 INFO 139919854057280] Epoch[49] Batch[5] avg_epoch_loss=4.385572\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:03 INFO 139919854057280] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=4.385571718215942\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:03 INFO 139919854057280] Epoch[49] Batch [5]#011Speed: 1814.22 samples/sec#011loss=4.385572\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:03 INFO 139919854057280] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158843.2356517, \"EndTime\": 1681158843.7539947, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 517.9100036621094, \"count\": 1, \"min\": 517.9100036621094, \"max\": 517.9100036621094}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:03 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1231.0955656992905 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:03 INFO 139919854057280] #progress_metric: host=algo-1, completed 12.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:03 INFO 139919854057280] #quality_metric: host=algo-1, epoch=49, train loss <loss>=4.392346525192261\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:03 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:03 INFO 139919854057280] Epoch[50] Batch[0] avg_epoch_loss=4.331192\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:03 INFO 139919854057280] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=4.3311920166015625\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:04 INFO 139919854057280] Epoch[50] Batch[5] avg_epoch_loss=4.387030\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:04 INFO 139919854057280] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=4.387030442555745\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:04 INFO 139919854057280] Epoch[50] Batch [5]#011Speed: 2998.42 samples/sec#011loss=4.387030\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:04 INFO 139919854057280] Epoch[50] Batch[10] avg_epoch_loss=4.477514\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:04 INFO 139919854057280] #quality_metric: host=algo-1, epoch=50, batch=10 train loss <loss>=4.586094093322754\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:04 INFO 139919854057280] Epoch[50] Batch [10]#011Speed: 2903.50 samples/sec#011loss=4.586094\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:04 INFO 139919854057280] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158843.7542856, \"EndTime\": 1681158844.1310682, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 375.8113384246826, \"count\": 1, \"min\": 375.8113384246826, \"max\": 375.8113384246826}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:04 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1761.080845482418 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:04 INFO 139919854057280] #progress_metric: host=algo-1, completed 12.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:04 INFO 139919854057280] #quality_metric: host=algo-1, epoch=50, train loss <loss>=4.477513920177113\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:04 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:04 INFO 139919854057280] Epoch[51] Batch[0] avg_epoch_loss=4.315481\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:04 INFO 139919854057280] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=4.315481185913086\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:04 INFO 139919854057280] Epoch[51] Batch[5] avg_epoch_loss=4.217989\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:04 INFO 139919854057280] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=4.217989444732666\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:04 INFO 139919854057280] Epoch[51] Batch [5]#011Speed: 3017.18 samples/sec#011loss=4.217989\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:04 INFO 139919854057280] Epoch[51] Batch[10] avg_epoch_loss=4.269405\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:04 INFO 139919854057280] #quality_metric: host=algo-1, epoch=51, batch=10 train loss <loss>=4.331104469299317\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:04 INFO 139919854057280] Epoch[51] Batch [10]#011Speed: 2810.10 samples/sec#011loss=4.331104\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:04 INFO 139919854057280] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158844.1311338, \"EndTime\": 1681158844.5099487, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 378.2951831817627, \"count\": 1, \"min\": 378.2951831817627, \"max\": 378.2951831817627}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:04 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1765.355095457123 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:04 INFO 139919854057280] #progress_metric: host=algo-1, completed 13.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:04 INFO 139919854057280] #quality_metric: host=algo-1, epoch=51, train loss <loss>=4.269405364990234\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:04 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:04 INFO 139919854057280] Epoch[52] Batch[0] avg_epoch_loss=4.375974\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:04 INFO 139919854057280] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=4.375974178314209\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:04 INFO 139919854057280] Epoch[52] Batch[5] avg_epoch_loss=4.268928\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:04 INFO 139919854057280] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=4.268927812576294\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:04 INFO 139919854057280] Epoch[52] Batch [5]#011Speed: 3043.69 samples/sec#011loss=4.268928\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:04 INFO 139919854057280] Epoch[52] Batch[10] avg_epoch_loss=4.297918\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:04 INFO 139919854057280] #quality_metric: host=algo-1, epoch=52, batch=10 train loss <loss>=4.332706069946289\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:04 INFO 139919854057280] Epoch[52] Batch [10]#011Speed: 2975.94 samples/sec#011loss=4.332706\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:04 INFO 139919854057280] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158844.5100176, \"EndTime\": 1681158844.8828666, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 372.5130558013916, \"count\": 1, \"min\": 372.5130558013916, \"max\": 372.5130558013916}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:04 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1728.3637308311993 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:04 INFO 139919854057280] #progress_metric: host=algo-1, completed 13.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:04 INFO 139919854057280] #quality_metric: host=algo-1, epoch=52, train loss <loss>=4.297917929562655\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:04 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:05 INFO 139919854057280] Epoch[53] Batch[0] avg_epoch_loss=5.693748\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:05 INFO 139919854057280] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=5.693747520446777\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:05 INFO 139919854057280] Epoch[53] Batch[5] avg_epoch_loss=4.998776\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:05 INFO 139919854057280] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=4.998775959014893\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:05 INFO 139919854057280] Epoch[53] Batch [5]#011Speed: 3048.03 samples/sec#011loss=4.998776\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:05 INFO 139919854057280] Epoch[53] Batch[10] avg_epoch_loss=4.944653\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:05 INFO 139919854057280] #quality_metric: host=algo-1, epoch=53, batch=10 train loss <loss>=4.8797060489654545\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:05 INFO 139919854057280] Epoch[53] Batch [10]#011Speed: 2871.11 samples/sec#011loss=4.879706\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:05 INFO 139919854057280] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158844.8829317, \"EndTime\": 1681158845.2581718, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 374.86886978149414, \"count\": 1, \"min\": 374.86886978149414, \"max\": 374.86886978149414}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:05 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1789.4857884061473 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:05 INFO 139919854057280] #progress_metric: host=algo-1, completed 13.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:05 INFO 139919854057280] #quality_metric: host=algo-1, epoch=53, train loss <loss>=4.944653272628784\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:05 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:05 INFO 139919854057280] Epoch[54] Batch[0] avg_epoch_loss=4.314200\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:05 INFO 139919854057280] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=4.314200401306152\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:05 INFO 139919854057280] Epoch[54] Batch[5] avg_epoch_loss=4.315748\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:05 INFO 139919854057280] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=4.31574821472168\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:05 INFO 139919854057280] Epoch[54] Batch [5]#011Speed: 3025.93 samples/sec#011loss=4.315748\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:05 INFO 139919854057280] Epoch[54] Batch[10] avg_epoch_loss=4.517774\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:05 INFO 139919854057280] #quality_metric: host=algo-1, epoch=54, batch=10 train loss <loss>=4.760204410552978\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:05 INFO 139919854057280] Epoch[54] Batch [10]#011Speed: 2747.72 samples/sec#011loss=4.760204\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:05 INFO 139919854057280] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158845.2582397, \"EndTime\": 1681158845.6069796, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 348.36268424987793, \"count\": 1, \"min\": 348.36268424987793, \"max\": 348.36268424987793}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:05 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1913.8577933450088 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:05 INFO 139919854057280] #progress_metric: host=algo-1, completed 13.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:05 INFO 139919854057280] #quality_metric: host=algo-1, epoch=54, train loss <loss>=4.517773758281361\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:05 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:05 INFO 139919854057280] Epoch[55] Batch[0] avg_epoch_loss=4.435225\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:05 INFO 139919854057280] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=4.435225486755371\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:05 INFO 139919854057280] Epoch[55] Batch[5] avg_epoch_loss=4.479379\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:05 INFO 139919854057280] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=4.479378859202067\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:05 INFO 139919854057280] Epoch[55] Batch [5]#011Speed: 3071.28 samples/sec#011loss=4.479379\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:05 INFO 139919854057280] Epoch[55] Batch[10] avg_epoch_loss=4.391780\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:05 INFO 139919854057280] #quality_metric: host=algo-1, epoch=55, batch=10 train loss <loss>=4.286660814285279\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:05 INFO 139919854057280] Epoch[55] Batch [10]#011Speed: 2723.24 samples/sec#011loss=4.286661\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:05 INFO 139919854057280] processed a total of 683 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158845.6070976, \"EndTime\": 1681158845.9855611, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 377.9170513153076, \"count\": 1, \"min\": 377.9170513153076, \"max\": 377.9170513153076}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:05 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1806.8292023286176 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:05 INFO 139919854057280] #progress_metric: host=algo-1, completed 14.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:05 INFO 139919854057280] #quality_metric: host=algo-1, epoch=55, train loss <loss>=4.391779747876254\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:05 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:06 INFO 139919854057280] Epoch[56] Batch[0] avg_epoch_loss=4.101151\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:06 INFO 139919854057280] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=4.101150989532471\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:06 INFO 139919854057280] Epoch[56] Batch[5] avg_epoch_loss=4.239053\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:06 INFO 139919854057280] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=4.239052693049113\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:06 INFO 139919854057280] Epoch[56] Batch [5]#011Speed: 2918.21 samples/sec#011loss=4.239053\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:06 INFO 139919854057280] Epoch[56] Batch[10] avg_epoch_loss=4.196188\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:06 INFO 139919854057280] #quality_metric: host=algo-1, epoch=56, batch=10 train loss <loss>=4.144749546051026\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:06 INFO 139919854057280] Epoch[56] Batch [10]#011Speed: 2616.23 samples/sec#011loss=4.144750\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:06 INFO 139919854057280] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158845.9856272, \"EndTime\": 1681158846.353105, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 367.1865463256836, \"count\": 1, \"min\": 367.1865463256836, \"max\": 367.1865463256836}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:06 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1769.758862596406 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:06 INFO 139919854057280] #progress_metric: host=algo-1, completed 14.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:06 INFO 139919854057280] #quality_metric: host=algo-1, epoch=56, train loss <loss>=4.196187626231801\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:06 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:06 INFO 139919854057280] Epoch[57] Batch[0] avg_epoch_loss=4.451437\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:06 INFO 139919854057280] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=4.451436996459961\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:06 INFO 139919854057280] Epoch[57] Batch[5] avg_epoch_loss=4.322778\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:06 INFO 139919854057280] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=4.322777509689331\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:06 INFO 139919854057280] Epoch[57] Batch [5]#011Speed: 2769.30 samples/sec#011loss=4.322778\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:06 INFO 139919854057280] Epoch[57] Batch[10] avg_epoch_loss=4.391513\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:06 INFO 139919854057280] #quality_metric: host=algo-1, epoch=57, batch=10 train loss <loss>=4.473994636535645\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:06 INFO 139919854057280] Epoch[57] Batch [10]#011Speed: 2686.40 samples/sec#011loss=4.473995\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:06 INFO 139919854057280] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158846.3531718, \"EndTime\": 1681158846.73169, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 378.1447410583496, \"count\": 1, \"min\": 378.1447410583496, \"max\": 378.1447410583496}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:06 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1734.3382253478335 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:06 INFO 139919854057280] #progress_metric: host=algo-1, completed 14.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:06 INFO 139919854057280] #quality_metric: host=algo-1, epoch=57, train loss <loss>=4.391512567346746\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:06 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:06 INFO 139919854057280] Epoch[58] Batch[0] avg_epoch_loss=4.661872\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:06 INFO 139919854057280] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=4.661872386932373\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:06 INFO 139919854057280] Epoch[58] Batch[5] avg_epoch_loss=4.541659\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:06 INFO 139919854057280] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=4.541658798853557\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:06 INFO 139919854057280] Epoch[58] Batch [5]#011Speed: 2997.11 samples/sec#011loss=4.541659\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:07 INFO 139919854057280] Epoch[58] Batch[10] avg_epoch_loss=4.376715\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:07 INFO 139919854057280] #quality_metric: host=algo-1, epoch=58, batch=10 train loss <loss>=4.178783321380616\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:07 INFO 139919854057280] Epoch[58] Batch [10]#011Speed: 2678.19 samples/sec#011loss=4.178783\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:07 INFO 139919854057280] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158846.731754, \"EndTime\": 1681158847.0960448, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 363.8937473297119, \"count\": 1, \"min\": 363.8937473297119, \"max\": 363.8937473297119}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:07 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1824.0917790642764 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:07 INFO 139919854057280] #progress_metric: host=algo-1, completed 14.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:07 INFO 139919854057280] #quality_metric: host=algo-1, epoch=58, train loss <loss>=4.37671540000222\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:07 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:07 INFO 139919854057280] Epoch[59] Batch[0] avg_epoch_loss=4.063238\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:07 INFO 139919854057280] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=4.06323766708374\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:07 INFO 139919854057280] Epoch[59] Batch[5] avg_epoch_loss=4.186190\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:07 INFO 139919854057280] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=4.186190207799275\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:07 INFO 139919854057280] Epoch[59] Batch [5]#011Speed: 2988.57 samples/sec#011loss=4.186190\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:07 INFO 139919854057280] Epoch[59] Batch[10] avg_epoch_loss=4.301940\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:07 INFO 139919854057280] #quality_metric: host=algo-1, epoch=59, batch=10 train loss <loss>=4.440839099884033\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:07 INFO 139919854057280] Epoch[59] Batch [10]#011Speed: 2602.02 samples/sec#011loss=4.440839\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:07 INFO 139919854057280] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158847.096132, \"EndTime\": 1681158847.46311, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 366.5592670440674, \"count\": 1, \"min\": 366.5592670440674, \"max\": 366.5592670440674}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:07 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1838.2421402194348 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:07 INFO 139919854057280] #progress_metric: host=algo-1, completed 15.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:07 INFO 139919854057280] #quality_metric: host=algo-1, epoch=59, train loss <loss>=4.3019397042014385\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:07 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:07 INFO 139919854057280] Epoch[60] Batch[0] avg_epoch_loss=4.925929\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:07 INFO 139919854057280] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=4.925928592681885\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:07 INFO 139919854057280] Epoch[60] Batch[5] avg_epoch_loss=4.453897\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:07 INFO 139919854057280] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=4.45389723777771\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:07 INFO 139919854057280] Epoch[60] Batch [5]#011Speed: 2875.75 samples/sec#011loss=4.453897\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:07 INFO 139919854057280] Epoch[60] Batch[10] avg_epoch_loss=4.456766\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:07 INFO 139919854057280] #quality_metric: host=algo-1, epoch=60, batch=10 train loss <loss>=4.460208129882813\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:07 INFO 139919854057280] Epoch[60] Batch [10]#011Speed: 2689.79 samples/sec#011loss=4.460208\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:07 INFO 139919854057280] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158847.4631755, \"EndTime\": 1681158847.8205254, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 357.04970359802246, \"count\": 1, \"min\": 357.04970359802246, \"max\": 357.04970359802246}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:07 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1853.5959342282783 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:07 INFO 139919854057280] #progress_metric: host=algo-1, completed 15.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:07 INFO 139919854057280] #quality_metric: host=algo-1, epoch=60, train loss <loss>=4.456765825098211\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:07 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:07 INFO 139919854057280] Epoch[61] Batch[0] avg_epoch_loss=5.061234\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:07 INFO 139919854057280] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=5.0612335205078125\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:08 INFO 139919854057280] Epoch[61] Batch[5] avg_epoch_loss=4.573954\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:08 INFO 139919854057280] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=4.573954105377197\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:08 INFO 139919854057280] Epoch[61] Batch [5]#011Speed: 3015.25 samples/sec#011loss=4.573954\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:08 INFO 139919854057280] Epoch[61] Batch[10] avg_epoch_loss=4.505410\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:08 INFO 139919854057280] #quality_metric: host=algo-1, epoch=61, batch=10 train loss <loss>=4.423157978057861\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:08 INFO 139919854057280] Epoch[61] Batch [10]#011Speed: 2533.95 samples/sec#011loss=4.423158\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:08 INFO 139919854057280] processed a total of 729 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158847.8205912, \"EndTime\": 1681158848.2306135, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 409.7023010253906, \"count\": 1, \"min\": 409.7023010253906, \"max\": 409.7023010253906}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:08 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1778.070121490262 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:08 INFO 139919854057280] #progress_metric: host=algo-1, completed 15.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:08 INFO 139919854057280] #quality_metric: host=algo-1, epoch=61, train loss <loss>=4.537785092989604\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:08 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:08 INFO 139919854057280] Epoch[62] Batch[0] avg_epoch_loss=3.943166\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:08 INFO 139919854057280] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=3.9431662559509277\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:08 INFO 139919854057280] Epoch[62] Batch[5] avg_epoch_loss=4.314898\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:08 INFO 139919854057280] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=4.314897855122884\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:08 INFO 139919854057280] Epoch[62] Batch [5]#011Speed: 2975.18 samples/sec#011loss=4.314898\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:08 INFO 139919854057280] Epoch[62] Batch[10] avg_epoch_loss=4.302140\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:08 INFO 139919854057280] #quality_metric: host=algo-1, epoch=62, batch=10 train loss <loss>=4.286830997467041\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:08 INFO 139919854057280] Epoch[62] Batch [10]#011Speed: 2567.57 samples/sec#011loss=4.286831\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:08 INFO 139919854057280] processed a total of 696 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158848.230874, \"EndTime\": 1681158848.6226263, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 391.3397789001465, \"count\": 1, \"min\": 391.3397789001465, \"max\": 391.3397789001465}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:08 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1778.0181746422325 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:08 INFO 139919854057280] #progress_metric: host=algo-1, completed 15.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:08 INFO 139919854057280] #quality_metric: host=algo-1, epoch=62, train loss <loss>=4.302140192552046\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:08 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:08 INFO 139919854057280] Epoch[63] Batch[0] avg_epoch_loss=4.353903\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:08 INFO 139919854057280] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=4.353903293609619\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:08 INFO 139919854057280] Epoch[63] Batch[5] avg_epoch_loss=4.344787\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:08 INFO 139919854057280] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=4.344786802927653\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:08 INFO 139919854057280] Epoch[63] Batch [5]#011Speed: 2587.04 samples/sec#011loss=4.344787\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:09 INFO 139919854057280] Epoch[63] Batch[10] avg_epoch_loss=4.143217\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:09 INFO 139919854057280] #quality_metric: host=algo-1, epoch=63, batch=10 train loss <loss>=3.901333951950073\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:09 INFO 139919854057280] Epoch[63] Batch [10]#011Speed: 2510.14 samples/sec#011loss=3.901334\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:09 INFO 139919854057280] processed a total of 684 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158848.6227, \"EndTime\": 1681158849.0009077, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 377.7780532836914, \"count\": 1, \"min\": 377.7780532836914, \"max\": 377.7780532836914}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:09 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1810.0614368924198 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:09 INFO 139919854057280] #progress_metric: host=algo-1, completed 16.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:09 INFO 139919854057280] #quality_metric: host=algo-1, epoch=63, train loss <loss>=4.143217325210571\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:09 INFO 139919854057280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:09 INFO 139919854057280] Saved checkpoint to \"/opt/ml/model/state_207ace05-8792-4005-9d15-8189fa4f66ac-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158849.0009851, \"EndTime\": 1681158849.0081728, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 6.669044494628906, \"count\": 1, \"min\": 6.669044494628906, \"max\": 6.669044494628906}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:09 INFO 139919854057280] Epoch[64] Batch[0] avg_epoch_loss=4.476755\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:09 INFO 139919854057280] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=4.476754665374756\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:09 INFO 139919854057280] Epoch[64] Batch[5] avg_epoch_loss=4.334816\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:09 INFO 139919854057280] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=4.334815899531047\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:09 INFO 139919854057280] Epoch[64] Batch [5]#011Speed: 2937.71 samples/sec#011loss=4.334816\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:09 INFO 139919854057280] Epoch[64] Batch[10] avg_epoch_loss=4.343199\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:09 INFO 139919854057280] #quality_metric: host=algo-1, epoch=64, batch=10 train loss <loss>=4.353258752822876\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:09 INFO 139919854057280] Epoch[64] Batch [10]#011Speed: 2628.34 samples/sec#011loss=4.353259\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:09 INFO 139919854057280] processed a total of 702 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158849.0082636, \"EndTime\": 1681158849.3639574, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 355.64112663269043, \"count\": 1, \"min\": 355.64112663269043, \"max\": 355.64112663269043}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:09 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1973.366804506491 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:09 INFO 139919854057280] #progress_metric: host=algo-1, completed 16.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:09 INFO 139919854057280] #quality_metric: host=algo-1, epoch=64, train loss <loss>=4.343199014663696\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:09 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:09 INFO 139919854057280] Epoch[65] Batch[0] avg_epoch_loss=4.200471\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:09 INFO 139919854057280] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=4.200470924377441\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:09 INFO 139919854057280] Epoch[65] Batch[5] avg_epoch_loss=4.238926\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:09 INFO 139919854057280] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=4.2389258941014605\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:09 INFO 139919854057280] Epoch[65] Batch [5]#011Speed: 2855.31 samples/sec#011loss=4.238926\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:09 INFO 139919854057280] Epoch[65] Batch[10] avg_epoch_loss=4.287647\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:09 INFO 139919854057280] #quality_metric: host=algo-1, epoch=65, batch=10 train loss <loss>=4.346112251281738\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:09 INFO 139919854057280] Epoch[65] Batch [10]#011Speed: 2651.46 samples/sec#011loss=4.346112\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:09 INFO 139919854057280] processed a total of 683 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158849.3640256, \"EndTime\": 1681158849.754168, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 389.8334503173828, \"count\": 1, \"min\": 389.8334503173828, \"max\": 389.8334503173828}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:09 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1751.5674797861466 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:09 INFO 139919854057280] #progress_metric: host=algo-1, completed 16.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:09 INFO 139919854057280] #quality_metric: host=algo-1, epoch=65, train loss <loss>=4.287646965547041\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:09 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:09 INFO 139919854057280] Epoch[66] Batch[0] avg_epoch_loss=4.423486\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:09 INFO 139919854057280] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=4.42348575592041\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:09 INFO 139919854057280] Epoch[66] Batch[5] avg_epoch_loss=4.354592\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:09 INFO 139919854057280] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=4.354592243830363\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:09 INFO 139919854057280] Epoch[66] Batch [5]#011Speed: 2920.47 samples/sec#011loss=4.354592\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:10 INFO 139919854057280] Epoch[66] Batch[10] avg_epoch_loss=4.169938\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:10 INFO 139919854057280] #quality_metric: host=algo-1, epoch=66, batch=10 train loss <loss>=3.948352813720703\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:10 INFO 139919854057280] Epoch[66] Batch [10]#011Speed: 2643.75 samples/sec#011loss=3.948353\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:10 INFO 139919854057280] processed a total of 682 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158849.7542412, \"EndTime\": 1681158850.110268, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 355.5865287780762, \"count\": 1, \"min\": 355.5865287780762, \"max\": 355.5865287780762}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:10 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1917.3794064970025 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:10 INFO 139919854057280] #progress_metric: host=algo-1, completed 16.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:10 INFO 139919854057280] #quality_metric: host=algo-1, epoch=66, train loss <loss>=4.169937957416881\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:10 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:10 INFO 139919854057280] Epoch[67] Batch[0] avg_epoch_loss=4.304964\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:10 INFO 139919854057280] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=4.304964065551758\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:10 INFO 139919854057280] Epoch[67] Batch[5] avg_epoch_loss=4.243611\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:10 INFO 139919854057280] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=4.243611176808675\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:10 INFO 139919854057280] Epoch[67] Batch [5]#011Speed: 2482.13 samples/sec#011loss=4.243611\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:10 INFO 139919854057280] Epoch[67] Batch[10] avg_epoch_loss=4.285413\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:10 INFO 139919854057280] #quality_metric: host=algo-1, epoch=67, batch=10 train loss <loss>=4.335574150085449\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:10 INFO 139919854057280] Epoch[67] Batch [10]#011Speed: 2545.20 samples/sec#011loss=4.335574\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:10 INFO 139919854057280] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158850.110342, \"EndTime\": 1681158850.4983525, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 387.54963874816895, \"count\": 1, \"min\": 387.54963874816895, \"max\": 387.54963874816895}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:10 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1723.190085704533 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:10 INFO 139919854057280] #progress_metric: host=algo-1, completed 17.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:10 INFO 139919854057280] #quality_metric: host=algo-1, epoch=67, train loss <loss>=4.285412528298118\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:10 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:10 INFO 139919854057280] Epoch[68] Batch[0] avg_epoch_loss=4.055288\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:10 INFO 139919854057280] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=4.055287837982178\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:10 INFO 139919854057280] Epoch[68] Batch[5] avg_epoch_loss=4.257645\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:10 INFO 139919854057280] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=4.257645050684611\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:10 INFO 139919854057280] Epoch[68] Batch [5]#011Speed: 2391.25 samples/sec#011loss=4.257645\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:10 INFO 139919854057280] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158850.4984236, \"EndTime\": 1681158850.8572857, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 358.3333492279053, \"count\": 1, \"min\": 358.3333492279053, \"max\": 358.3333492279053}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:10 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1746.4758339486095 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:10 INFO 139919854057280] #progress_metric: host=algo-1, completed 17.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:10 INFO 139919854057280] #quality_metric: host=algo-1, epoch=68, train loss <loss>=4.295541143417358\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:10 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:10 INFO 139919854057280] Epoch[69] Batch[0] avg_epoch_loss=4.401926\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:10 INFO 139919854057280] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=4.401926040649414\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:11 INFO 139919854057280] Epoch[69] Batch[5] avg_epoch_loss=4.267007\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:11 INFO 139919854057280] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=4.267006556193034\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:11 INFO 139919854057280] Epoch[69] Batch [5]#011Speed: 2391.04 samples/sec#011loss=4.267007\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:11 INFO 139919854057280] Epoch[69] Batch[10] avg_epoch_loss=4.176724\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:11 INFO 139919854057280] #quality_metric: host=algo-1, epoch=69, batch=10 train loss <loss>=4.068384027481079\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:11 INFO 139919854057280] Epoch[69] Batch [10]#011Speed: 2682.99 samples/sec#011loss=4.068384\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:11 INFO 139919854057280] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158850.857356, \"EndTime\": 1681158851.2445302, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 386.5535259246826, \"count\": 1, \"min\": 386.5535259246826, \"max\": 386.5535259246826}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:11 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1673.1980173540098 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:11 INFO 139919854057280] #progress_metric: host=algo-1, completed 17.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:11 INFO 139919854057280] #quality_metric: host=algo-1, epoch=69, train loss <loss>=4.1767235885966905\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:11 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:11 INFO 139919854057280] Epoch[70] Batch[0] avg_epoch_loss=4.607312\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:11 INFO 139919854057280] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=4.607311725616455\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:11 INFO 139919854057280] Epoch[70] Batch[5] avg_epoch_loss=4.429992\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:11 INFO 139919854057280] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=4.429992198944092\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:11 INFO 139919854057280] Epoch[70] Batch [5]#011Speed: 2918.22 samples/sec#011loss=4.429992\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:11 INFO 139919854057280] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158851.244628, \"EndTime\": 1681158851.5988157, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 353.7619113922119, \"count\": 1, \"min\": 353.7619113922119, \"max\": 353.7619113922119}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:11 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1769.0402489128926 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:11 INFO 139919854057280] #progress_metric: host=algo-1, completed 17.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:11 INFO 139919854057280] #quality_metric: host=algo-1, epoch=70, train loss <loss>=4.413091373443604\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:11 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:11 INFO 139919854057280] Epoch[71] Batch[0] avg_epoch_loss=4.113144\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:11 INFO 139919854057280] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=4.1131439208984375\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:11 INFO 139919854057280] Epoch[71] Batch[5] avg_epoch_loss=4.280211\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:11 INFO 139919854057280] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=4.280211130777995\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:11 INFO 139919854057280] Epoch[71] Batch [5]#011Speed: 2648.19 samples/sec#011loss=4.280211\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:11 INFO 139919854057280] Epoch[71] Batch[10] avg_epoch_loss=4.306021\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:11 INFO 139919854057280] #quality_metric: host=algo-1, epoch=71, batch=10 train loss <loss>=4.336993312835693\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:11 INFO 139919854057280] Epoch[71] Batch [10]#011Speed: 2660.32 samples/sec#011loss=4.336993\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:11 INFO 139919854057280] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158851.5988882, \"EndTime\": 1681158851.971931, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 372.5087642669678, \"count\": 1, \"min\": 372.5087642669678, \"max\": 372.5087642669678}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:11 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1768.628571830775 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:11 INFO 139919854057280] #progress_metric: host=algo-1, completed 18.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:11 INFO 139919854057280] #quality_metric: host=algo-1, epoch=71, train loss <loss>=4.306021213531494\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:11 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:12 INFO 139919854057280] Epoch[72] Batch[0] avg_epoch_loss=4.147211\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:12 INFO 139919854057280] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=4.147210597991943\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:12 INFO 139919854057280] Epoch[72] Batch[5] avg_epoch_loss=4.206944\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:12 INFO 139919854057280] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=4.20694375038147\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:12 INFO 139919854057280] Epoch[72] Batch [5]#011Speed: 3035.21 samples/sec#011loss=4.206944\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:12 INFO 139919854057280] Epoch[72] Batch[10] avg_epoch_loss=4.278701\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:12 INFO 139919854057280] #quality_metric: host=algo-1, epoch=72, batch=10 train loss <loss>=4.364810466766357\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:12 INFO 139919854057280] Epoch[72] Batch [10]#011Speed: 2315.70 samples/sec#011loss=4.364810\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:12 INFO 139919854057280] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158851.9719958, \"EndTime\": 1681158852.341111, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 368.7117099761963, \"count\": 1, \"min\": 368.7117099761963, \"max\": 368.7117099761963}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:12 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1819.36293613756 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:12 INFO 139919854057280] #progress_metric: host=algo-1, completed 18.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:12 INFO 139919854057280] #quality_metric: host=algo-1, epoch=72, train loss <loss>=4.278701348738237\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:12 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:12 INFO 139919854057280] Epoch[73] Batch[0] avg_epoch_loss=4.019883\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:12 INFO 139919854057280] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=4.019882678985596\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:12 INFO 139919854057280] Epoch[73] Batch[5] avg_epoch_loss=4.144864\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:12 INFO 139919854057280] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=4.144863684972127\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:12 INFO 139919854057280] Epoch[73] Batch [5]#011Speed: 2406.61 samples/sec#011loss=4.144864\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:12 INFO 139919854057280] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158852.3411787, \"EndTime\": 1681158852.7322953, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 390.7034397125244, \"count\": 1, \"min\": 390.7034397125244, \"max\": 390.7034397125244}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:12 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1629.9668963985755 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:12 INFO 139919854057280] #progress_metric: host=algo-1, completed 18.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:12 INFO 139919854057280] #quality_metric: host=algo-1, epoch=73, train loss <loss>=4.143415856361389\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:12 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:12 INFO 139919854057280] Epoch[74] Batch[0] avg_epoch_loss=4.117614\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:12 INFO 139919854057280] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=4.117613792419434\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:13 INFO 139919854057280] Epoch[74] Batch[5] avg_epoch_loss=4.430042\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:13 INFO 139919854057280] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=4.430042028427124\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:13 INFO 139919854057280] Epoch[74] Batch [5]#011Speed: 2883.25 samples/sec#011loss=4.430042\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:13 INFO 139919854057280] Epoch[74] Batch[10] avg_epoch_loss=4.367116\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:13 INFO 139919854057280] #quality_metric: host=algo-1, epoch=74, batch=10 train loss <loss>=4.29160509109497\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:13 INFO 139919854057280] Epoch[74] Batch [10]#011Speed: 2830.25 samples/sec#011loss=4.291605\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:13 INFO 139919854057280] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158852.7323666, \"EndTime\": 1681158853.1369035, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 404.083251953125, \"count\": 1, \"min\": 404.083251953125, \"max\": 404.083251953125}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:13 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1642.7517691172905 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:13 INFO 139919854057280] #progress_metric: host=algo-1, completed 18.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:13 INFO 139919854057280] #quality_metric: host=algo-1, epoch=74, train loss <loss>=4.3671161478216\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:13 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:13 INFO 139919854057280] Epoch[75] Batch[0] avg_epoch_loss=4.199151\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:13 INFO 139919854057280] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=4.199150562286377\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:13 INFO 139919854057280] Epoch[75] Batch[5] avg_epoch_loss=4.211023\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:13 INFO 139919854057280] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=4.211023489634196\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:13 INFO 139919854057280] Epoch[75] Batch [5]#011Speed: 2939.54 samples/sec#011loss=4.211023\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:13 INFO 139919854057280] Epoch[75] Batch[10] avg_epoch_loss=3.978177\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:13 INFO 139919854057280] #quality_metric: host=algo-1, epoch=75, batch=10 train loss <loss>=3.698762226104736\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:13 INFO 139919854057280] Epoch[75] Batch [10]#011Speed: 2768.12 samples/sec#011loss=3.698762\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:13 INFO 139919854057280] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158853.1369908, \"EndTime\": 1681158853.4891164, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 351.6812324523926, \"count\": 1, \"min\": 351.6812324523926, \"max\": 351.6812324523926}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:13 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1884.695889326563 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:13 INFO 139919854057280] #progress_metric: host=algo-1, completed 19.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:13 INFO 139919854057280] #quality_metric: host=algo-1, epoch=75, train loss <loss>=3.978177460757169\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:13 INFO 139919854057280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:13 INFO 139919854057280] Saved checkpoint to \"/opt/ml/model/state_fda314cf-fd87-4ea7-89ac-4a5a2535c5c4-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158853.4891853, \"EndTime\": 1681158853.4996998, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 10.029792785644531, \"count\": 1, \"min\": 10.029792785644531, \"max\": 10.029792785644531}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:13 INFO 139919854057280] Epoch[76] Batch[0] avg_epoch_loss=4.291922\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:13 INFO 139919854057280] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=4.291922092437744\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:13 INFO 139919854057280] Epoch[76] Batch[5] avg_epoch_loss=4.277530\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:13 INFO 139919854057280] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=4.277530272801717\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:13 INFO 139919854057280] Epoch[76] Batch [5]#011Speed: 2367.74 samples/sec#011loss=4.277530\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:13 INFO 139919854057280] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158853.4997663, \"EndTime\": 1681158853.9054868, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 405.6663513183594, \"count\": 1, \"min\": 405.6663513183594, \"max\": 405.6663513183594}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:13 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1564.9109868215019 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:13 INFO 139919854057280] #progress_metric: host=algo-1, completed 19.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:13 INFO 139919854057280] #quality_metric: host=algo-1, epoch=76, train loss <loss>=4.274898290634155\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:13 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:14 INFO 139919854057280] Epoch[77] Batch[0] avg_epoch_loss=4.299143\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:14 INFO 139919854057280] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=4.299143314361572\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:14 INFO 139919854057280] Epoch[77] Batch[5] avg_epoch_loss=4.127061\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:14 INFO 139919854057280] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=4.127061049143474\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:14 INFO 139919854057280] Epoch[77] Batch [5]#011Speed: 2774.61 samples/sec#011loss=4.127061\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:14 INFO 139919854057280] Epoch[77] Batch[10] avg_epoch_loss=4.180913\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:14 INFO 139919854057280] #quality_metric: host=algo-1, epoch=77, batch=10 train loss <loss>=4.245536279678345\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:14 INFO 139919854057280] Epoch[77] Batch [10]#011Speed: 2797.75 samples/sec#011loss=4.245536\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:14 INFO 139919854057280] processed a total of 680 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158853.905562, \"EndTime\": 1681158854.2662613, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 360.1217269897461, \"count\": 1, \"min\": 360.1217269897461, \"max\": 360.1217269897461}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:14 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1887.5143823926292 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:14 INFO 139919854057280] #progress_metric: host=algo-1, completed 19.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:14 INFO 139919854057280] #quality_metric: host=algo-1, epoch=77, train loss <loss>=4.180913426659324\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:14 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:14 INFO 139919854057280] Epoch[78] Batch[0] avg_epoch_loss=4.093539\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:14 INFO 139919854057280] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=4.093538761138916\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:14 INFO 139919854057280] Epoch[78] Batch[5] avg_epoch_loss=4.099327\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:14 INFO 139919854057280] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=4.099327087402344\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:14 INFO 139919854057280] Epoch[78] Batch [5]#011Speed: 2973.04 samples/sec#011loss=4.099327\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:14 INFO 139919854057280] Epoch[78] Batch[10] avg_epoch_loss=4.104073\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:14 INFO 139919854057280] #quality_metric: host=algo-1, epoch=78, batch=10 train loss <loss>=4.109768438339233\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:14 INFO 139919854057280] Epoch[78] Batch [10]#011Speed: 2732.13 samples/sec#011loss=4.109768\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:14 INFO 139919854057280] processed a total of 689 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158854.2663705, \"EndTime\": 1681158854.6186142, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 351.7773151397705, \"count\": 1, \"min\": 351.7773151397705, \"max\": 351.7773151397705}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:14 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1958.0959277980296 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:14 INFO 139919854057280] #progress_metric: host=algo-1, completed 19.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:14 INFO 139919854057280] #quality_metric: host=algo-1, epoch=78, train loss <loss>=4.1040731560100205\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:14 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:14 INFO 139919854057280] Epoch[79] Batch[0] avg_epoch_loss=4.034905\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:14 INFO 139919854057280] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=4.034904956817627\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:14 INFO 139919854057280] Epoch[79] Batch[5] avg_epoch_loss=4.148946\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:14 INFO 139919854057280] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=4.148945569992065\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:14 INFO 139919854057280] Epoch[79] Batch [5]#011Speed: 2858.38 samples/sec#011loss=4.148946\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:14 INFO 139919854057280] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158854.6186814, \"EndTime\": 1681158854.9502156, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 331.15696907043457, \"count\": 1, \"min\": 331.15696907043457, \"max\": 331.15696907043457}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:14 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1932.0548907496736 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:14 INFO 139919854057280] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:14 INFO 139919854057280] #quality_metric: host=algo-1, epoch=79, train loss <loss>=4.310799169540405\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:14 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:15 INFO 139919854057280] Epoch[80] Batch[0] avg_epoch_loss=4.250511\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:15 INFO 139919854057280] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=4.2505106925964355\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:15 INFO 139919854057280] Epoch[80] Batch[5] avg_epoch_loss=4.372595\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:15 INFO 139919854057280] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=4.372594594955444\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:15 INFO 139919854057280] Epoch[80] Batch [5]#011Speed: 2883.65 samples/sec#011loss=4.372595\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:15 INFO 139919854057280] Epoch[80] Batch[10] avg_epoch_loss=4.372503\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:15 INFO 139919854057280] #quality_metric: host=algo-1, epoch=80, batch=10 train loss <loss>=4.372393226623535\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:15 INFO 139919854057280] Epoch[80] Batch [10]#011Speed: 2577.43 samples/sec#011loss=4.372393\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:15 INFO 139919854057280] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158854.950283, \"EndTime\": 1681158855.3272765, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 376.65438652038574, \"count\": 1, \"min\": 376.65438652038574, \"max\": 376.65438652038574}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:15 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1738.5215471389345 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:15 INFO 139919854057280] #progress_metric: host=algo-1, completed 20.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:15 INFO 139919854057280] #quality_metric: host=algo-1, epoch=80, train loss <loss>=4.372503063895485\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:15 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:15 INFO 139919854057280] Epoch[81] Batch[0] avg_epoch_loss=4.709178\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:15 INFO 139919854057280] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=4.709178447723389\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:15 INFO 139919854057280] Epoch[81] Batch[5] avg_epoch_loss=4.249905\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:15 INFO 139919854057280] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=4.2499048709869385\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:15 INFO 139919854057280] Epoch[81] Batch [5]#011Speed: 2505.84 samples/sec#011loss=4.249905\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:15 INFO 139919854057280] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158855.327346, \"EndTime\": 1681158855.6714365, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 343.6541557312012, \"count\": 1, \"min\": 343.6541557312012, \"max\": 343.6541557312012}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:15 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1823.9519277584736 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:15 INFO 139919854057280] #progress_metric: host=algo-1, completed 20.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:15 INFO 139919854057280] #quality_metric: host=algo-1, epoch=81, train loss <loss>=4.161541080474853\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:15 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:15 INFO 139919854057280] Epoch[82] Batch[0] avg_epoch_loss=4.036483\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:15 INFO 139919854057280] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=4.036482810974121\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:15 INFO 139919854057280] Epoch[82] Batch[5] avg_epoch_loss=4.165823\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:15 INFO 139919854057280] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=4.165823221206665\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:15 INFO 139919854057280] Epoch[82] Batch [5]#011Speed: 3041.41 samples/sec#011loss=4.165823\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:16 INFO 139919854057280] Epoch[82] Batch[10] avg_epoch_loss=4.190252\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:16 INFO 139919854057280] #quality_metric: host=algo-1, epoch=82, batch=10 train loss <loss>=4.2195662498474125\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:16 INFO 139919854057280] Epoch[82] Batch [10]#011Speed: 2669.43 samples/sec#011loss=4.219566\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:16 INFO 139919854057280] processed a total of 673 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158855.671511, \"EndTime\": 1681158856.0258102, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 353.81245613098145, \"count\": 1, \"min\": 353.81245613098145, \"max\": 353.81245613098145}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:16 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1901.6302232629769 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:16 INFO 139919854057280] #progress_metric: host=algo-1, completed 20.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:16 INFO 139919854057280] #quality_metric: host=algo-1, epoch=82, train loss <loss>=4.190251870588823\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:16 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:16 INFO 139919854057280] Epoch[83] Batch[0] avg_epoch_loss=4.135950\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:16 INFO 139919854057280] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=4.135950088500977\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:16 INFO 139919854057280] Epoch[83] Batch[5] avg_epoch_loss=4.320782\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:16 INFO 139919854057280] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=4.32078234354655\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:16 INFO 139919854057280] Epoch[83] Batch [5]#011Speed: 2998.63 samples/sec#011loss=4.320782\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:16 INFO 139919854057280] Epoch[83] Batch[10] avg_epoch_loss=4.382651\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:16 INFO 139919854057280] #quality_metric: host=algo-1, epoch=83, batch=10 train loss <loss>=4.4568933010101315\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:16 INFO 139919854057280] Epoch[83] Batch [10]#011Speed: 2927.76 samples/sec#011loss=4.456893\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:16 INFO 139919854057280] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158856.0258758, \"EndTime\": 1681158856.401547, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 375.3628730773926, \"count\": 1, \"min\": 375.3628730773926, \"max\": 375.3628730773926}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:16 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1736.5214962849416 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:16 INFO 139919854057280] #progress_metric: host=algo-1, completed 21.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:16 INFO 139919854057280] #quality_metric: host=algo-1, epoch=83, train loss <loss>=4.38265096057545\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:16 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:16 INFO 139919854057280] Epoch[84] Batch[0] avg_epoch_loss=5.088444\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:16 INFO 139919854057280] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=5.088443756103516\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:16 INFO 139919854057280] Epoch[84] Batch[5] avg_epoch_loss=4.524076\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:16 INFO 139919854057280] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=4.524076461791992\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:16 INFO 139919854057280] Epoch[84] Batch [5]#011Speed: 2805.32 samples/sec#011loss=4.524076\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:16 INFO 139919854057280] Epoch[84] Batch[10] avg_epoch_loss=4.430477\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:16 INFO 139919854057280] #quality_metric: host=algo-1, epoch=84, batch=10 train loss <loss>=4.318157768249511\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:16 INFO 139919854057280] Epoch[84] Batch [10]#011Speed: 2781.48 samples/sec#011loss=4.318158\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:16 INFO 139919854057280] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158856.4016178, \"EndTime\": 1681158856.761814, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 359.81225967407227, \"count\": 1, \"min\": 359.81225967407227, \"max\": 359.81225967407227}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:16 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1786.5553275294885 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:16 INFO 139919854057280] #progress_metric: host=algo-1, completed 21.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:16 INFO 139919854057280] #quality_metric: host=algo-1, epoch=84, train loss <loss>=4.430477055636319\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:16 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:16 INFO 139919854057280] Epoch[85] Batch[0] avg_epoch_loss=4.259436\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:16 INFO 139919854057280] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=4.259436130523682\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:17 INFO 139919854057280] Epoch[85] Batch[5] avg_epoch_loss=4.138538\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:17 INFO 139919854057280] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=4.138538400332133\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:17 INFO 139919854057280] Epoch[85] Batch [5]#011Speed: 2950.20 samples/sec#011loss=4.138538\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:17 INFO 139919854057280] Epoch[85] Batch[10] avg_epoch_loss=4.092579\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:17 INFO 139919854057280] #quality_metric: host=algo-1, epoch=85, batch=10 train loss <loss>=4.0374267578125\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:17 INFO 139919854057280] Epoch[85] Batch [10]#011Speed: 2810.67 samples/sec#011loss=4.037427\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:17 INFO 139919854057280] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158856.7618837, \"EndTime\": 1681158857.125076, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 362.89358139038086, \"count\": 1, \"min\": 362.89358139038086, \"max\": 362.89358139038086}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:17 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1834.7400893788406 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:17 INFO 139919854057280] #progress_metric: host=algo-1, completed 21.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:17 INFO 139919854057280] #quality_metric: host=algo-1, epoch=85, train loss <loss>=4.092578562823209\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:17 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:17 INFO 139919854057280] Epoch[86] Batch[0] avg_epoch_loss=4.112253\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:17 INFO 139919854057280] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=4.112253189086914\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:17 INFO 139919854057280] Epoch[86] Batch[5] avg_epoch_loss=4.083746\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:17 INFO 139919854057280] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=4.083746075630188\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:17 INFO 139919854057280] Epoch[86] Batch [5]#011Speed: 2936.93 samples/sec#011loss=4.083746\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:17 INFO 139919854057280] Epoch[86] Batch[10] avg_epoch_loss=4.200581\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:17 INFO 139919854057280] #quality_metric: host=algo-1, epoch=86, batch=10 train loss <loss>=4.340782737731933\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:17 INFO 139919854057280] Epoch[86] Batch [10]#011Speed: 2584.20 samples/sec#011loss=4.340783\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:17 INFO 139919854057280] processed a total of 681 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158857.1251478, \"EndTime\": 1681158857.5158408, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 390.34414291381836, \"count\": 1, \"min\": 390.34414291381836, \"max\": 390.34414291381836}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:17 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1744.1743880657168 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:17 INFO 139919854057280] #progress_metric: host=algo-1, completed 21.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:17 INFO 139919854057280] #quality_metric: host=algo-1, epoch=86, train loss <loss>=4.200580922040072\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:17 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:17 INFO 139919854057280] Epoch[87] Batch[0] avg_epoch_loss=4.677216\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:17 INFO 139919854057280] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=4.677215576171875\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:17 INFO 139919854057280] Epoch[87] Batch[5] avg_epoch_loss=4.300273\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:17 INFO 139919854057280] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=4.300272623697917\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:17 INFO 139919854057280] Epoch[87] Batch [5]#011Speed: 2922.03 samples/sec#011loss=4.300273\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:17 INFO 139919854057280] Epoch[87] Batch[10] avg_epoch_loss=4.293635\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:17 INFO 139919854057280] #quality_metric: host=algo-1, epoch=87, batch=10 train loss <loss>=4.285670804977417\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:17 INFO 139919854057280] Epoch[87] Batch [10]#011Speed: 2691.96 samples/sec#011loss=4.285671\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:17 INFO 139919854057280] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158857.5159106, \"EndTime\": 1681158857.8771927, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 360.78357696533203, \"count\": 1, \"min\": 360.78357696533203, \"max\": 360.78357696533203}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:17 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1809.4877796437947 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:17 INFO 139919854057280] #progress_metric: host=algo-1, completed 22.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:17 INFO 139919854057280] #quality_metric: host=algo-1, epoch=87, train loss <loss>=4.293635433370417\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:17 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:18 INFO 139919854057280] Epoch[88] Batch[0] avg_epoch_loss=4.151129\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:18 INFO 139919854057280] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=4.151128768920898\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:18 INFO 139919854057280] Epoch[88] Batch[5] avg_epoch_loss=4.160282\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:18 INFO 139919854057280] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=4.160281658172607\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:18 INFO 139919854057280] Epoch[88] Batch [5]#011Speed: 3007.11 samples/sec#011loss=4.160282\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:18 INFO 139919854057280] Epoch[88] Batch[10] avg_epoch_loss=4.205535\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:18 INFO 139919854057280] #quality_metric: host=algo-1, epoch=88, batch=10 train loss <loss>=4.259839010238648\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:18 INFO 139919854057280] Epoch[88] Batch [10]#011Speed: 2816.14 samples/sec#011loss=4.259839\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:18 INFO 139919854057280] processed a total of 673 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158857.8772566, \"EndTime\": 1681158858.2560005, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 378.4449100494385, \"count\": 1, \"min\": 378.4449100494385, \"max\": 378.4449100494385}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:18 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1777.8653472131248 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:18 INFO 139919854057280] #progress_metric: host=algo-1, completed 22.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:18 INFO 139919854057280] #quality_metric: host=algo-1, epoch=88, train loss <loss>=4.205535000020808\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:18 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:18 INFO 139919854057280] Epoch[89] Batch[0] avg_epoch_loss=4.008543\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:18 INFO 139919854057280] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=4.008542537689209\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:18 INFO 139919854057280] Epoch[89] Batch[5] avg_epoch_loss=4.188860\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:18 INFO 139919854057280] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=4.188860495885213\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:18 INFO 139919854057280] Epoch[89] Batch [5]#011Speed: 2993.61 samples/sec#011loss=4.188860\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:18 INFO 139919854057280] Epoch[89] Batch[10] avg_epoch_loss=4.241228\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:18 INFO 139919854057280] #quality_metric: host=algo-1, epoch=89, batch=10 train loss <loss>=4.304068851470947\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:18 INFO 139919854057280] Epoch[89] Batch [10]#011Speed: 2598.09 samples/sec#011loss=4.304069\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:18 INFO 139919854057280] processed a total of 746 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158858.2560694, \"EndTime\": 1681158858.6308901, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 374.4339942932129, \"count\": 1, \"min\": 374.4339942932129, \"max\": 374.4339942932129}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:18 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1991.8065275499375 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:18 INFO 139919854057280] #progress_metric: host=algo-1, completed 22.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:18 INFO 139919854057280] #quality_metric: host=algo-1, epoch=89, train loss <loss>=4.189533253510793\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:18 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:18 INFO 139919854057280] Epoch[90] Batch[0] avg_epoch_loss=4.718786\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:18 INFO 139919854057280] #quality_metric: host=algo-1, epoch=90, batch=0 train loss <loss>=4.718785762786865\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:18 INFO 139919854057280] Epoch[90] Batch[5] avg_epoch_loss=4.422002\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:18 INFO 139919854057280] #quality_metric: host=algo-1, epoch=90, batch=5 train loss <loss>=4.4220015207926435\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:18 INFO 139919854057280] Epoch[90] Batch [5]#011Speed: 2840.80 samples/sec#011loss=4.422002\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:18 INFO 139919854057280] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158858.6309586, \"EndTime\": 1681158858.9599068, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 328.4437656402588, \"count\": 1, \"min\": 328.4437656402588, \"max\": 328.4437656402588}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:18 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1941.8933643150058 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:18 INFO 139919854057280] #progress_metric: host=algo-1, completed 22.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:18 INFO 139919854057280] #quality_metric: host=algo-1, epoch=90, train loss <loss>=4.301578426361084\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:18 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:19 INFO 139919854057280] Epoch[91] Batch[0] avg_epoch_loss=4.139286\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:19 INFO 139919854057280] #quality_metric: host=algo-1, epoch=91, batch=0 train loss <loss>=4.139285564422607\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:19 INFO 139919854057280] Epoch[91] Batch[5] avg_epoch_loss=4.173268\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:19 INFO 139919854057280] #quality_metric: host=algo-1, epoch=91, batch=5 train loss <loss>=4.173267761866252\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:19 INFO 139919854057280] Epoch[91] Batch [5]#011Speed: 2957.33 samples/sec#011loss=4.173268\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:19 INFO 139919854057280] Epoch[91] Batch[10] avg_epoch_loss=4.197106\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:19 INFO 139919854057280] #quality_metric: host=algo-1, epoch=91, batch=10 train loss <loss>=4.2257115840911865\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:19 INFO 139919854057280] Epoch[91] Batch [10]#011Speed: 2726.59 samples/sec#011loss=4.225712\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:19 INFO 139919854057280] processed a total of 697 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158858.9599793, \"EndTime\": 1681158859.3088374, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 348.32310676574707, \"count\": 1, \"min\": 348.32310676574707, \"max\": 348.32310676574707}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:19 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=2000.387214731852 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:19 INFO 139919854057280] #progress_metric: host=algo-1, completed 23.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:19 INFO 139919854057280] #quality_metric: host=algo-1, epoch=91, train loss <loss>=4.197105862877586\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:19 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:19 INFO 139919854057280] Epoch[92] Batch[0] avg_epoch_loss=4.102898\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:19 INFO 139919854057280] #quality_metric: host=algo-1, epoch=92, batch=0 train loss <loss>=4.102898120880127\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:19 INFO 139919854057280] Epoch[92] Batch[5] avg_epoch_loss=4.128808\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:19 INFO 139919854057280] #quality_metric: host=algo-1, epoch=92, batch=5 train loss <loss>=4.128807624181111\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:19 INFO 139919854057280] Epoch[92] Batch [5]#011Speed: 2950.82 samples/sec#011loss=4.128808\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:19 INFO 139919854057280] Epoch[92] Batch[10] avg_epoch_loss=4.175150\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:19 INFO 139919854057280] #quality_metric: host=algo-1, epoch=92, batch=10 train loss <loss>=4.230759763717652\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:19 INFO 139919854057280] Epoch[92] Batch [10]#011Speed: 2844.46 samples/sec#011loss=4.230760\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:19 INFO 139919854057280] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158859.30892, \"EndTime\": 1681158859.6573763, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 347.9294776916504, \"count\": 1, \"min\": 347.9294776916504, \"max\": 347.9294776916504}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:19 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1887.7768584340877 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:19 INFO 139919854057280] #progress_metric: host=algo-1, completed 23.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:19 INFO 139919854057280] #quality_metric: host=algo-1, epoch=92, train loss <loss>=4.17514950578863\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:19 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:19 INFO 139919854057280] Epoch[93] Batch[0] avg_epoch_loss=4.394041\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:19 INFO 139919854057280] #quality_metric: host=algo-1, epoch=93, batch=0 train loss <loss>=4.394040584564209\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:19 INFO 139919854057280] Epoch[93] Batch[5] avg_epoch_loss=4.147493\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:19 INFO 139919854057280] #quality_metric: host=algo-1, epoch=93, batch=5 train loss <loss>=4.14749280611674\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:19 INFO 139919854057280] Epoch[93] Batch [5]#011Speed: 2959.47 samples/sec#011loss=4.147493\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:20 INFO 139919854057280] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158859.6574442, \"EndTime\": 1681158860.0156853, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 357.9225540161133, \"count\": 1, \"min\": 357.9225540161133, \"max\": 357.9225540161133}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:20 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1783.8444314435696 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:20 INFO 139919854057280] #progress_metric: host=algo-1, completed 23.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:20 INFO 139919854057280] #quality_metric: host=algo-1, epoch=93, train loss <loss>=4.106999468803406\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:20 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:20 INFO 139919854057280] Epoch[94] Batch[0] avg_epoch_loss=4.072722\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:20 INFO 139919854057280] #quality_metric: host=algo-1, epoch=94, batch=0 train loss <loss>=4.0727219581604\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:20 INFO 139919854057280] Epoch[94] Batch[5] avg_epoch_loss=3.984593\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:20 INFO 139919854057280] #quality_metric: host=algo-1, epoch=94, batch=5 train loss <loss>=3.9845934311548867\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:20 INFO 139919854057280] Epoch[94] Batch [5]#011Speed: 3043.79 samples/sec#011loss=3.984593\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:20 INFO 139919854057280] Epoch[94] Batch[10] avg_epoch_loss=4.063077\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:20 INFO 139919854057280] #quality_metric: host=algo-1, epoch=94, batch=10 train loss <loss>=4.157257080078125\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:20 INFO 139919854057280] Epoch[94] Batch [10]#011Speed: 2900.51 samples/sec#011loss=4.157257\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:20 INFO 139919854057280] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158860.015942, \"EndTime\": 1681158860.391018, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 374.5455741882324, \"count\": 1, \"min\": 374.5455741882324, \"max\": 374.5455741882324}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:20 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1710.9639479051652 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:20 INFO 139919854057280] #progress_metric: host=algo-1, completed 23.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:20 INFO 139919854057280] #quality_metric: host=algo-1, epoch=94, train loss <loss>=4.063076907938177\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:20 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:20 INFO 139919854057280] Epoch[95] Batch[0] avg_epoch_loss=4.347973\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:20 INFO 139919854057280] #quality_metric: host=algo-1, epoch=95, batch=0 train loss <loss>=4.347973346710205\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:20 INFO 139919854057280] Epoch[95] Batch[5] avg_epoch_loss=4.139191\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:20 INFO 139919854057280] #quality_metric: host=algo-1, epoch=95, batch=5 train loss <loss>=4.139190872510274\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:20 INFO 139919854057280] Epoch[95] Batch [5]#011Speed: 3056.69 samples/sec#011loss=4.139191\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:20 INFO 139919854057280] Epoch[95] Batch[10] avg_epoch_loss=4.120473\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:20 INFO 139919854057280] #quality_metric: host=algo-1, epoch=95, batch=10 train loss <loss>=4.098011827468872\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:20 INFO 139919854057280] Epoch[95] Batch [10]#011Speed: 2694.92 samples/sec#011loss=4.098012\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:20 INFO 139919854057280] processed a total of 693 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158860.391086, \"EndTime\": 1681158860.7705398, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 379.1005611419678, \"count\": 1, \"min\": 379.1005611419678, \"max\": 379.1005611419678}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:20 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1827.4915181560689 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:20 INFO 139919854057280] #progress_metric: host=algo-1, completed 24.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:20 INFO 139919854057280] #quality_metric: host=algo-1, epoch=95, train loss <loss>=4.120473124764183\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:20 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:20 INFO 139919854057280] Epoch[96] Batch[0] avg_epoch_loss=4.795468\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:20 INFO 139919854057280] #quality_metric: host=algo-1, epoch=96, batch=0 train loss <loss>=4.795467853546143\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:21 INFO 139919854057280] Epoch[96] Batch[5] avg_epoch_loss=4.256604\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:21 INFO 139919854057280] #quality_metric: host=algo-1, epoch=96, batch=5 train loss <loss>=4.256604154904683\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:21 INFO 139919854057280] Epoch[96] Batch [5]#011Speed: 2548.17 samples/sec#011loss=4.256604\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:21 INFO 139919854057280] Epoch[96] Batch[10] avg_epoch_loss=4.110685\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:21 INFO 139919854057280] #quality_metric: host=algo-1, epoch=96, batch=10 train loss <loss>=3.935581398010254\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:21 INFO 139919854057280] Epoch[96] Batch [10]#011Speed: 2442.89 samples/sec#011loss=3.935581\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:21 INFO 139919854057280] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158860.770613, \"EndTime\": 1681158861.1648173, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 393.4307098388672, \"count\": 1, \"min\": 393.4307098388672, \"max\": 393.4307098388672}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:21 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1674.542256644138 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:21 INFO 139919854057280] #progress_metric: host=algo-1, completed 24.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:21 INFO 139919854057280] #quality_metric: host=algo-1, epoch=96, train loss <loss>=4.11068471995267\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:21 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:21 INFO 139919854057280] Epoch[97] Batch[0] avg_epoch_loss=4.171847\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:21 INFO 139919854057280] #quality_metric: host=algo-1, epoch=97, batch=0 train loss <loss>=4.171847343444824\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:21 INFO 139919854057280] Epoch[97] Batch[5] avg_epoch_loss=4.160240\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:21 INFO 139919854057280] #quality_metric: host=algo-1, epoch=97, batch=5 train loss <loss>=4.160240133603414\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:21 INFO 139919854057280] Epoch[97] Batch [5]#011Speed: 3040.27 samples/sec#011loss=4.160240\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:21 INFO 139919854057280] Epoch[97] Batch[10] avg_epoch_loss=4.074709\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:21 INFO 139919854057280] #quality_metric: host=algo-1, epoch=97, batch=10 train loss <loss>=3.9720712661743165\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:21 INFO 139919854057280] Epoch[97] Batch [10]#011Speed: 2708.80 samples/sec#011loss=3.972071\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:21 INFO 139919854057280] processed a total of 678 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158861.1648915, \"EndTime\": 1681158861.519064, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 353.6694049835205, \"count\": 1, \"min\": 353.6694049835205, \"max\": 353.6694049835205}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:21 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1916.523865750101 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:21 INFO 139919854057280] #progress_metric: host=algo-1, completed 24.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:21 INFO 139919854057280] #quality_metric: host=algo-1, epoch=97, train loss <loss>=4.074708830226552\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:21 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:21 INFO 139919854057280] Epoch[98] Batch[0] avg_epoch_loss=5.066308\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:21 INFO 139919854057280] #quality_metric: host=algo-1, epoch=98, batch=0 train loss <loss>=5.06630802154541\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:21 INFO 139919854057280] Epoch[98] Batch[5] avg_epoch_loss=4.548352\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:21 INFO 139919854057280] #quality_metric: host=algo-1, epoch=98, batch=5 train loss <loss>=4.548351526260376\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:21 INFO 139919854057280] Epoch[98] Batch [5]#011Speed: 2505.49 samples/sec#011loss=4.548352\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:21 INFO 139919854057280] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158861.519131, \"EndTime\": 1681158861.8991706, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 379.70781326293945, \"count\": 1, \"min\": 379.70781326293945, \"max\": 379.70781326293945}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:21 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1682.4493088873837 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:21 INFO 139919854057280] #progress_metric: host=algo-1, completed 24.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:21 INFO 139919854057280] #quality_metric: host=algo-1, epoch=98, train loss <loss>=4.420639896392823\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:21 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:22 INFO 139919854057280] Epoch[99] Batch[0] avg_epoch_loss=4.031990\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:22 INFO 139919854057280] #quality_metric: host=algo-1, epoch=99, batch=0 train loss <loss>=4.031989574432373\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:22 INFO 139919854057280] Epoch[99] Batch[5] avg_epoch_loss=4.076357\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:22 INFO 139919854057280] #quality_metric: host=algo-1, epoch=99, batch=5 train loss <loss>=4.0763574441274\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:22 INFO 139919854057280] Epoch[99] Batch [5]#011Speed: 2757.79 samples/sec#011loss=4.076357\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:22 INFO 139919854057280] Epoch[99] Batch[10] avg_epoch_loss=4.143494\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:22 INFO 139919854057280] #quality_metric: host=algo-1, epoch=99, batch=10 train loss <loss>=4.22405743598938\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:22 INFO 139919854057280] Epoch[99] Batch [10]#011Speed: 2638.72 samples/sec#011loss=4.224057\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:22 INFO 139919854057280] processed a total of 718 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158861.8992383, \"EndTime\": 1681158862.2969067, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 397.31597900390625, \"count\": 1, \"min\": 397.31597900390625, \"max\": 397.31597900390625}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:22 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1806.68032815898 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:22 INFO 139919854057280] #progress_metric: host=algo-1, completed 25.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:22 INFO 139919854057280] #quality_metric: host=algo-1, epoch=99, train loss <loss>=4.224429349104564\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:22 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:22 INFO 139919854057280] Epoch[100] Batch[0] avg_epoch_loss=4.287538\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:22 INFO 139919854057280] #quality_metric: host=algo-1, epoch=100, batch=0 train loss <loss>=4.287538051605225\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:22 INFO 139919854057280] Epoch[100] Batch[5] avg_epoch_loss=4.135917\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:22 INFO 139919854057280] #quality_metric: host=algo-1, epoch=100, batch=5 train loss <loss>=4.135916709899902\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:22 INFO 139919854057280] Epoch[100] Batch [5]#011Speed: 2951.67 samples/sec#011loss=4.135917\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:22 INFO 139919854057280] Epoch[100] Batch[10] avg_epoch_loss=4.133817\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:22 INFO 139919854057280] #quality_metric: host=algo-1, epoch=100, batch=10 train loss <loss>=4.131296968460083\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:22 INFO 139919854057280] Epoch[100] Batch [10]#011Speed: 2802.58 samples/sec#011loss=4.131297\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:22 INFO 139919854057280] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158862.296975, \"EndTime\": 1681158862.6478205, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 350.3592014312744, \"count\": 1, \"min\": 350.3592014312744, \"max\": 350.3592014312744}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:22 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1917.5187311297457 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:22 INFO 139919854057280] #progress_metric: host=algo-1, completed 25.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:22 INFO 139919854057280] #quality_metric: host=algo-1, epoch=100, train loss <loss>=4.133816827427257\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:22 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:22 INFO 139919854057280] Epoch[101] Batch[0] avg_epoch_loss=4.041050\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:22 INFO 139919854057280] #quality_metric: host=algo-1, epoch=101, batch=0 train loss <loss>=4.041049957275391\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:22 INFO 139919854057280] Epoch[101] Batch[5] avg_epoch_loss=3.890227\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:22 INFO 139919854057280] #quality_metric: host=algo-1, epoch=101, batch=5 train loss <loss>=3.890227476755778\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:22 INFO 139919854057280] Epoch[101] Batch [5]#011Speed: 2806.66 samples/sec#011loss=3.890227\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:23 INFO 139919854057280] Epoch[101] Batch[10] avg_epoch_loss=3.968553\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:23 INFO 139919854057280] #quality_metric: host=algo-1, epoch=101, batch=10 train loss <loss>=4.062543773651123\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:23 INFO 139919854057280] Epoch[101] Batch [10]#011Speed: 2763.40 samples/sec#011loss=4.062544\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:23 INFO 139919854057280] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158862.6478846, \"EndTime\": 1681158863.0409086, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 392.7156925201416, \"count\": 1, \"min\": 392.7156925201416, \"max\": 392.7156925201416}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:23 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1677.6299540541033 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:23 INFO 139919854057280] #progress_metric: host=algo-1, completed 25.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:23 INFO 139919854057280] #quality_metric: host=algo-1, epoch=101, train loss <loss>=3.968553066253662\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:23 INFO 139919854057280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:23 INFO 139919854057280] Saved checkpoint to \"/opt/ml/model/state_efbd3e20-f11f-4dba-a438-1124cd779f46-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158863.0409794, \"EndTime\": 1681158863.0514898, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 9.935855865478516, \"count\": 1, \"min\": 9.935855865478516, \"max\": 9.935855865478516}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:23 INFO 139919854057280] Epoch[102] Batch[0] avg_epoch_loss=4.008762\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:23 INFO 139919854057280] #quality_metric: host=algo-1, epoch=102, batch=0 train loss <loss>=4.008761882781982\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:23 INFO 139919854057280] Epoch[102] Batch[5] avg_epoch_loss=4.172148\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:23 INFO 139919854057280] #quality_metric: host=algo-1, epoch=102, batch=5 train loss <loss>=4.172147830327352\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:23 INFO 139919854057280] Epoch[102] Batch [5]#011Speed: 2962.44 samples/sec#011loss=4.172148\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:23 INFO 139919854057280] Epoch[102] Batch[10] avg_epoch_loss=4.122254\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:23 INFO 139919854057280] #quality_metric: host=algo-1, epoch=102, batch=10 train loss <loss>=4.062380599975586\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:23 INFO 139919854057280] Epoch[102] Batch [10]#011Speed: 2770.27 samples/sec#011loss=4.062381\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:23 INFO 139919854057280] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158863.0515592, \"EndTime\": 1681158863.3996077, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 347.9936122894287, \"count\": 1, \"min\": 347.9936122894287, \"max\": 347.9936122894287}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:23 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1930.260578837326 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:23 INFO 139919854057280] #progress_metric: host=algo-1, completed 25.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:23 INFO 139919854057280] #quality_metric: host=algo-1, epoch=102, train loss <loss>=4.122253634712913\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:23 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:23 INFO 139919854057280] Epoch[103] Batch[0] avg_epoch_loss=4.247466\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:23 INFO 139919854057280] #quality_metric: host=algo-1, epoch=103, batch=0 train loss <loss>=4.247466087341309\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:23 INFO 139919854057280] Epoch[103] Batch[5] avg_epoch_loss=3.959047\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:23 INFO 139919854057280] #quality_metric: host=algo-1, epoch=103, batch=5 train loss <loss>=3.959047476450602\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:23 INFO 139919854057280] Epoch[103] Batch [5]#011Speed: 2887.72 samples/sec#011loss=3.959047\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:23 INFO 139919854057280] Epoch[103] Batch[10] avg_epoch_loss=3.995992\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:23 INFO 139919854057280] #quality_metric: host=algo-1, epoch=103, batch=10 train loss <loss>=4.040325021743774\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:23 INFO 139919854057280] Epoch[103] Batch [10]#011Speed: 2662.27 samples/sec#011loss=4.040325\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:23 INFO 139919854057280] processed a total of 695 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158863.3997233, \"EndTime\": 1681158863.7623808, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 362.1644973754883, \"count\": 1, \"min\": 362.1644973754883, \"max\": 362.1644973754883}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:23 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1918.5148075654886 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:23 INFO 139919854057280] #progress_metric: host=algo-1, completed 26.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:23 INFO 139919854057280] #quality_metric: host=algo-1, epoch=103, train loss <loss>=3.995991815220226\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:23 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:23 INFO 139919854057280] Epoch[104] Batch[0] avg_epoch_loss=4.115107\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:23 INFO 139919854057280] #quality_metric: host=algo-1, epoch=104, batch=0 train loss <loss>=4.115106582641602\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:24 INFO 139919854057280] Epoch[104] Batch[5] avg_epoch_loss=3.960873\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:24 INFO 139919854057280] #quality_metric: host=algo-1, epoch=104, batch=5 train loss <loss>=3.9608734448750815\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:24 INFO 139919854057280] Epoch[104] Batch [5]#011Speed: 2938.20 samples/sec#011loss=3.960873\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:24 INFO 139919854057280] Epoch[104] Batch[10] avg_epoch_loss=3.968042\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:24 INFO 139919854057280] #quality_metric: host=algo-1, epoch=104, batch=10 train loss <loss>=3.9766445636749266\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:24 INFO 139919854057280] Epoch[104] Batch [10]#011Speed: 2745.29 samples/sec#011loss=3.976645\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:24 INFO 139919854057280] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158863.7624454, \"EndTime\": 1681158864.144408, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 381.6051483154297, \"count\": 1, \"min\": 381.6051483154297, \"max\": 381.6051483154297}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:24 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1757.7887464165037 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:24 INFO 139919854057280] #progress_metric: host=algo-1, completed 26.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:24 INFO 139919854057280] #quality_metric: host=algo-1, epoch=104, train loss <loss>=3.9680421352386475\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:24 INFO 139919854057280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:24 INFO 139919854057280] Saved checkpoint to \"/opt/ml/model/state_099c9b7b-f3c5-4a13-998b-01fe1adf195f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158864.1445005, \"EndTime\": 1681158864.152179, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 7.082700729370117, \"count\": 1, \"min\": 7.082700729370117, \"max\": 7.082700729370117}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:24 INFO 139919854057280] Epoch[105] Batch[0] avg_epoch_loss=4.345888\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:24 INFO 139919854057280] #quality_metric: host=algo-1, epoch=105, batch=0 train loss <loss>=4.345887660980225\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:24 INFO 139919854057280] Epoch[105] Batch[5] avg_epoch_loss=4.103034\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:24 INFO 139919854057280] #quality_metric: host=algo-1, epoch=105, batch=5 train loss <loss>=4.1030335028966265\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:24 INFO 139919854057280] Epoch[105] Batch [5]#011Speed: 3008.48 samples/sec#011loss=4.103034\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:24 INFO 139919854057280] Epoch[105] Batch[10] avg_epoch_loss=4.143824\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:24 INFO 139919854057280] #quality_metric: host=algo-1, epoch=105, batch=10 train loss <loss>=4.192773151397705\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:24 INFO 139919854057280] Epoch[105] Batch [10]#011Speed: 2716.04 samples/sec#011loss=4.192773\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:24 INFO 139919854057280] processed a total of 686 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158864.1522431, \"EndTime\": 1681158864.5044653, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 352.1451950073242, \"count\": 1, \"min\": 352.1451950073242, \"max\": 352.1451950073242}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:24 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1947.4917976086933 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:24 INFO 139919854057280] #progress_metric: host=algo-1, completed 26.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:24 INFO 139919854057280] #quality_metric: host=algo-1, epoch=105, train loss <loss>=4.143824252215299\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:24 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:24 INFO 139919854057280] Epoch[106] Batch[0] avg_epoch_loss=4.033772\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:24 INFO 139919854057280] #quality_metric: host=algo-1, epoch=106, batch=0 train loss <loss>=4.033771991729736\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:24 INFO 139919854057280] Epoch[106] Batch[5] avg_epoch_loss=3.973503\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:24 INFO 139919854057280] #quality_metric: host=algo-1, epoch=106, batch=5 train loss <loss>=3.973503351211548\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:24 INFO 139919854057280] Epoch[106] Batch [5]#011Speed: 2962.30 samples/sec#011loss=3.973503\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:24 INFO 139919854057280] Epoch[106] Batch[10] avg_epoch_loss=3.997928\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:24 INFO 139919854057280] #quality_metric: host=algo-1, epoch=106, batch=10 train loss <loss>=4.027236843109131\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:24 INFO 139919854057280] Epoch[106] Batch [10]#011Speed: 2514.53 samples/sec#011loss=4.027237\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:24 INFO 139919854057280] processed a total of 685 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158864.5045395, \"EndTime\": 1681158864.8900132, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 385.1163387298584, \"count\": 1, \"min\": 385.1163387298584, \"max\": 385.1163387298584}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:24 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1778.150502017912 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:24 INFO 139919854057280] #progress_metric: host=algo-1, completed 26.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:24 INFO 139919854057280] #quality_metric: host=algo-1, epoch=106, train loss <loss>=3.997927665710449\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:24 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:25 INFO 139919854057280] Epoch[107] Batch[0] avg_epoch_loss=4.134035\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:25 INFO 139919854057280] #quality_metric: host=algo-1, epoch=107, batch=0 train loss <loss>=4.134034633636475\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:25 INFO 139919854057280] Epoch[107] Batch[5] avg_epoch_loss=4.085765\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:25 INFO 139919854057280] #quality_metric: host=algo-1, epoch=107, batch=5 train loss <loss>=4.08576496442159\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:25 INFO 139919854057280] Epoch[107] Batch [5]#011Speed: 2942.08 samples/sec#011loss=4.085765\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:25 INFO 139919854057280] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158864.890097, \"EndTime\": 1681158865.2224402, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 331.8638801574707, \"count\": 1, \"min\": 331.8638801574707, \"max\": 331.8638801574707}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:25 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1924.9018976386888 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:25 INFO 139919854057280] #progress_metric: host=algo-1, completed 27.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:25 INFO 139919854057280] #quality_metric: host=algo-1, epoch=107, train loss <loss>=4.00500340461731\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:25 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:25 INFO 139919854057280] Epoch[108] Batch[0] avg_epoch_loss=4.061092\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:25 INFO 139919854057280] #quality_metric: host=algo-1, epoch=108, batch=0 train loss <loss>=4.061091899871826\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:25 INFO 139919854057280] Epoch[108] Batch[5] avg_epoch_loss=3.919711\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:25 INFO 139919854057280] #quality_metric: host=algo-1, epoch=108, batch=5 train loss <loss>=3.919710715611776\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:25 INFO 139919854057280] Epoch[108] Batch [5]#011Speed: 2969.10 samples/sec#011loss=3.919711\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:25 INFO 139919854057280] Epoch[108] Batch[10] avg_epoch_loss=3.996285\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:25 INFO 139919854057280] #quality_metric: host=algo-1, epoch=108, batch=10 train loss <loss>=4.088173532485962\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:25 INFO 139919854057280] Epoch[108] Batch [10]#011Speed: 2790.76 samples/sec#011loss=4.088174\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:25 INFO 139919854057280] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158865.2225106, \"EndTime\": 1681158865.60184, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 378.9348602294922, \"count\": 1, \"min\": 378.9348602294922, \"max\": 378.9348602294922}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:25 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1778.2580856192474 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:25 INFO 139919854057280] #progress_metric: host=algo-1, completed 27.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:25 INFO 139919854057280] #quality_metric: host=algo-1, epoch=108, train loss <loss>=3.9962847232818604\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:25 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:25 INFO 139919854057280] Epoch[109] Batch[0] avg_epoch_loss=3.888872\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:25 INFO 139919854057280] #quality_metric: host=algo-1, epoch=109, batch=0 train loss <loss>=3.888871669769287\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:25 INFO 139919854057280] Epoch[109] Batch[5] avg_epoch_loss=4.089026\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:25 INFO 139919854057280] #quality_metric: host=algo-1, epoch=109, batch=5 train loss <loss>=4.089025537172954\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:25 INFO 139919854057280] Epoch[109] Batch [5]#011Speed: 2784.34 samples/sec#011loss=4.089026\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:25 INFO 139919854057280] Epoch[109] Batch[10] avg_epoch_loss=4.078403\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:25 INFO 139919854057280] #quality_metric: host=algo-1, epoch=109, batch=10 train loss <loss>=4.065656042098999\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:25 INFO 139919854057280] Epoch[109] Batch [10]#011Speed: 2410.22 samples/sec#011loss=4.065656\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:25 INFO 139919854057280] processed a total of 677 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158865.6019006, \"EndTime\": 1681158865.9835577, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 381.35790824890137, \"count\": 1, \"min\": 381.35790824890137, \"max\": 381.35790824890137}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:25 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1774.7614674885215 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:25 INFO 139919854057280] #progress_metric: host=algo-1, completed 27.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:25 INFO 139919854057280] #quality_metric: host=algo-1, epoch=109, train loss <loss>=4.078403039412065\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:25 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:26 INFO 139919854057280] Epoch[110] Batch[0] avg_epoch_loss=4.132483\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:26 INFO 139919854057280] #quality_metric: host=algo-1, epoch=110, batch=0 train loss <loss>=4.132482528686523\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:26 INFO 139919854057280] Epoch[110] Batch[5] avg_epoch_loss=4.404943\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:26 INFO 139919854057280] #quality_metric: host=algo-1, epoch=110, batch=5 train loss <loss>=4.404942552248637\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:26 INFO 139919854057280] Epoch[110] Batch [5]#011Speed: 3051.92 samples/sec#011loss=4.404943\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:26 INFO 139919854057280] Epoch[110] Batch[10] avg_epoch_loss=4.446546\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:26 INFO 139919854057280] #quality_metric: host=algo-1, epoch=110, batch=10 train loss <loss>=4.496470642089844\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:26 INFO 139919854057280] Epoch[110] Batch [10]#011Speed: 2849.09 samples/sec#011loss=4.496471\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:26 INFO 139919854057280] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158865.9836304, \"EndTime\": 1681158866.3598764, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 375.901460647583, \"count\": 1, \"min\": 375.901460647583, \"max\": 375.901460647583}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:26 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1731.3383015270442 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:26 INFO 139919854057280] #progress_metric: host=algo-1, completed 27.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:26 INFO 139919854057280] #quality_metric: host=algo-1, epoch=110, train loss <loss>=4.4465462294491855\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:26 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:26 INFO 139919854057280] Epoch[111] Batch[0] avg_epoch_loss=4.110635\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:26 INFO 139919854057280] #quality_metric: host=algo-1, epoch=111, batch=0 train loss <loss>=4.110635280609131\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:26 INFO 139919854057280] Epoch[111] Batch[5] avg_epoch_loss=4.040965\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:26 INFO 139919854057280] #quality_metric: host=algo-1, epoch=111, batch=5 train loss <loss>=4.040964603424072\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:26 INFO 139919854057280] Epoch[111] Batch [5]#011Speed: 2582.50 samples/sec#011loss=4.040965\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:26 INFO 139919854057280] Epoch[111] Batch[10] avg_epoch_loss=4.120223\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:26 INFO 139919854057280] #quality_metric: host=algo-1, epoch=111, batch=10 train loss <loss>=4.215333461761475\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:26 INFO 139919854057280] Epoch[111] Batch [10]#011Speed: 2922.97 samples/sec#011loss=4.215333\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:26 INFO 139919854057280] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158866.359943, \"EndTime\": 1681158866.7243323, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 363.9340400695801, \"count\": 1, \"min\": 363.9340400695801, \"max\": 363.9340400695801}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:26 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1818.4081584538126 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:26 INFO 139919854057280] #progress_metric: host=algo-1, completed 28.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:26 INFO 139919854057280] #quality_metric: host=algo-1, epoch=111, train loss <loss>=4.120223175395619\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:26 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:26 INFO 139919854057280] Epoch[112] Batch[0] avg_epoch_loss=4.047489\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:26 INFO 139919854057280] #quality_metric: host=algo-1, epoch=112, batch=0 train loss <loss>=4.047488689422607\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:26 INFO 139919854057280] Epoch[112] Batch[5] avg_epoch_loss=4.008856\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:26 INFO 139919854057280] #quality_metric: host=algo-1, epoch=112, batch=5 train loss <loss>=4.008856097857158\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:26 INFO 139919854057280] Epoch[112] Batch [5]#011Speed: 2985.35 samples/sec#011loss=4.008856\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:27 INFO 139919854057280] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158866.7244058, \"EndTime\": 1681158867.0547607, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 329.9212455749512, \"count\": 1, \"min\": 329.9212455749512, \"max\": 329.9212455749512}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:27 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1878.6417266519677 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:27 INFO 139919854057280] #progress_metric: host=algo-1, completed 28.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:27 INFO 139919854057280] #quality_metric: host=algo-1, epoch=112, train loss <loss>=4.08195698261261\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:27 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:27 INFO 139919854057280] Epoch[113] Batch[0] avg_epoch_loss=4.404337\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:27 INFO 139919854057280] #quality_metric: host=algo-1, epoch=113, batch=0 train loss <loss>=4.404337406158447\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:27 INFO 139919854057280] Epoch[113] Batch[5] avg_epoch_loss=4.338754\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:27 INFO 139919854057280] #quality_metric: host=algo-1, epoch=113, batch=5 train loss <loss>=4.338753898938497\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:27 INFO 139919854057280] Epoch[113] Batch [5]#011Speed: 3000.42 samples/sec#011loss=4.338754\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:27 INFO 139919854057280] Epoch[113] Batch[10] avg_epoch_loss=4.336035\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:27 INFO 139919854057280] #quality_metric: host=algo-1, epoch=113, batch=10 train loss <loss>=4.332771682739258\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:27 INFO 139919854057280] Epoch[113] Batch [10]#011Speed: 2758.56 samples/sec#011loss=4.332772\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:27 INFO 139919854057280] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158867.0548337, \"EndTime\": 1681158867.4097493, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 354.4960021972656, \"count\": 1, \"min\": 354.4960021972656, \"max\": 354.4960021972656}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:27 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1872.5780063257096 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:27 INFO 139919854057280] #progress_metric: host=algo-1, completed 28.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:27 INFO 139919854057280] #quality_metric: host=algo-1, epoch=113, train loss <loss>=4.336034709757024\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:27 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:27 INFO 139919854057280] Epoch[114] Batch[0] avg_epoch_loss=4.424964\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:27 INFO 139919854057280] #quality_metric: host=algo-1, epoch=114, batch=0 train loss <loss>=4.42496395111084\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:27 INFO 139919854057280] Epoch[114] Batch[5] avg_epoch_loss=4.114005\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:27 INFO 139919854057280] #quality_metric: host=algo-1, epoch=114, batch=5 train loss <loss>=4.114004810651143\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:27 INFO 139919854057280] Epoch[114] Batch [5]#011Speed: 2760.87 samples/sec#011loss=4.114005\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:27 INFO 139919854057280] Epoch[114] Batch[10] avg_epoch_loss=4.193865\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:27 INFO 139919854057280] #quality_metric: host=algo-1, epoch=114, batch=10 train loss <loss>=4.289698123931885\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:27 INFO 139919854057280] Epoch[114] Batch [10]#011Speed: 2974.53 samples/sec#011loss=4.289698\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:27 INFO 139919854057280] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158867.4098163, \"EndTime\": 1681158867.7680476, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 357.93328285217285, \"count\": 1, \"min\": 357.93328285217285, \"max\": 357.93328285217285}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:27 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1793.1275099853367 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:27 INFO 139919854057280] #progress_metric: host=algo-1, completed 28.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:27 INFO 139919854057280] #quality_metric: host=algo-1, epoch=114, train loss <loss>=4.193865407596935\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:27 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:27 INFO 139919854057280] Epoch[115] Batch[0] avg_epoch_loss=4.118191\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:27 INFO 139919854057280] #quality_metric: host=algo-1, epoch=115, batch=0 train loss <loss>=4.118191242218018\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:28 INFO 139919854057280] Epoch[115] Batch[5] avg_epoch_loss=4.123944\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:28 INFO 139919854057280] #quality_metric: host=algo-1, epoch=115, batch=5 train loss <loss>=4.123944083849589\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:28 INFO 139919854057280] Epoch[115] Batch [5]#011Speed: 2541.95 samples/sec#011loss=4.123944\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:28 INFO 139919854057280] Epoch[115] Batch[10] avg_epoch_loss=4.132075\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:28 INFO 139919854057280] #quality_metric: host=algo-1, epoch=115, batch=10 train loss <loss>=4.141831111907959\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:28 INFO 139919854057280] Epoch[115] Batch [10]#011Speed: 2773.24 samples/sec#011loss=4.141831\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:28 INFO 139919854057280] processed a total of 669 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158867.768121, \"EndTime\": 1681158868.1495543, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 381.0708522796631, \"count\": 1, \"min\": 381.0708522796631, \"max\": 381.0708522796631}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:28 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1755.1452822942326 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:28 INFO 139919854057280] #progress_metric: host=algo-1, completed 29.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:28 INFO 139919854057280] #quality_metric: host=algo-1, epoch=115, train loss <loss>=4.132074551148848\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:28 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:28 INFO 139919854057280] Epoch[116] Batch[0] avg_epoch_loss=5.013152\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:28 INFO 139919854057280] #quality_metric: host=algo-1, epoch=116, batch=0 train loss <loss>=5.0131516456604\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:28 INFO 139919854057280] Epoch[116] Batch[5] avg_epoch_loss=4.443551\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:28 INFO 139919854057280] #quality_metric: host=algo-1, epoch=116, batch=5 train loss <loss>=4.443551421165466\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:28 INFO 139919854057280] Epoch[116] Batch [5]#011Speed: 2885.00 samples/sec#011loss=4.443551\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:28 INFO 139919854057280] Epoch[116] Batch[10] avg_epoch_loss=4.329525\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:28 INFO 139919854057280] #quality_metric: host=algo-1, epoch=116, batch=10 train loss <loss>=4.192693281173706\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:28 INFO 139919854057280] Epoch[116] Batch [10]#011Speed: 2773.21 samples/sec#011loss=4.192693\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:28 INFO 139919854057280] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158868.1496208, \"EndTime\": 1681158868.5083065, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 358.3858013153076, \"count\": 1, \"min\": 358.3858013153076, \"max\": 358.3858013153076}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:28 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1852.276327262207 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:28 INFO 139919854057280] #progress_metric: host=algo-1, completed 29.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:28 INFO 139919854057280] #quality_metric: host=algo-1, epoch=116, train loss <loss>=4.329524993896484\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:28 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:28 INFO 139919854057280] Epoch[117] Batch[0] avg_epoch_loss=4.146077\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:28 INFO 139919854057280] #quality_metric: host=algo-1, epoch=117, batch=0 train loss <loss>=4.146076679229736\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:28 INFO 139919854057280] Epoch[117] Batch[5] avg_epoch_loss=4.092481\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:28 INFO 139919854057280] #quality_metric: host=algo-1, epoch=117, batch=5 train loss <loss>=4.092480977376302\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:28 INFO 139919854057280] Epoch[117] Batch [5]#011Speed: 2826.84 samples/sec#011loss=4.092481\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:28 INFO 139919854057280] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158868.50837, \"EndTime\": 1681158868.866173, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 357.4860095977783, \"count\": 1, \"min\": 357.4860095977783, \"max\": 357.4860095977783}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:28 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1739.403881836735 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:28 INFO 139919854057280] #progress_metric: host=algo-1, completed 29.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:28 INFO 139919854057280] #quality_metric: host=algo-1, epoch=117, train loss <loss>=3.925974988937378\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:28 INFO 139919854057280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:28 INFO 139919854057280] Saved checkpoint to \"/opt/ml/model/state_eca37bba-36ae-48a0-a3d8-03a43b820e87-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158868.8662486, \"EndTime\": 1681158868.8745124, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 7.710456848144531, \"count\": 1, \"min\": 7.710456848144531, \"max\": 7.710456848144531}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:29 INFO 139919854057280] Epoch[118] Batch[0] avg_epoch_loss=4.320306\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:29 INFO 139919854057280] #quality_metric: host=algo-1, epoch=118, batch=0 train loss <loss>=4.320306301116943\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:29 INFO 139919854057280] Epoch[118] Batch[5] avg_epoch_loss=4.087352\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:29 INFO 139919854057280] #quality_metric: host=algo-1, epoch=118, batch=5 train loss <loss>=4.087351560592651\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:29 INFO 139919854057280] Epoch[118] Batch [5]#011Speed: 2598.35 samples/sec#011loss=4.087352\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:29 INFO 139919854057280] Epoch[118] Batch[10] avg_epoch_loss=3.890961\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:29 INFO 139919854057280] #quality_metric: host=algo-1, epoch=118, batch=10 train loss <loss>=3.6552916526794434\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:29 INFO 139919854057280] Epoch[118] Batch [10]#011Speed: 2902.73 samples/sec#011loss=3.655292\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:29 INFO 139919854057280] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158868.8746107, \"EndTime\": 1681158869.2434514, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 368.7856197357178, \"count\": 1, \"min\": 368.7856197357178, \"max\": 368.7856197357178}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:29 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1756.5165469011438 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:29 INFO 139919854057280] #progress_metric: host=algo-1, completed 29.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:29 INFO 139919854057280] #quality_metric: host=algo-1, epoch=118, train loss <loss>=3.890960693359375\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:29 INFO 139919854057280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:29 INFO 139919854057280] Saved checkpoint to \"/opt/ml/model/state_8b8cbde2-3f09-4686-b508-611385793f91-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158869.243518, \"EndTime\": 1681158869.2510011, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 7.059812545776367, \"count\": 1, \"min\": 7.059812545776367, \"max\": 7.059812545776367}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:29 INFO 139919854057280] Epoch[119] Batch[0] avg_epoch_loss=4.219979\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:29 INFO 139919854057280] #quality_metric: host=algo-1, epoch=119, batch=0 train loss <loss>=4.2199788093566895\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:29 INFO 139919854057280] Epoch[119] Batch[5] avg_epoch_loss=4.170571\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:29 INFO 139919854057280] #quality_metric: host=algo-1, epoch=119, batch=5 train loss <loss>=4.170570611953735\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:29 INFO 139919854057280] Epoch[119] Batch [5]#011Speed: 2943.55 samples/sec#011loss=4.170571\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:29 INFO 139919854057280] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158869.251054, \"EndTime\": 1681158869.5874665, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 336.3480567932129, \"count\": 1, \"min\": 336.3480567932129, \"max\": 336.3480567932129}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:29 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1860.4656942147471 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:29 INFO 139919854057280] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:29 INFO 139919854057280] #quality_metric: host=algo-1, epoch=119, train loss <loss>=4.085478329658509\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:29 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:29 INFO 139919854057280] Epoch[120] Batch[0] avg_epoch_loss=4.392048\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:29 INFO 139919854057280] #quality_metric: host=algo-1, epoch=120, batch=0 train loss <loss>=4.392048358917236\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:29 INFO 139919854057280] Epoch[120] Batch[5] avg_epoch_loss=4.131809\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:29 INFO 139919854057280] #quality_metric: host=algo-1, epoch=120, batch=5 train loss <loss>=4.131808916727702\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:29 INFO 139919854057280] Epoch[120] Batch [5]#011Speed: 2996.37 samples/sec#011loss=4.131809\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:29 INFO 139919854057280] Epoch[120] Batch[10] avg_epoch_loss=3.938293\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:29 INFO 139919854057280] #quality_metric: host=algo-1, epoch=120, batch=10 train loss <loss>=3.7060749530792236\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:29 INFO 139919854057280] Epoch[120] Batch [10]#011Speed: 2748.34 samples/sec#011loss=3.706075\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:29 INFO 139919854057280] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158869.5875483, \"EndTime\": 1681158869.9406152, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 352.50210762023926, \"count\": 1, \"min\": 352.50210762023926, \"max\": 352.50210762023926}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:29 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1866.1405346779281 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:29 INFO 139919854057280] #progress_metric: host=algo-1, completed 30.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:29 INFO 139919854057280] #quality_metric: host=algo-1, epoch=120, train loss <loss>=3.9382934787056665\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:29 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:30 INFO 139919854057280] Epoch[121] Batch[0] avg_epoch_loss=3.833100\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:30 INFO 139919854057280] #quality_metric: host=algo-1, epoch=121, batch=0 train loss <loss>=3.833099842071533\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:30 INFO 139919854057280] Epoch[121] Batch[5] avg_epoch_loss=3.952690\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:30 INFO 139919854057280] #quality_metric: host=algo-1, epoch=121, batch=5 train loss <loss>=3.952690283457438\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:30 INFO 139919854057280] Epoch[121] Batch [5]#011Speed: 3031.20 samples/sec#011loss=3.952690\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:30 INFO 139919854057280] Epoch[121] Batch[10] avg_epoch_loss=3.849923\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:30 INFO 139919854057280] #quality_metric: host=algo-1, epoch=121, batch=10 train loss <loss>=3.7266016006469727\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:30 INFO 139919854057280] Epoch[121] Batch [10]#011Speed: 2550.17 samples/sec#011loss=3.726602\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:30 INFO 139919854057280] processed a total of 670 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158869.9406831, \"EndTime\": 1681158870.3247383, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 383.7580680847168, \"count\": 1, \"min\": 383.7580680847168, \"max\": 383.7580680847168}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:30 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1745.3592192729718 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:30 INFO 139919854057280] #progress_metric: host=algo-1, completed 30.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:30 INFO 139919854057280] #quality_metric: host=algo-1, epoch=121, train loss <loss>=3.849922700361772\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:30 INFO 139919854057280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:30 INFO 139919854057280] Saved checkpoint to \"/opt/ml/model/state_8751cd58-c42a-43b8-bcc6-a567c9523084-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158870.3248258, \"EndTime\": 1681158870.3323417, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 7.107019424438477, \"count\": 1, \"min\": 7.107019424438477, \"max\": 7.107019424438477}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:30 INFO 139919854057280] Epoch[122] Batch[0] avg_epoch_loss=4.248184\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:30 INFO 139919854057280] #quality_metric: host=algo-1, epoch=122, batch=0 train loss <loss>=4.2481842041015625\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:30 INFO 139919854057280] Epoch[122] Batch[5] avg_epoch_loss=4.242818\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:30 INFO 139919854057280] #quality_metric: host=algo-1, epoch=122, batch=5 train loss <loss>=4.242817719777425\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:30 INFO 139919854057280] Epoch[122] Batch [5]#011Speed: 2608.67 samples/sec#011loss=4.242818\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:30 INFO 139919854057280] Epoch[122] Batch[10] avg_epoch_loss=4.033816\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:30 INFO 139919854057280] #quality_metric: host=algo-1, epoch=122, batch=10 train loss <loss>=3.7830141305923464\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:30 INFO 139919854057280] Epoch[122] Batch [10]#011Speed: 2981.98 samples/sec#011loss=3.783014\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:30 INFO 139919854057280] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158870.3325274, \"EndTime\": 1681158870.6974397, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 364.84241485595703, \"count\": 1, \"min\": 364.84241485595703, \"max\": 364.84241485595703}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:30 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1759.144220467338 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:30 INFO 139919854057280] #progress_metric: host=algo-1, completed 30.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:30 INFO 139919854057280] #quality_metric: host=algo-1, epoch=122, train loss <loss>=4.033816088329662\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:30 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:30 INFO 139919854057280] Epoch[123] Batch[0] avg_epoch_loss=3.724340\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:30 INFO 139919854057280] #quality_metric: host=algo-1, epoch=123, batch=0 train loss <loss>=3.724339723587036\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:30 INFO 139919854057280] Epoch[123] Batch[5] avg_epoch_loss=4.203370\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:30 INFO 139919854057280] #quality_metric: host=algo-1, epoch=123, batch=5 train loss <loss>=4.203369935353597\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:30 INFO 139919854057280] Epoch[123] Batch [5]#011Speed: 2628.56 samples/sec#011loss=4.203370\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:31 INFO 139919854057280] Epoch[123] Batch[10] avg_epoch_loss=4.286595\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:31 INFO 139919854057280] #quality_metric: host=algo-1, epoch=123, batch=10 train loss <loss>=4.386464977264405\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:31 INFO 139919854057280] Epoch[123] Batch [10]#011Speed: 2896.94 samples/sec#011loss=4.386465\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:31 INFO 139919854057280] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158870.6975055, \"EndTime\": 1681158871.066667, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 368.86048316955566, \"count\": 1, \"min\": 368.86048316955566, \"max\": 368.86048316955566}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:31 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1791.5353181447206 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:31 INFO 139919854057280] #progress_metric: host=algo-1, completed 31.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:31 INFO 139919854057280] #quality_metric: host=algo-1, epoch=123, train loss <loss>=4.286594954403964\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:31 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:31 INFO 139919854057280] Epoch[124] Batch[0] avg_epoch_loss=4.431434\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:31 INFO 139919854057280] #quality_metric: host=algo-1, epoch=124, batch=0 train loss <loss>=4.431434154510498\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:31 INFO 139919854057280] Epoch[124] Batch[5] avg_epoch_loss=4.061304\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:31 INFO 139919854057280] #quality_metric: host=algo-1, epoch=124, batch=5 train loss <loss>=4.061303933461507\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:31 INFO 139919854057280] Epoch[124] Batch [5]#011Speed: 3038.78 samples/sec#011loss=4.061304\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:31 INFO 139919854057280] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158871.0667334, \"EndTime\": 1681158871.3917818, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 324.7497081756592, \"count\": 1, \"min\": 324.7497081756592, \"max\": 324.7497081756592}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:31 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1896.2788156529218 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:31 INFO 139919854057280] #progress_metric: host=algo-1, completed 31.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:31 INFO 139919854057280] #quality_metric: host=algo-1, epoch=124, train loss <loss>=3.9804917097091677\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:31 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:31 INFO 139919854057280] Epoch[125] Batch[0] avg_epoch_loss=4.139319\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:31 INFO 139919854057280] #quality_metric: host=algo-1, epoch=125, batch=0 train loss <loss>=4.13931941986084\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:31 INFO 139919854057280] Epoch[125] Batch[5] avg_epoch_loss=4.128392\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:31 INFO 139919854057280] #quality_metric: host=algo-1, epoch=125, batch=5 train loss <loss>=4.128392418225606\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:31 INFO 139919854057280] Epoch[125] Batch [5]#011Speed: 3070.80 samples/sec#011loss=4.128392\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:31 INFO 139919854057280] Epoch[125] Batch[10] avg_epoch_loss=4.179719\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:31 INFO 139919854057280] #quality_metric: host=algo-1, epoch=125, batch=10 train loss <loss>=4.2413105964660645\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:31 INFO 139919854057280] Epoch[125] Batch [10]#011Speed: 2742.41 samples/sec#011loss=4.241311\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:31 INFO 139919854057280] processed a total of 673 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158871.3918493, \"EndTime\": 1681158871.7492478, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 356.9643497467041, \"count\": 1, \"min\": 356.9643497467041, \"max\": 356.9643497467041}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:31 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1884.8387752008691 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:31 INFO 139919854057280] #progress_metric: host=algo-1, completed 31.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:31 INFO 139919854057280] #quality_metric: host=algo-1, epoch=125, train loss <loss>=4.17971886288036\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:31 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:31 INFO 139919854057280] Epoch[126] Batch[0] avg_epoch_loss=4.117333\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:31 INFO 139919854057280] #quality_metric: host=algo-1, epoch=126, batch=0 train loss <loss>=4.117332935333252\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:32 INFO 139919854057280] Epoch[126] Batch[5] avg_epoch_loss=4.160039\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:32 INFO 139919854057280] #quality_metric: host=algo-1, epoch=126, batch=5 train loss <loss>=4.1600392659505205\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:32 INFO 139919854057280] Epoch[126] Batch [5]#011Speed: 2971.60 samples/sec#011loss=4.160039\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:32 INFO 139919854057280] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158871.7493143, \"EndTime\": 1681158872.106804, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 357.18441009521484, \"count\": 1, \"min\": 357.18441009521484, \"max\": 357.18441009521484}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:32 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1788.516711621594 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:32 INFO 139919854057280] #progress_metric: host=algo-1, completed 31.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:32 INFO 139919854057280] #quality_metric: host=algo-1, epoch=126, train loss <loss>=4.220527172088623\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:32 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:32 INFO 139919854057280] Epoch[127] Batch[0] avg_epoch_loss=4.022397\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:32 INFO 139919854057280] #quality_metric: host=algo-1, epoch=127, batch=0 train loss <loss>=4.022397041320801\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:32 INFO 139919854057280] Epoch[127] Batch[5] avg_epoch_loss=3.998551\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:32 INFO 139919854057280] #quality_metric: host=algo-1, epoch=127, batch=5 train loss <loss>=3.9985514084498086\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:32 INFO 139919854057280] Epoch[127] Batch [5]#011Speed: 2866.53 samples/sec#011loss=3.998551\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:32 INFO 139919854057280] processed a total of 604 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158872.1068695, \"EndTime\": 1681158872.4348416, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 327.55327224731445, \"count\": 1, \"min\": 327.55327224731445, \"max\": 327.55327224731445}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:32 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1843.4287535518986 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:32 INFO 139919854057280] #progress_metric: host=algo-1, completed 32.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:32 INFO 139919854057280] #quality_metric: host=algo-1, epoch=127, train loss <loss>=4.1066041707992555\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:32 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:32 INFO 139919854057280] Epoch[128] Batch[0] avg_epoch_loss=3.905691\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:32 INFO 139919854057280] #quality_metric: host=algo-1, epoch=128, batch=0 train loss <loss>=3.905691385269165\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:32 INFO 139919854057280] Epoch[128] Batch[5] avg_epoch_loss=4.181399\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:32 INFO 139919854057280] #quality_metric: host=algo-1, epoch=128, batch=5 train loss <loss>=4.18139926592509\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:32 INFO 139919854057280] Epoch[128] Batch [5]#011Speed: 3054.68 samples/sec#011loss=4.181399\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:32 INFO 139919854057280] Epoch[128] Batch[10] avg_epoch_loss=4.213449\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:32 INFO 139919854057280] #quality_metric: host=algo-1, epoch=128, batch=10 train loss <loss>=4.251909589767456\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:32 INFO 139919854057280] Epoch[128] Batch [10]#011Speed: 2742.61 samples/sec#011loss=4.251910\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:32 INFO 139919854057280] processed a total of 688 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158872.4349089, \"EndTime\": 1681158872.8124058, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 377.1495819091797, \"count\": 1, \"min\": 377.1495819091797, \"max\": 377.1495819091797}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:32 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1823.7164648096325 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:32 INFO 139919854057280] #progress_metric: host=algo-1, completed 32.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:32 INFO 139919854057280] #quality_metric: host=algo-1, epoch=128, train loss <loss>=4.213449413126165\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:32 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:32 INFO 139919854057280] Epoch[129] Batch[0] avg_epoch_loss=4.606877\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:32 INFO 139919854057280] #quality_metric: host=algo-1, epoch=129, batch=0 train loss <loss>=4.606877326965332\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:33 INFO 139919854057280] Epoch[129] Batch[5] avg_epoch_loss=4.215512\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:33 INFO 139919854057280] #quality_metric: host=algo-1, epoch=129, batch=5 train loss <loss>=4.2155115604400635\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:33 INFO 139919854057280] Epoch[129] Batch [5]#011Speed: 2539.38 samples/sec#011loss=4.215512\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:33 INFO 139919854057280] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158872.8124802, \"EndTime\": 1681158873.1624758, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 349.6894836425781, \"count\": 1, \"min\": 349.6894836425781, \"max\": 349.6894836425781}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:33 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1809.6604251995232 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:33 INFO 139919854057280] #progress_metric: host=algo-1, completed 32.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:33 INFO 139919854057280] #quality_metric: host=algo-1, epoch=129, train loss <loss>=4.1934737205505375\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:33 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:33 INFO 139919854057280] Epoch[130] Batch[0] avg_epoch_loss=4.066808\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:33 INFO 139919854057280] #quality_metric: host=algo-1, epoch=130, batch=0 train loss <loss>=4.066807746887207\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:33 INFO 139919854057280] Epoch[130] Batch[5] avg_epoch_loss=4.062198\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:33 INFO 139919854057280] #quality_metric: host=algo-1, epoch=130, batch=5 train loss <loss>=4.062197923660278\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:33 INFO 139919854057280] Epoch[130] Batch [5]#011Speed: 2976.95 samples/sec#011loss=4.062198\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:33 INFO 139919854057280] Epoch[130] Batch[10] avg_epoch_loss=3.927659\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:33 INFO 139919854057280] #quality_metric: host=algo-1, epoch=130, batch=10 train loss <loss>=3.766211462020874\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:33 INFO 139919854057280] Epoch[130] Batch [10]#011Speed: 2819.22 samples/sec#011loss=3.766211\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:33 INFO 139919854057280] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158873.1625452, \"EndTime\": 1681158873.5434132, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 380.4206848144531, \"count\": 1, \"min\": 380.4206848144531, \"max\": 380.4206848144531}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:33 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1731.8542660294083 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:33 INFO 139919854057280] #progress_metric: host=algo-1, completed 32.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:33 INFO 139919854057280] #quality_metric: host=algo-1, epoch=130, train loss <loss>=3.9276586229150947\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:33 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:33 INFO 139919854057280] Epoch[131] Batch[0] avg_epoch_loss=3.742196\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:33 INFO 139919854057280] #quality_metric: host=algo-1, epoch=131, batch=0 train loss <loss>=3.7421960830688477\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:33 INFO 139919854057280] Epoch[131] Batch[5] avg_epoch_loss=3.820414\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:33 INFO 139919854057280] #quality_metric: host=algo-1, epoch=131, batch=5 train loss <loss>=3.820414344469706\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:33 INFO 139919854057280] Epoch[131] Batch [5]#011Speed: 2832.57 samples/sec#011loss=3.820414\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:33 INFO 139919854057280] Epoch[131] Batch[10] avg_epoch_loss=3.859821\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:33 INFO 139919854057280] #quality_metric: host=algo-1, epoch=131, batch=10 train loss <loss>=3.907109832763672\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:33 INFO 139919854057280] Epoch[131] Batch [10]#011Speed: 2659.09 samples/sec#011loss=3.907110\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:33 INFO 139919854057280] processed a total of 699 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158873.5434809, \"EndTime\": 1681158873.909635, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 365.6589984893799, \"count\": 1, \"min\": 365.6589984893799, \"max\": 365.6589984893799}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:33 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1911.0974993840045 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:33 INFO 139919854057280] #progress_metric: host=algo-1, completed 33.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:33 INFO 139919854057280] #quality_metric: host=algo-1, epoch=131, train loss <loss>=3.859821384603327\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:33 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:34 INFO 139919854057280] Epoch[132] Batch[0] avg_epoch_loss=4.236325\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:34 INFO 139919854057280] #quality_metric: host=algo-1, epoch=132, batch=0 train loss <loss>=4.236325263977051\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:34 INFO 139919854057280] Epoch[132] Batch[5] avg_epoch_loss=4.116653\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:34 INFO 139919854057280] #quality_metric: host=algo-1, epoch=132, batch=5 train loss <loss>=4.116652687390645\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:34 INFO 139919854057280] Epoch[132] Batch [5]#011Speed: 3039.16 samples/sec#011loss=4.116653\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:34 INFO 139919854057280] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158873.909705, \"EndTime\": 1681158874.2642887, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 354.0358543395996, \"count\": 1, \"min\": 354.0358543395996, \"max\": 354.0358543395996}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:34 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1801.5447648503102 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:34 INFO 139919854057280] #progress_metric: host=algo-1, completed 33.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:34 INFO 139919854057280] #quality_metric: host=algo-1, epoch=132, train loss <loss>=4.169052481651306\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:34 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:34 INFO 139919854057280] Epoch[133] Batch[0] avg_epoch_loss=4.212841\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:34 INFO 139919854057280] #quality_metric: host=algo-1, epoch=133, batch=0 train loss <loss>=4.212841033935547\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:34 INFO 139919854057280] Epoch[133] Batch[5] avg_epoch_loss=4.211952\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:34 INFO 139919854057280] #quality_metric: host=algo-1, epoch=133, batch=5 train loss <loss>=4.211952368418376\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:34 INFO 139919854057280] Epoch[133] Batch [5]#011Speed: 2918.84 samples/sec#011loss=4.211952\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:34 INFO 139919854057280] Epoch[133] Batch[10] avg_epoch_loss=4.222896\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:34 INFO 139919854057280] #quality_metric: host=algo-1, epoch=133, batch=10 train loss <loss>=4.236028957366943\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:34 INFO 139919854057280] Epoch[133] Batch [10]#011Speed: 2868.37 samples/sec#011loss=4.236029\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:34 INFO 139919854057280] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158874.264363, \"EndTime\": 1681158874.6158679, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 350.9838581085205, \"count\": 1, \"min\": 350.9838581085205, \"max\": 350.9838581085205}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:34 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1865.6833592864512 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:34 INFO 139919854057280] #progress_metric: host=algo-1, completed 33.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:34 INFO 139919854057280] #quality_metric: host=algo-1, epoch=133, train loss <loss>=4.222896272485906\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:34 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:34 INFO 139919854057280] Epoch[134] Batch[0] avg_epoch_loss=4.399574\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:34 INFO 139919854057280] #quality_metric: host=algo-1, epoch=134, batch=0 train loss <loss>=4.399573802947998\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:34 INFO 139919854057280] Epoch[134] Batch[5] avg_epoch_loss=4.087713\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:34 INFO 139919854057280] #quality_metric: host=algo-1, epoch=134, batch=5 train loss <loss>=4.0877126057942705\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:34 INFO 139919854057280] Epoch[134] Batch [5]#011Speed: 3034.32 samples/sec#011loss=4.087713\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:34 INFO 139919854057280] Epoch[134] Batch[10] avg_epoch_loss=4.050943\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:34 INFO 139919854057280] #quality_metric: host=algo-1, epoch=134, batch=10 train loss <loss>=4.0068199634552\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:34 INFO 139919854057280] Epoch[134] Batch [10]#011Speed: 2630.79 samples/sec#011loss=4.006820\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:34 INFO 139919854057280] processed a total of 705 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158874.6159284, \"EndTime\": 1681158874.99526, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 378.86977195739746, \"count\": 1, \"min\": 378.86977195739746, \"max\": 378.86977195739746}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:34 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1860.3235734507707 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:34 INFO 139919854057280] #progress_metric: host=algo-1, completed 33.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:34 INFO 139919854057280] #quality_metric: host=algo-1, epoch=134, train loss <loss>=4.163227339585622\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:34 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:35 INFO 139919854057280] Epoch[135] Batch[0] avg_epoch_loss=4.057788\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:35 INFO 139919854057280] #quality_metric: host=algo-1, epoch=135, batch=0 train loss <loss>=4.057788372039795\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:35 INFO 139919854057280] Epoch[135] Batch[5] avg_epoch_loss=4.117678\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:35 INFO 139919854057280] #quality_metric: host=algo-1, epoch=135, batch=5 train loss <loss>=4.1176778475443525\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:35 INFO 139919854057280] Epoch[135] Batch [5]#011Speed: 3047.90 samples/sec#011loss=4.117678\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:35 INFO 139919854057280] Epoch[135] Batch[10] avg_epoch_loss=4.186694\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:35 INFO 139919854057280] #quality_metric: host=algo-1, epoch=135, batch=10 train loss <loss>=4.2695128440856935\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:35 INFO 139919854057280] Epoch[135] Batch [10]#011Speed: 2862.16 samples/sec#011loss=4.269513\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:35 INFO 139919854057280] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158874.995326, \"EndTime\": 1681158875.3704667, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 374.7999668121338, \"count\": 1, \"min\": 374.7999668121338, \"max\": 374.7999668121338}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:35 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1789.8522419117685 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:35 INFO 139919854057280] #progress_metric: host=algo-1, completed 34.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:35 INFO 139919854057280] #quality_metric: host=algo-1, epoch=135, train loss <loss>=4.186693755063144\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:35 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:35 INFO 139919854057280] Epoch[136] Batch[0] avg_epoch_loss=4.193698\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:35 INFO 139919854057280] #quality_metric: host=algo-1, epoch=136, batch=0 train loss <loss>=4.193698406219482\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:35 INFO 139919854057280] Epoch[136] Batch[5] avg_epoch_loss=4.129716\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:35 INFO 139919854057280] #quality_metric: host=algo-1, epoch=136, batch=5 train loss <loss>=4.12971568107605\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:35 INFO 139919854057280] Epoch[136] Batch [5]#011Speed: 2915.34 samples/sec#011loss=4.129716\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:35 INFO 139919854057280] Epoch[136] Batch[10] avg_epoch_loss=4.146798\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:35 INFO 139919854057280] #quality_metric: host=algo-1, epoch=136, batch=10 train loss <loss>=4.16729736328125\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:35 INFO 139919854057280] Epoch[136] Batch [10]#011Speed: 2959.64 samples/sec#011loss=4.167297\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:35 INFO 139919854057280] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158875.3705301, \"EndTime\": 1681158875.7236323, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 352.7538776397705, \"count\": 1, \"min\": 352.7538776397705, \"max\": 352.7538776397705}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:35 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1853.2623865558028 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:35 INFO 139919854057280] #progress_metric: host=algo-1, completed 34.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:35 INFO 139919854057280] #quality_metric: host=algo-1, epoch=136, train loss <loss>=4.146798263896596\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:35 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:35 INFO 139919854057280] Epoch[137] Batch[0] avg_epoch_loss=4.373396\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:35 INFO 139919854057280] #quality_metric: host=algo-1, epoch=137, batch=0 train loss <loss>=4.373395919799805\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:35 INFO 139919854057280] Epoch[137] Batch[5] avg_epoch_loss=4.270137\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:35 INFO 139919854057280] #quality_metric: host=algo-1, epoch=137, batch=5 train loss <loss>=4.270137389500936\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:35 INFO 139919854057280] Epoch[137] Batch [5]#011Speed: 2584.13 samples/sec#011loss=4.270137\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:36 INFO 139919854057280] Epoch[137] Batch[10] avg_epoch_loss=4.116526\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:36 INFO 139919854057280] #quality_metric: host=algo-1, epoch=137, batch=10 train loss <loss>=3.932191801071167\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:36 INFO 139919854057280] Epoch[137] Batch [10]#011Speed: 2952.54 samples/sec#011loss=3.932192\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:36 INFO 139919854057280] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158875.7237415, \"EndTime\": 1681158876.095013, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 370.91565132141113, \"count\": 1, \"min\": 370.91565132141113, \"max\": 370.91565132141113}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:36 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1730.3957078494714 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:36 INFO 139919854057280] #progress_metric: host=algo-1, completed 34.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:36 INFO 139919854057280] #quality_metric: host=algo-1, epoch=137, train loss <loss>=4.116525758396495\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:36 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:36 INFO 139919854057280] Epoch[138] Batch[0] avg_epoch_loss=4.230789\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:36 INFO 139919854057280] #quality_metric: host=algo-1, epoch=138, batch=0 train loss <loss>=4.2307891845703125\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:36 INFO 139919854057280] Epoch[138] Batch[5] avg_epoch_loss=4.076881\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:36 INFO 139919854057280] #quality_metric: host=algo-1, epoch=138, batch=5 train loss <loss>=4.076881289482117\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:36 INFO 139919854057280] Epoch[138] Batch [5]#011Speed: 3032.22 samples/sec#011loss=4.076881\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:36 INFO 139919854057280] Epoch[138] Batch[10] avg_epoch_loss=4.112540\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:36 INFO 139919854057280] #quality_metric: host=algo-1, epoch=138, batch=10 train loss <loss>=4.155330657958984\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:36 INFO 139919854057280] Epoch[138] Batch [10]#011Speed: 2836.03 samples/sec#011loss=4.155331\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:36 INFO 139919854057280] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158876.095083, \"EndTime\": 1681158876.4391768, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 343.72472763061523, \"count\": 1, \"min\": 343.72472763061523, \"max\": 343.72472763061523}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:36 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1890.5183449021906 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:36 INFO 139919854057280] #progress_metric: host=algo-1, completed 34.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:36 INFO 139919854057280] #quality_metric: host=algo-1, epoch=138, train loss <loss>=4.112540093335238\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:36 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:36 INFO 139919854057280] Epoch[139] Batch[0] avg_epoch_loss=5.125580\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:36 INFO 139919854057280] #quality_metric: host=algo-1, epoch=139, batch=0 train loss <loss>=5.125579833984375\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:36 INFO 139919854057280] Epoch[139] Batch[5] avg_epoch_loss=4.415163\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:36 INFO 139919854057280] #quality_metric: host=algo-1, epoch=139, batch=5 train loss <loss>=4.415163000424703\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:36 INFO 139919854057280] Epoch[139] Batch [5]#011Speed: 3084.44 samples/sec#011loss=4.415163\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:36 INFO 139919854057280] Epoch[139] Batch[10] avg_epoch_loss=4.243144\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:36 INFO 139919854057280] #quality_metric: host=algo-1, epoch=139, batch=10 train loss <loss>=4.036720561981201\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:36 INFO 139919854057280] Epoch[139] Batch [10]#011Speed: 2350.49 samples/sec#011loss=4.036721\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:36 INFO 139919854057280] processed a total of 694 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158876.439243, \"EndTime\": 1681158876.8024378, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 362.7052307128906, \"count\": 1, \"min\": 362.7052307128906, \"max\": 362.7052307128906}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:36 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1912.8839447304538 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:36 INFO 139919854057280] #progress_metric: host=algo-1, completed 35.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:36 INFO 139919854057280] #quality_metric: host=algo-1, epoch=139, train loss <loss>=4.243143710223111\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:36 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:36 INFO 139919854057280] Epoch[140] Batch[0] avg_epoch_loss=3.718777\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:36 INFO 139919854057280] #quality_metric: host=algo-1, epoch=140, batch=0 train loss <loss>=3.7187769412994385\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:37 INFO 139919854057280] Epoch[140] Batch[5] avg_epoch_loss=3.888070\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:37 INFO 139919854057280] #quality_metric: host=algo-1, epoch=140, batch=5 train loss <loss>=3.8880701462427774\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:37 INFO 139919854057280] Epoch[140] Batch [5]#011Speed: 2534.77 samples/sec#011loss=3.888070\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:37 INFO 139919854057280] Epoch[140] Batch[10] avg_epoch_loss=3.932402\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:37 INFO 139919854057280] #quality_metric: host=algo-1, epoch=140, batch=10 train loss <loss>=3.985599660873413\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:37 INFO 139919854057280] Epoch[140] Batch [10]#011Speed: 2440.41 samples/sec#011loss=3.985600\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:37 INFO 139919854057280] processed a total of 678 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158876.8025026, \"EndTime\": 1681158877.1914954, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 388.638973236084, \"count\": 1, \"min\": 388.638973236084, \"max\": 388.638973236084}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:37 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1744.0981691415843 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:37 INFO 139919854057280] #progress_metric: host=algo-1, completed 35.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:37 INFO 139919854057280] #quality_metric: host=algo-1, epoch=140, train loss <loss>=3.9324017438021572\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:37 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:37 INFO 139919854057280] Epoch[141] Batch[0] avg_epoch_loss=3.813385\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:37 INFO 139919854057280] #quality_metric: host=algo-1, epoch=141, batch=0 train loss <loss>=3.813384532928467\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:37 INFO 139919854057280] Epoch[141] Batch[5] avg_epoch_loss=3.905902\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:37 INFO 139919854057280] #quality_metric: host=algo-1, epoch=141, batch=5 train loss <loss>=3.9059024651845298\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:37 INFO 139919854057280] Epoch[141] Batch [5]#011Speed: 2671.14 samples/sec#011loss=3.905902\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:37 INFO 139919854057280] Epoch[141] Batch[10] avg_epoch_loss=4.007007\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:37 INFO 139919854057280] #quality_metric: host=algo-1, epoch=141, batch=10 train loss <loss>=4.128331708908081\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:37 INFO 139919854057280] Epoch[141] Batch [10]#011Speed: 2913.74 samples/sec#011loss=4.128332\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:37 INFO 139919854057280] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158877.1915648, \"EndTime\": 1681158877.5501752, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 358.2782745361328, \"count\": 1, \"min\": 358.2782745361328, \"max\": 358.2782745361328}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:37 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1819.3750132227524 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:37 INFO 139919854057280] #progress_metric: host=algo-1, completed 35.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:37 INFO 139919854057280] #quality_metric: host=algo-1, epoch=141, train loss <loss>=4.007006666877053\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:37 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:37 INFO 139919854057280] Epoch[142] Batch[0] avg_epoch_loss=4.258063\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:37 INFO 139919854057280] #quality_metric: host=algo-1, epoch=142, batch=0 train loss <loss>=4.258063316345215\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:37 INFO 139919854057280] Epoch[142] Batch[5] avg_epoch_loss=4.034304\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:37 INFO 139919854057280] #quality_metric: host=algo-1, epoch=142, batch=5 train loss <loss>=4.034304062525432\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:37 INFO 139919854057280] Epoch[142] Batch [5]#011Speed: 2807.26 samples/sec#011loss=4.034304\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:37 INFO 139919854057280] Epoch[142] Batch[10] avg_epoch_loss=4.067592\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:37 INFO 139919854057280] #quality_metric: host=algo-1, epoch=142, batch=10 train loss <loss>=4.107537269592285\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:37 INFO 139919854057280] Epoch[142] Batch [10]#011Speed: 2941.52 samples/sec#011loss=4.107537\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:37 INFO 139919854057280] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158877.550235, \"EndTime\": 1681158877.9008024, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 349.98464584350586, \"count\": 1, \"min\": 349.98464584350586, \"max\": 349.98464584350586}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:37 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1845.2773540985347 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:37 INFO 139919854057280] #progress_metric: host=algo-1, completed 35.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:37 INFO 139919854057280] #quality_metric: host=algo-1, epoch=142, train loss <loss>=4.067591883919456\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:37 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:38 INFO 139919854057280] Epoch[143] Batch[0] avg_epoch_loss=3.715236\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:38 INFO 139919854057280] #quality_metric: host=algo-1, epoch=143, batch=0 train loss <loss>=3.715235710144043\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:38 INFO 139919854057280] Epoch[143] Batch[5] avg_epoch_loss=3.965500\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:38 INFO 139919854057280] #quality_metric: host=algo-1, epoch=143, batch=5 train loss <loss>=3.9655000368754068\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:38 INFO 139919854057280] Epoch[143] Batch [5]#011Speed: 2986.78 samples/sec#011loss=3.965500\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:38 INFO 139919854057280] Epoch[143] Batch[10] avg_epoch_loss=4.019541\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:38 INFO 139919854057280] #quality_metric: host=algo-1, epoch=143, batch=10 train loss <loss>=4.084390640258789\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:38 INFO 139919854057280] Epoch[143] Batch [10]#011Speed: 2842.00 samples/sec#011loss=4.084391\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:38 INFO 139919854057280] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158877.9008696, \"EndTime\": 1681158878.2609594, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 359.68899726867676, \"count\": 1, \"min\": 359.68899726867676, \"max\": 359.68899726867676}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:38 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1873.2090270442911 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:38 INFO 139919854057280] #progress_metric: host=algo-1, completed 36.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:38 INFO 139919854057280] #quality_metric: host=algo-1, epoch=143, train loss <loss>=4.019541220231489\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:38 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:38 INFO 139919854057280] Epoch[144] Batch[0] avg_epoch_loss=4.193396\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:38 INFO 139919854057280] #quality_metric: host=algo-1, epoch=144, batch=0 train loss <loss>=4.193396091461182\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:38 INFO 139919854057280] Epoch[144] Batch[5] avg_epoch_loss=4.070591\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:38 INFO 139919854057280] #quality_metric: host=algo-1, epoch=144, batch=5 train loss <loss>=4.0705907344818115\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:38 INFO 139919854057280] Epoch[144] Batch [5]#011Speed: 2862.01 samples/sec#011loss=4.070591\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:38 INFO 139919854057280] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158878.2610476, \"EndTime\": 1681158878.5865767, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 325.09827613830566, \"count\": 1, \"min\": 325.09827613830566, \"max\": 325.09827613830566}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:38 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1964.9297550289664 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:38 INFO 139919854057280] #progress_metric: host=algo-1, completed 36.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:38 INFO 139919854057280] #quality_metric: host=algo-1, epoch=144, train loss <loss>=4.0690075874328615\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:38 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:38 INFO 139919854057280] Epoch[145] Batch[0] avg_epoch_loss=3.593235\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:38 INFO 139919854057280] #quality_metric: host=algo-1, epoch=145, batch=0 train loss <loss>=3.593235492706299\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:38 INFO 139919854057280] Epoch[145] Batch[5] avg_epoch_loss=3.835863\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:38 INFO 139919854057280] #quality_metric: host=algo-1, epoch=145, batch=5 train loss <loss>=3.8358625570933023\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:38 INFO 139919854057280] Epoch[145] Batch [5]#011Speed: 2950.01 samples/sec#011loss=3.835863\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:38 INFO 139919854057280] Epoch[145] Batch[10] avg_epoch_loss=3.691746\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:38 INFO 139919854057280] #quality_metric: host=algo-1, epoch=145, batch=10 train loss <loss>=3.518805980682373\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:38 INFO 139919854057280] Epoch[145] Batch [10]#011Speed: 2844.90 samples/sec#011loss=3.518806\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:38 INFO 139919854057280] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158878.586648, \"EndTime\": 1681158878.9420996, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 354.9332618713379, \"count\": 1, \"min\": 354.9332618713379, \"max\": 354.9332618713379}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:38 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1864.6145655445475 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:38 INFO 139919854057280] #progress_metric: host=algo-1, completed 36.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:38 INFO 139919854057280] #quality_metric: host=algo-1, epoch=145, train loss <loss>=3.6917459314519707\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:38 INFO 139919854057280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:38 INFO 139919854057280] Saved checkpoint to \"/opt/ml/model/state_6aba98b1-541d-4c47-b51d-ea5598296e70-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158878.9421666, \"EndTime\": 1681158878.9496303, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 7.043123245239258, \"count\": 1, \"min\": 7.043123245239258, \"max\": 7.043123245239258}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:39 INFO 139919854057280] Epoch[146] Batch[0] avg_epoch_loss=4.055159\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:39 INFO 139919854057280] #quality_metric: host=algo-1, epoch=146, batch=0 train loss <loss>=4.055159091949463\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:39 INFO 139919854057280] Epoch[146] Batch[5] avg_epoch_loss=4.280896\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:39 INFO 139919854057280] #quality_metric: host=algo-1, epoch=146, batch=5 train loss <loss>=4.280896266301473\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:39 INFO 139919854057280] Epoch[146] Batch [5]#011Speed: 2905.88 samples/sec#011loss=4.280896\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:39 INFO 139919854057280] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158878.9496868, \"EndTime\": 1681158879.275575, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 325.8357048034668, \"count\": 1, \"min\": 325.8357048034668, \"max\": 325.8357048034668}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:39 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1880.735490931009 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:39 INFO 139919854057280] #progress_metric: host=algo-1, completed 36.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:39 INFO 139919854057280] #quality_metric: host=algo-1, epoch=146, train loss <loss>=4.350207901000976\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:39 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:39 INFO 139919854057280] Epoch[147] Batch[0] avg_epoch_loss=3.862382\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:39 INFO 139919854057280] #quality_metric: host=algo-1, epoch=147, batch=0 train loss <loss>=3.86238169670105\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:39 INFO 139919854057280] Epoch[147] Batch[5] avg_epoch_loss=3.888761\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:39 INFO 139919854057280] #quality_metric: host=algo-1, epoch=147, batch=5 train loss <loss>=3.8887606859207153\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:39 INFO 139919854057280] Epoch[147] Batch [5]#011Speed: 2949.24 samples/sec#011loss=3.888761\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:39 INFO 139919854057280] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158879.275646, \"EndTime\": 1681158879.633194, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 357.18488693237305, \"count\": 1, \"min\": 357.18488693237305, \"max\": 357.18488693237305}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:39 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1777.2889664445654 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:39 INFO 139919854057280] #progress_metric: host=algo-1, completed 37.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:39 INFO 139919854057280] #quality_metric: host=algo-1, epoch=147, train loss <loss>=3.982056975364685\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:39 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:39 INFO 139919854057280] Epoch[148] Batch[0] avg_epoch_loss=4.163832\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:39 INFO 139919854057280] #quality_metric: host=algo-1, epoch=148, batch=0 train loss <loss>=4.16383171081543\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:39 INFO 139919854057280] Epoch[148] Batch[5] avg_epoch_loss=4.004088\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:39 INFO 139919854057280] #quality_metric: host=algo-1, epoch=148, batch=5 train loss <loss>=4.004087885220845\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:39 INFO 139919854057280] Epoch[148] Batch [5]#011Speed: 2958.40 samples/sec#011loss=4.004088\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:39 INFO 139919854057280] Epoch[148] Batch[10] avg_epoch_loss=4.076483\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:39 INFO 139919854057280] #quality_metric: host=algo-1, epoch=148, batch=10 train loss <loss>=4.163357210159302\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:39 INFO 139919854057280] Epoch[148] Batch [10]#011Speed: 2819.29 samples/sec#011loss=4.163357\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:39 INFO 139919854057280] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158879.6332624, \"EndTime\": 1681158879.9875667, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 353.63173484802246, \"count\": 1, \"min\": 353.63173484802246, \"max\": 353.63173484802246}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:39 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1885.6178996152737 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:39 INFO 139919854057280] #progress_metric: host=algo-1, completed 37.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:39 INFO 139919854057280] #quality_metric: host=algo-1, epoch=148, train loss <loss>=4.076483032920144\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:39 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:40 INFO 139919854057280] Epoch[149] Batch[0] avg_epoch_loss=4.215886\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:40 INFO 139919854057280] #quality_metric: host=algo-1, epoch=149, batch=0 train loss <loss>=4.215885639190674\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:40 INFO 139919854057280] Epoch[149] Batch[5] avg_epoch_loss=4.243949\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:40 INFO 139919854057280] #quality_metric: host=algo-1, epoch=149, batch=5 train loss <loss>=4.243949254353841\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:40 INFO 139919854057280] Epoch[149] Batch [5]#011Speed: 3068.44 samples/sec#011loss=4.243949\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:40 INFO 139919854057280] Epoch[149] Batch[10] avg_epoch_loss=4.228851\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:40 INFO 139919854057280] #quality_metric: host=algo-1, epoch=149, batch=10 train loss <loss>=4.210733795166016\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:40 INFO 139919854057280] Epoch[149] Batch [10]#011Speed: 2565.81 samples/sec#011loss=4.210734\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:40 INFO 139919854057280] processed a total of 692 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158879.987637, \"EndTime\": 1681158880.3445213, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 356.49561882019043, \"count\": 1, \"min\": 356.49561882019043, \"max\": 356.49561882019043}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:40 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1940.5896053777 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:40 INFO 139919854057280] #progress_metric: host=algo-1, completed 37.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:40 INFO 139919854057280] #quality_metric: host=algo-1, epoch=149, train loss <loss>=4.228851318359375\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:40 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:40 INFO 139919854057280] Epoch[150] Batch[0] avg_epoch_loss=3.606560\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:40 INFO 139919854057280] #quality_metric: host=algo-1, epoch=150, batch=0 train loss <loss>=3.606560230255127\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:40 INFO 139919854057280] Epoch[150] Batch[5] avg_epoch_loss=3.897652\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:40 INFO 139919854057280] #quality_metric: host=algo-1, epoch=150, batch=5 train loss <loss>=3.8976524670918784\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:40 INFO 139919854057280] Epoch[150] Batch [5]#011Speed: 2626.49 samples/sec#011loss=3.897652\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:40 INFO 139919854057280] Epoch[150] Batch[10] avg_epoch_loss=3.979659\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:40 INFO 139919854057280] #quality_metric: host=algo-1, epoch=150, batch=10 train loss <loss>=4.078067111968994\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:40 INFO 139919854057280] Epoch[150] Batch [10]#011Speed: 2718.31 samples/sec#011loss=4.078067\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:40 INFO 139919854057280] processed a total of 722 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158880.3445888, \"EndTime\": 1681158880.753537, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 408.4181785583496, \"count\": 1, \"min\": 408.4181785583496, \"max\": 408.4181785583496}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:40 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1766.8223018552208 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:40 INFO 139919854057280] #progress_metric: host=algo-1, completed 37.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:40 INFO 139919854057280] #quality_metric: host=algo-1, epoch=150, train loss <loss>=3.8879299561182656\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:40 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:40 INFO 139919854057280] Epoch[151] Batch[0] avg_epoch_loss=4.997965\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:40 INFO 139919854057280] #quality_metric: host=algo-1, epoch=151, batch=0 train loss <loss>=4.997964859008789\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:40 INFO 139919854057280] Epoch[151] Batch[5] avg_epoch_loss=4.466219\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:40 INFO 139919854057280] #quality_metric: host=algo-1, epoch=151, batch=5 train loss <loss>=4.466218829154968\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:40 INFO 139919854057280] Epoch[151] Batch [5]#011Speed: 2941.12 samples/sec#011loss=4.466219\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:41 INFO 139919854057280] Epoch[151] Batch[10] avg_epoch_loss=4.165855\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:41 INFO 139919854057280] #quality_metric: host=algo-1, epoch=151, batch=10 train loss <loss>=3.8054193019866944\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:41 INFO 139919854057280] Epoch[151] Batch [10]#011Speed: 2814.60 samples/sec#011loss=3.805419\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:41 INFO 139919854057280] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158880.7537246, \"EndTime\": 1681158881.1041944, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 350.0022888183594, \"count\": 1, \"min\": 350.0022888183594, \"max\": 350.0022888183594}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:41 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1896.627875824449 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:41 INFO 139919854057280] #progress_metric: host=algo-1, completed 38.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:41 INFO 139919854057280] #quality_metric: host=algo-1, epoch=151, train loss <loss>=4.165855407714844\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:41 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:41 INFO 139919854057280] Epoch[152] Batch[0] avg_epoch_loss=4.399380\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:41 INFO 139919854057280] #quality_metric: host=algo-1, epoch=152, batch=0 train loss <loss>=4.399379730224609\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:41 INFO 139919854057280] Epoch[152] Batch[5] avg_epoch_loss=4.070889\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:41 INFO 139919854057280] #quality_metric: host=algo-1, epoch=152, batch=5 train loss <loss>=4.070888916651408\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:41 INFO 139919854057280] Epoch[152] Batch [5]#011Speed: 3037.30 samples/sec#011loss=4.070889\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:41 INFO 139919854057280] Epoch[152] Batch[10] avg_epoch_loss=4.129795\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:41 INFO 139919854057280] #quality_metric: host=algo-1, epoch=152, batch=10 train loss <loss>=4.200481986999511\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:41 INFO 139919854057280] Epoch[152] Batch [10]#011Speed: 2993.10 samples/sec#011loss=4.200482\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:41 INFO 139919854057280] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158881.1042595, \"EndTime\": 1681158881.4465647, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 342.00143814086914, \"count\": 1, \"min\": 342.00143814086914, \"max\": 342.00143814086914}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:41 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1879.129178668428 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:41 INFO 139919854057280] #progress_metric: host=algo-1, completed 38.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:41 INFO 139919854057280] #quality_metric: host=algo-1, epoch=152, train loss <loss>=4.129794857718728\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:41 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:41 INFO 139919854057280] Epoch[153] Batch[0] avg_epoch_loss=4.133781\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:41 INFO 139919854057280] #quality_metric: host=algo-1, epoch=153, batch=0 train loss <loss>=4.1337809562683105\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:41 INFO 139919854057280] Epoch[153] Batch[5] avg_epoch_loss=3.882704\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:41 INFO 139919854057280] #quality_metric: host=algo-1, epoch=153, batch=5 train loss <loss>=3.8827041387557983\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:41 INFO 139919854057280] Epoch[153] Batch [5]#011Speed: 3049.51 samples/sec#011loss=3.882704\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:41 INFO 139919854057280] Epoch[153] Batch[10] avg_epoch_loss=3.951388\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:41 INFO 139919854057280] #quality_metric: host=algo-1, epoch=153, batch=10 train loss <loss>=4.033808994293213\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:41 INFO 139919854057280] Epoch[153] Batch [10]#011Speed: 2647.23 samples/sec#011loss=4.033809\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:41 INFO 139919854057280] processed a total of 680 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158881.44671, \"EndTime\": 1681158881.7995434, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 352.4148464202881, \"count\": 1, \"min\": 352.4148464202881, \"max\": 352.4148464202881}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:41 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1929.0182058222379 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:41 INFO 139919854057280] #progress_metric: host=algo-1, completed 38.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:41 INFO 139919854057280] #quality_metric: host=algo-1, epoch=153, train loss <loss>=3.9513881640000776\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:41 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:41 INFO 139919854057280] Epoch[154] Batch[0] avg_epoch_loss=4.601523\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:41 INFO 139919854057280] #quality_metric: host=algo-1, epoch=154, batch=0 train loss <loss>=4.601522922515869\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:42 INFO 139919854057280] Epoch[154] Batch[5] avg_epoch_loss=4.330729\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:42 INFO 139919854057280] #quality_metric: host=algo-1, epoch=154, batch=5 train loss <loss>=4.330729166666667\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:42 INFO 139919854057280] Epoch[154] Batch [5]#011Speed: 3067.58 samples/sec#011loss=4.330729\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:42 INFO 139919854057280] Epoch[154] Batch[10] avg_epoch_loss=3.968937\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:42 INFO 139919854057280] #quality_metric: host=algo-1, epoch=154, batch=10 train loss <loss>=3.5347859859466553\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:42 INFO 139919854057280] Epoch[154] Batch [10]#011Speed: 2704.41 samples/sec#011loss=3.534786\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:42 INFO 139919854057280] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158881.799611, \"EndTime\": 1681158882.1500726, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 350.0959873199463, \"count\": 1, \"min\": 350.0959873199463, \"max\": 350.0959873199463}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:42 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1864.6973441711539 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:42 INFO 139919854057280] #progress_metric: host=algo-1, completed 38.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:42 INFO 139919854057280] #quality_metric: host=algo-1, epoch=154, train loss <loss>=3.968936811793934\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:42 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:42 INFO 139919854057280] Epoch[155] Batch[0] avg_epoch_loss=4.301300\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:42 INFO 139919854057280] #quality_metric: host=algo-1, epoch=155, batch=0 train loss <loss>=4.301299571990967\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:42 INFO 139919854057280] Epoch[155] Batch[5] avg_epoch_loss=4.152466\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:42 INFO 139919854057280] #quality_metric: host=algo-1, epoch=155, batch=5 train loss <loss>=4.152466098467509\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:42 INFO 139919854057280] Epoch[155] Batch [5]#011Speed: 2850.53 samples/sec#011loss=4.152466\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:42 INFO 139919854057280] Epoch[155] Batch[10] avg_epoch_loss=4.133842\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:42 INFO 139919854057280] #quality_metric: host=algo-1, epoch=155, batch=10 train loss <loss>=4.111493873596191\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:42 INFO 139919854057280] Epoch[155] Batch [10]#011Speed: 2841.33 samples/sec#011loss=4.111494\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:42 INFO 139919854057280] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158882.15014, \"EndTime\": 1681158882.5096238, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 358.95466804504395, \"count\": 1, \"min\": 358.95466804504395, \"max\": 358.95466804504395}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:42 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1849.285128722036 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:42 INFO 139919854057280] #progress_metric: host=algo-1, completed 39.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:42 INFO 139919854057280] #quality_metric: host=algo-1, epoch=155, train loss <loss>=4.133842359889638\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:42 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:42 INFO 139919854057280] Epoch[156] Batch[0] avg_epoch_loss=3.873861\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:42 INFO 139919854057280] #quality_metric: host=algo-1, epoch=156, batch=0 train loss <loss>=3.873861074447632\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:42 INFO 139919854057280] Epoch[156] Batch[5] avg_epoch_loss=3.963376\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:42 INFO 139919854057280] #quality_metric: host=algo-1, epoch=156, batch=5 train loss <loss>=3.9633758068084717\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:42 INFO 139919854057280] Epoch[156] Batch [5]#011Speed: 2369.10 samples/sec#011loss=3.963376\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:42 INFO 139919854057280] Epoch[156] Batch[10] avg_epoch_loss=4.004642\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:42 INFO 139919854057280] #quality_metric: host=algo-1, epoch=156, batch=10 train loss <loss>=4.054160404205322\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:42 INFO 139919854057280] Epoch[156] Batch [10]#011Speed: 2904.22 samples/sec#011loss=4.054160\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:42 INFO 139919854057280] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158882.5096982, \"EndTime\": 1681158882.8861213, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 376.04498863220215, \"count\": 1, \"min\": 376.04498863220215, \"max\": 376.04498863220215}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:42 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1746.6396956057936 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:42 INFO 139919854057280] #progress_metric: host=algo-1, completed 39.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:42 INFO 139919854057280] #quality_metric: host=algo-1, epoch=156, train loss <loss>=4.004641532897949\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:42 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:43 INFO 139919854057280] Epoch[157] Batch[0] avg_epoch_loss=3.885710\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:43 INFO 139919854057280] #quality_metric: host=algo-1, epoch=157, batch=0 train loss <loss>=3.8857104778289795\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:43 INFO 139919854057280] Epoch[157] Batch[5] avg_epoch_loss=3.908023\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:43 INFO 139919854057280] #quality_metric: host=algo-1, epoch=157, batch=5 train loss <loss>=3.908022880554199\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:43 INFO 139919854057280] Epoch[157] Batch [5]#011Speed: 2722.01 samples/sec#011loss=3.908023\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:43 INFO 139919854057280] Epoch[157] Batch[10] avg_epoch_loss=3.929768\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:43 INFO 139919854057280] #quality_metric: host=algo-1, epoch=157, batch=10 train loss <loss>=3.9558613777160643\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:43 INFO 139919854057280] Epoch[157] Batch [10]#011Speed: 2762.22 samples/sec#011loss=3.955861\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:43 INFO 139919854057280] processed a total of 678 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158882.8861945, \"EndTime\": 1681158883.248942, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 362.26963996887207, \"count\": 1, \"min\": 362.26963996887207, \"max\": 362.26963996887207}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:43 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1870.9086466439953 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:43 INFO 139919854057280] #progress_metric: host=algo-1, completed 39.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:43 INFO 139919854057280] #quality_metric: host=algo-1, epoch=157, train loss <loss>=3.9297676519914106\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:43 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:43 INFO 139919854057280] Epoch[158] Batch[0] avg_epoch_loss=4.986381\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:43 INFO 139919854057280] #quality_metric: host=algo-1, epoch=158, batch=0 train loss <loss>=4.986380577087402\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:43 INFO 139919854057280] Epoch[158] Batch[5] avg_epoch_loss=4.457860\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:43 INFO 139919854057280] #quality_metric: host=algo-1, epoch=158, batch=5 train loss <loss>=4.4578596750895185\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:43 INFO 139919854057280] Epoch[158] Batch [5]#011Speed: 2860.19 samples/sec#011loss=4.457860\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:43 INFO 139919854057280] Epoch[158] Batch[10] avg_epoch_loss=4.397148\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:43 INFO 139919854057280] #quality_metric: host=algo-1, epoch=158, batch=10 train loss <loss>=4.324293947219848\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:43 INFO 139919854057280] Epoch[158] Batch [10]#011Speed: 2776.53 samples/sec#011loss=4.324294\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:43 INFO 139919854057280] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158883.2490294, \"EndTime\": 1681158883.6038535, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 354.4297218322754, \"count\": 1, \"min\": 354.4297218322754, \"max\": 354.4297218322754}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:43 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1850.3664292059827 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:43 INFO 139919854057280] #progress_metric: host=algo-1, completed 39.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:43 INFO 139919854057280] #quality_metric: host=algo-1, epoch=158, train loss <loss>=4.397147980603305\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:43 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:43 INFO 139919854057280] Epoch[159] Batch[0] avg_epoch_loss=3.982107\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:43 INFO 139919854057280] #quality_metric: host=algo-1, epoch=159, batch=0 train loss <loss>=3.982107162475586\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:43 INFO 139919854057280] Epoch[159] Batch[5] avg_epoch_loss=4.010273\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:43 INFO 139919854057280] #quality_metric: host=algo-1, epoch=159, batch=5 train loss <loss>=4.010272582372029\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:43 INFO 139919854057280] Epoch[159] Batch [5]#011Speed: 2909.84 samples/sec#011loss=4.010273\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:43 INFO 139919854057280] Epoch[159] Batch[10] avg_epoch_loss=4.067548\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:43 INFO 139919854057280] #quality_metric: host=algo-1, epoch=159, batch=10 train loss <loss>=4.136278676986694\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:43 INFO 139919854057280] Epoch[159] Batch [10]#011Speed: 2768.84 samples/sec#011loss=4.136279\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:43 INFO 139919854057280] processed a total of 697 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158883.6039202, \"EndTime\": 1681158883.9598951, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 355.6699752807617, \"count\": 1, \"min\": 355.6699752807617, \"max\": 355.6699752807617}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:43 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1959.1381649401355 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:43 INFO 139919854057280] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:43 INFO 139919854057280] #quality_metric: host=algo-1, epoch=159, train loss <loss>=4.06754807992415\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:43 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:44 INFO 139919854057280] Epoch[160] Batch[0] avg_epoch_loss=4.044222\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:44 INFO 139919854057280] #quality_metric: host=algo-1, epoch=160, batch=0 train loss <loss>=4.044221878051758\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:44 INFO 139919854057280] Epoch[160] Batch[5] avg_epoch_loss=4.115435\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:44 INFO 139919854057280] #quality_metric: host=algo-1, epoch=160, batch=5 train loss <loss>=4.115434527397156\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:44 INFO 139919854057280] Epoch[160] Batch [5]#011Speed: 3023.00 samples/sec#011loss=4.115435\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:44 INFO 139919854057280] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158883.9599636, \"EndTime\": 1681158884.2819273, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 321.58923149108887, \"count\": 1, \"min\": 321.58923149108887, \"max\": 321.58923149108887}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:44 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1985.9511663060553 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:44 INFO 139919854057280] #progress_metric: host=algo-1, completed 40.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:44 INFO 139919854057280] #quality_metric: host=algo-1, epoch=160, train loss <loss>=4.054167985916138\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:44 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:44 INFO 139919854057280] Epoch[161] Batch[0] avg_epoch_loss=3.704751\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:44 INFO 139919854057280] #quality_metric: host=algo-1, epoch=161, batch=0 train loss <loss>=3.7047507762908936\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:44 INFO 139919854057280] Epoch[161] Batch[5] avg_epoch_loss=3.904516\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:44 INFO 139919854057280] #quality_metric: host=algo-1, epoch=161, batch=5 train loss <loss>=3.904516339302063\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:44 INFO 139919854057280] Epoch[161] Batch [5]#011Speed: 2813.59 samples/sec#011loss=3.904516\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:44 INFO 139919854057280] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158884.2820673, \"EndTime\": 1681158884.6435595, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 360.9471321105957, \"count\": 1, \"min\": 360.9471321105957, \"max\": 360.9471321105957}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:44 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1756.0039224983013 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:44 INFO 139919854057280] #progress_metric: host=algo-1, completed 40.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:44 INFO 139919854057280] #quality_metric: host=algo-1, epoch=161, train loss <loss>=3.9532686710357665\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:44 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:44 INFO 139919854057280] Epoch[162] Batch[0] avg_epoch_loss=4.406618\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:44 INFO 139919854057280] #quality_metric: host=algo-1, epoch=162, batch=0 train loss <loss>=4.406617641448975\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:44 INFO 139919854057280] Epoch[162] Batch[5] avg_epoch_loss=3.992203\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:44 INFO 139919854057280] #quality_metric: host=algo-1, epoch=162, batch=5 train loss <loss>=3.9922033548355103\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:44 INFO 139919854057280] Epoch[162] Batch [5]#011Speed: 3038.08 samples/sec#011loss=3.992203\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:45 INFO 139919854057280] Epoch[162] Batch[10] avg_epoch_loss=4.020561\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:45 INFO 139919854057280] #quality_metric: host=algo-1, epoch=162, batch=10 train loss <loss>=4.054589176177979\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:45 INFO 139919854057280] Epoch[162] Batch [10]#011Speed: 2692.90 samples/sec#011loss=4.054589\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:45 INFO 139919854057280] processed a total of 688 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158884.6436298, \"EndTime\": 1681158885.0239778, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 380.0044059753418, \"count\": 1, \"min\": 380.0044059753418, \"max\": 380.0044059753418}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:45 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1810.0658256813117 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:45 INFO 139919854057280] #progress_metric: host=algo-1, completed 40.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:45 INFO 139919854057280] #quality_metric: host=algo-1, epoch=162, train loss <loss>=4.020560546354814\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:45 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:45 INFO 139919854057280] Epoch[163] Batch[0] avg_epoch_loss=3.875805\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:45 INFO 139919854057280] #quality_metric: host=algo-1, epoch=163, batch=0 train loss <loss>=3.875805377960205\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:45 INFO 139919854057280] Epoch[163] Batch[5] avg_epoch_loss=3.947142\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:45 INFO 139919854057280] #quality_metric: host=algo-1, epoch=163, batch=5 train loss <loss>=3.9471415281295776\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:45 INFO 139919854057280] Epoch[163] Batch [5]#011Speed: 3043.71 samples/sec#011loss=3.947142\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:45 INFO 139919854057280] Epoch[163] Batch[10] avg_epoch_loss=3.844347\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:45 INFO 139919854057280] #quality_metric: host=algo-1, epoch=163, batch=10 train loss <loss>=3.720992660522461\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:45 INFO 139919854057280] Epoch[163] Batch [10]#011Speed: 2931.34 samples/sec#011loss=3.720993\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:45 INFO 139919854057280] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158885.0240412, \"EndTime\": 1681158885.3689039, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 344.3174362182617, \"count\": 1, \"min\": 344.3174362182617, \"max\": 344.3174362182617}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:45 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1866.8629827859752 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:45 INFO 139919854057280] #progress_metric: host=algo-1, completed 41.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:45 INFO 139919854057280] #quality_metric: host=algo-1, epoch=163, train loss <loss>=3.844346588308161\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:45 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:45 INFO 139919854057280] Epoch[164] Batch[0] avg_epoch_loss=3.978833\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:45 INFO 139919854057280] #quality_metric: host=algo-1, epoch=164, batch=0 train loss <loss>=3.978832721710205\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:45 INFO 139919854057280] Epoch[164] Batch[5] avg_epoch_loss=3.985392\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:45 INFO 139919854057280] #quality_metric: host=algo-1, epoch=164, batch=5 train loss <loss>=3.985391537348429\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:45 INFO 139919854057280] Epoch[164] Batch [5]#011Speed: 3057.25 samples/sec#011loss=3.985392\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:45 INFO 139919854057280] Epoch[164] Batch[10] avg_epoch_loss=4.077404\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:45 INFO 139919854057280] #quality_metric: host=algo-1, epoch=164, batch=10 train loss <loss>=4.1878186702728275\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:45 INFO 139919854057280] Epoch[164] Batch [10]#011Speed: 2970.98 samples/sec#011loss=4.187819\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:45 INFO 139919854057280] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158885.368985, \"EndTime\": 1681158885.7373052, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 367.9189682006836, \"count\": 1, \"min\": 367.9189682006836, \"max\": 367.9189682006836}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:45 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1755.1935615100188 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:45 INFO 139919854057280] #progress_metric: host=algo-1, completed 41.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:45 INFO 139919854057280] #quality_metric: host=algo-1, epoch=164, train loss <loss>=4.077403870495883\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:45 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:45 INFO 139919854057280] Epoch[165] Batch[0] avg_epoch_loss=4.718084\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:45 INFO 139919854057280] #quality_metric: host=algo-1, epoch=165, batch=0 train loss <loss>=4.718084335327148\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:46 INFO 139919854057280] Epoch[165] Batch[5] avg_epoch_loss=4.303939\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:46 INFO 139919854057280] #quality_metric: host=algo-1, epoch=165, batch=5 train loss <loss>=4.303939382235209\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:46 INFO 139919854057280] Epoch[165] Batch [5]#011Speed: 2844.49 samples/sec#011loss=4.303939\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:46 INFO 139919854057280] Epoch[165] Batch[10] avg_epoch_loss=4.364824\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:46 INFO 139919854057280] #quality_metric: host=algo-1, epoch=165, batch=10 train loss <loss>=4.437886524200439\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:46 INFO 139919854057280] Epoch[165] Batch [10]#011Speed: 2822.13 samples/sec#011loss=4.437887\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:46 INFO 139919854057280] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158885.7374074, \"EndTime\": 1681158886.122357, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 384.56177711486816, \"count\": 1, \"min\": 384.56177711486816, \"max\": 384.56177711486816}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:46 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1726.220064684801 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:46 INFO 139919854057280] #progress_metric: host=algo-1, completed 41.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:46 INFO 139919854057280] #quality_metric: host=algo-1, epoch=165, train loss <loss>=4.364824446764859\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:46 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:46 INFO 139919854057280] Epoch[166] Batch[0] avg_epoch_loss=4.752190\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:46 INFO 139919854057280] #quality_metric: host=algo-1, epoch=166, batch=0 train loss <loss>=4.752189636230469\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:46 INFO 139919854057280] Epoch[166] Batch[5] avg_epoch_loss=4.461397\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:46 INFO 139919854057280] #quality_metric: host=algo-1, epoch=166, batch=5 train loss <loss>=4.46139669418335\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:46 INFO 139919854057280] Epoch[166] Batch [5]#011Speed: 3000.56 samples/sec#011loss=4.461397\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:46 INFO 139919854057280] Epoch[166] Batch[10] avg_epoch_loss=4.407039\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:46 INFO 139919854057280] #quality_metric: host=algo-1, epoch=166, batch=10 train loss <loss>=4.3418103694915775\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:46 INFO 139919854057280] Epoch[166] Batch [10]#011Speed: 2777.05 samples/sec#011loss=4.341810\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:46 INFO 139919854057280] processed a total of 673 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158886.1224227, \"EndTime\": 1681158886.501669, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 378.94511222839355, \"count\": 1, \"min\": 378.94511222839355, \"max\": 378.94511222839355}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:46 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1775.5594084753332 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:46 INFO 139919854057280] #progress_metric: host=algo-1, completed 41.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:46 INFO 139919854057280] #quality_metric: host=algo-1, epoch=166, train loss <loss>=4.407039273868907\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:46 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:46 INFO 139919854057280] Epoch[167] Batch[0] avg_epoch_loss=4.511257\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:46 INFO 139919854057280] #quality_metric: host=algo-1, epoch=167, batch=0 train loss <loss>=4.511256694793701\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:46 INFO 139919854057280] Epoch[167] Batch[5] avg_epoch_loss=4.252842\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:46 INFO 139919854057280] #quality_metric: host=algo-1, epoch=167, batch=5 train loss <loss>=4.252842346827189\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:46 INFO 139919854057280] Epoch[167] Batch [5]#011Speed: 2840.76 samples/sec#011loss=4.252842\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:46 INFO 139919854057280] processed a total of 606 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158886.5017328, \"EndTime\": 1681158886.871822, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 369.74191665649414, \"count\": 1, \"min\": 369.74191665649414, \"max\": 369.74191665649414}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:46 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1638.529911290151 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:46 INFO 139919854057280] #progress_metric: host=algo-1, completed 42.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:46 INFO 139919854057280] #quality_metric: host=algo-1, epoch=167, train loss <loss>=4.25835087299347\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:46 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:47 INFO 139919854057280] Epoch[168] Batch[0] avg_epoch_loss=3.875728\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:47 INFO 139919854057280] #quality_metric: host=algo-1, epoch=168, batch=0 train loss <loss>=3.875727653503418\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:47 INFO 139919854057280] Epoch[168] Batch[5] avg_epoch_loss=4.049607\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:47 INFO 139919854057280] #quality_metric: host=algo-1, epoch=168, batch=5 train loss <loss>=4.049606919288635\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:47 INFO 139919854057280] Epoch[168] Batch [5]#011Speed: 2374.26 samples/sec#011loss=4.049607\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:47 INFO 139919854057280] Epoch[168] Batch[10] avg_epoch_loss=4.052430\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:47 INFO 139919854057280] #quality_metric: host=algo-1, epoch=168, batch=10 train loss <loss>=4.05581693649292\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:47 INFO 139919854057280] Epoch[168] Batch [10]#011Speed: 2483.80 samples/sec#011loss=4.055817\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:47 INFO 139919854057280] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158886.8718941, \"EndTime\": 1681158887.2802613, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 407.8538417816162, \"count\": 1, \"min\": 407.8538417816162, \"max\": 407.8538417816162}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:47 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1634.962549456201 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:47 INFO 139919854057280] #progress_metric: host=algo-1, completed 42.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:47 INFO 139919854057280] #quality_metric: host=algo-1, epoch=168, train loss <loss>=4.052429654381492\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:47 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:47 INFO 139919854057280] Epoch[169] Batch[0] avg_epoch_loss=4.337096\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:47 INFO 139919854057280] #quality_metric: host=algo-1, epoch=169, batch=0 train loss <loss>=4.337095737457275\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:47 INFO 139919854057280] Epoch[169] Batch[5] avg_epoch_loss=4.012248\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:47 INFO 139919854057280] #quality_metric: host=algo-1, epoch=169, batch=5 train loss <loss>=4.012247800827026\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:47 INFO 139919854057280] Epoch[169] Batch [5]#011Speed: 2974.42 samples/sec#011loss=4.012248\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:47 INFO 139919854057280] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158887.2803364, \"EndTime\": 1681158887.6061091, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 325.32310485839844, \"count\": 1, \"min\": 325.32310485839844, \"max\": 325.32310485839844}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:47 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1914.3780606185817 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:47 INFO 139919854057280] #progress_metric: host=algo-1, completed 42.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:47 INFO 139919854057280] #quality_metric: host=algo-1, epoch=169, train loss <loss>=3.9356968879699705\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:47 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:47 INFO 139919854057280] Epoch[170] Batch[0] avg_epoch_loss=3.935491\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:47 INFO 139919854057280] #quality_metric: host=algo-1, epoch=170, batch=0 train loss <loss>=3.935490846633911\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:47 INFO 139919854057280] Epoch[170] Batch[5] avg_epoch_loss=3.929566\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:47 INFO 139919854057280] #quality_metric: host=algo-1, epoch=170, batch=5 train loss <loss>=3.9295657873153687\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:47 INFO 139919854057280] Epoch[170] Batch [5]#011Speed: 3003.15 samples/sec#011loss=3.929566\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:47 INFO 139919854057280] Epoch[170] Batch[10] avg_epoch_loss=3.813871\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:47 INFO 139919854057280] #quality_metric: host=algo-1, epoch=170, batch=10 train loss <loss>=3.6750370025634767\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:47 INFO 139919854057280] Epoch[170] Batch [10]#011Speed: 2920.72 samples/sec#011loss=3.675037\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:47 INFO 139919854057280] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158887.6061842, \"EndTime\": 1681158887.956204, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 349.5943546295166, \"count\": 1, \"min\": 349.5943546295166, \"max\": 349.5943546295166}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:47 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1873.049492205823 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:47 INFO 139919854057280] #progress_metric: host=algo-1, completed 42.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:47 INFO 139919854057280] #quality_metric: host=algo-1, epoch=170, train loss <loss>=3.8138708851554175\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:47 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:48 INFO 139919854057280] Epoch[171] Batch[0] avg_epoch_loss=4.142844\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:48 INFO 139919854057280] #quality_metric: host=algo-1, epoch=171, batch=0 train loss <loss>=4.142844200134277\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:48 INFO 139919854057280] Epoch[171] Batch[5] avg_epoch_loss=4.153217\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:48 INFO 139919854057280] #quality_metric: host=algo-1, epoch=171, batch=5 train loss <loss>=4.153216520945231\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:48 INFO 139919854057280] Epoch[171] Batch [5]#011Speed: 2779.93 samples/sec#011loss=4.153217\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:48 INFO 139919854057280] Epoch[171] Batch[10] avg_epoch_loss=4.249668\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:48 INFO 139919854057280] #quality_metric: host=algo-1, epoch=171, batch=10 train loss <loss>=4.365409660339355\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:48 INFO 139919854057280] Epoch[171] Batch [10]#011Speed: 2592.21 samples/sec#011loss=4.365410\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:48 INFO 139919854057280] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158887.9562747, \"EndTime\": 1681158888.331271, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 374.6144771575928, \"count\": 1, \"min\": 374.6144771575928, \"max\": 374.6144771575928}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:48 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1769.2992481432632 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:48 INFO 139919854057280] #progress_metric: host=algo-1, completed 43.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:48 INFO 139919854057280] #quality_metric: host=algo-1, epoch=171, train loss <loss>=4.2496679479425605\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:48 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:48 INFO 139919854057280] Epoch[172] Batch[0] avg_epoch_loss=3.986487\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:48 INFO 139919854057280] #quality_metric: host=algo-1, epoch=172, batch=0 train loss <loss>=3.9864869117736816\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:48 INFO 139919854057280] Epoch[172] Batch[5] avg_epoch_loss=4.075756\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:48 INFO 139919854057280] #quality_metric: host=algo-1, epoch=172, batch=5 train loss <loss>=4.075756351153056\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:48 INFO 139919854057280] Epoch[172] Batch [5]#011Speed: 2685.40 samples/sec#011loss=4.075756\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:48 INFO 139919854057280] Epoch[172] Batch[10] avg_epoch_loss=4.030941\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:48 INFO 139919854057280] #quality_metric: host=algo-1, epoch=172, batch=10 train loss <loss>=3.9771618366241457\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:48 INFO 139919854057280] Epoch[172] Batch [10]#011Speed: 2647.90 samples/sec#011loss=3.977162\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:48 INFO 139919854057280] processed a total of 688 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158888.3313475, \"EndTime\": 1681158888.6963181, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 364.5739555358887, \"count\": 1, \"min\": 364.5739555358887, \"max\": 364.5739555358887}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:48 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1886.6173006100476 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:48 INFO 139919854057280] #progress_metric: host=algo-1, completed 43.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:48 INFO 139919854057280] #quality_metric: host=algo-1, epoch=172, train loss <loss>=4.030940662730824\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:48 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:48 INFO 139919854057280] Epoch[173] Batch[0] avg_epoch_loss=4.109603\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:48 INFO 139919854057280] #quality_metric: host=algo-1, epoch=173, batch=0 train loss <loss>=4.109602928161621\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:48 INFO 139919854057280] Epoch[173] Batch[5] avg_epoch_loss=4.046999\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:48 INFO 139919854057280] #quality_metric: host=algo-1, epoch=173, batch=5 train loss <loss>=4.046998659769694\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:48 INFO 139919854057280] Epoch[173] Batch [5]#011Speed: 2825.74 samples/sec#011loss=4.046999\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:49 INFO 139919854057280] Epoch[173] Batch[10] avg_epoch_loss=3.986038\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:49 INFO 139919854057280] #quality_metric: host=algo-1, epoch=173, batch=10 train loss <loss>=3.9128854751586912\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:49 INFO 139919854057280] Epoch[173] Batch [10]#011Speed: 2668.83 samples/sec#011loss=3.912885\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:49 INFO 139919854057280] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158888.6963882, \"EndTime\": 1681158889.0884259, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 391.53170585632324, \"count\": 1, \"min\": 391.53170585632324, \"max\": 391.53170585632324}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:49 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1715.8387536457215 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:49 INFO 139919854057280] #progress_metric: host=algo-1, completed 43.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:49 INFO 139919854057280] #quality_metric: host=algo-1, epoch=173, train loss <loss>=3.9860381213101475\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:49 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:49 INFO 139919854057280] Epoch[174] Batch[0] avg_epoch_loss=3.960971\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:49 INFO 139919854057280] #quality_metric: host=algo-1, epoch=174, batch=0 train loss <loss>=3.9609713554382324\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:49 INFO 139919854057280] Epoch[174] Batch[5] avg_epoch_loss=3.931895\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:49 INFO 139919854057280] #quality_metric: host=algo-1, epoch=174, batch=5 train loss <loss>=3.9318946997324624\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:49 INFO 139919854057280] Epoch[174] Batch [5]#011Speed: 2944.84 samples/sec#011loss=3.931895\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:49 INFO 139919854057280] Epoch[174] Batch[10] avg_epoch_loss=4.045040\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:49 INFO 139919854057280] #quality_metric: host=algo-1, epoch=174, batch=10 train loss <loss>=4.180815410614014\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:49 INFO 139919854057280] Epoch[174] Batch [10]#011Speed: 2897.88 samples/sec#011loss=4.180815\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:49 INFO 139919854057280] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158889.0885062, \"EndTime\": 1681158889.4367378, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 347.80263900756836, \"count\": 1, \"min\": 347.80263900756836, \"max\": 347.80263900756836}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:49 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1851.0880700761302 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:49 INFO 139919854057280] #progress_metric: host=algo-1, completed 43.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:49 INFO 139919854057280] #quality_metric: host=algo-1, epoch=174, train loss <loss>=4.045040477405895\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:49 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:49 INFO 139919854057280] Epoch[175] Batch[0] avg_epoch_loss=4.508956\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:49 INFO 139919854057280] #quality_metric: host=algo-1, epoch=175, batch=0 train loss <loss>=4.508955955505371\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:49 INFO 139919854057280] Epoch[175] Batch[5] avg_epoch_loss=4.227665\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:49 INFO 139919854057280] #quality_metric: host=algo-1, epoch=175, batch=5 train loss <loss>=4.2276649077733355\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:49 INFO 139919854057280] Epoch[175] Batch [5]#011Speed: 2930.07 samples/sec#011loss=4.227665\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:49 INFO 139919854057280] Epoch[175] Batch[10] avg_epoch_loss=4.273565\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:49 INFO 139919854057280] #quality_metric: host=algo-1, epoch=175, batch=10 train loss <loss>=4.328645849227906\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:49 INFO 139919854057280] Epoch[175] Batch [10]#011Speed: 2947.00 samples/sec#011loss=4.328646\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:49 INFO 139919854057280] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158889.4368067, \"EndTime\": 1681158889.8153908, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 378.24249267578125, \"count\": 1, \"min\": 378.24249267578125, \"max\": 378.24249267578125}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:49 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1707.4760888977394 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:49 INFO 139919854057280] #progress_metric: host=algo-1, completed 44.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:49 INFO 139919854057280] #quality_metric: host=algo-1, epoch=175, train loss <loss>=4.273565335707231\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:49 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:49 INFO 139919854057280] Epoch[176] Batch[0] avg_epoch_loss=4.150496\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:49 INFO 139919854057280] #quality_metric: host=algo-1, epoch=176, batch=0 train loss <loss>=4.150495529174805\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:50 INFO 139919854057280] Epoch[176] Batch[5] avg_epoch_loss=4.226415\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:50 INFO 139919854057280] #quality_metric: host=algo-1, epoch=176, batch=5 train loss <loss>=4.226414918899536\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:50 INFO 139919854057280] Epoch[176] Batch [5]#011Speed: 2766.06 samples/sec#011loss=4.226415\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:50 INFO 139919854057280] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158889.8154576, \"EndTime\": 1681158890.1798182, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 364.0246391296387, \"count\": 1, \"min\": 364.0246391296387, \"max\": 364.0246391296387}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:50 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1680.7381404795797 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:50 INFO 139919854057280] #progress_metric: host=algo-1, completed 44.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:50 INFO 139919854057280] #quality_metric: host=algo-1, epoch=176, train loss <loss>=4.074852323532104\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:50 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:50 INFO 139919854057280] Epoch[177] Batch[0] avg_epoch_loss=3.873255\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:50 INFO 139919854057280] #quality_metric: host=algo-1, epoch=177, batch=0 train loss <loss>=3.8732547760009766\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:50 INFO 139919854057280] Epoch[177] Batch[5] avg_epoch_loss=4.018118\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:50 INFO 139919854057280] #quality_metric: host=algo-1, epoch=177, batch=5 train loss <loss>=4.018118381500244\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:50 INFO 139919854057280] Epoch[177] Batch [5]#011Speed: 2956.16 samples/sec#011loss=4.018118\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:50 INFO 139919854057280] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158890.1798902, \"EndTime\": 1681158890.5023673, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 321.9146728515625, \"count\": 1, \"min\": 321.9146728515625, \"max\": 321.9146728515625}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:50 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1955.477644645438 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:50 INFO 139919854057280] #progress_metric: host=algo-1, completed 44.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:50 INFO 139919854057280] #quality_metric: host=algo-1, epoch=177, train loss <loss>=4.024044489860534\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:50 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:50 INFO 139919854057280] Epoch[178] Batch[0] avg_epoch_loss=3.934045\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:50 INFO 139919854057280] #quality_metric: host=algo-1, epoch=178, batch=0 train loss <loss>=3.9340453147888184\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:50 INFO 139919854057280] Epoch[178] Batch[5] avg_epoch_loss=3.909121\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:50 INFO 139919854057280] #quality_metric: host=algo-1, epoch=178, batch=5 train loss <loss>=3.9091206789016724\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:50 INFO 139919854057280] Epoch[178] Batch [5]#011Speed: 3024.27 samples/sec#011loss=3.909121\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:50 INFO 139919854057280] Epoch[178] Batch[10] avg_epoch_loss=3.972963\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:50 INFO 139919854057280] #quality_metric: host=algo-1, epoch=178, batch=10 train loss <loss>=4.04957389831543\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:50 INFO 139919854057280] Epoch[178] Batch [10]#011Speed: 2829.94 samples/sec#011loss=4.049574\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:50 INFO 139919854057280] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158890.5025918, \"EndTime\": 1681158890.8811831, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 378.1437873840332, \"count\": 1, \"min\": 378.1437873840332, \"max\": 378.1437873840332}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:50 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1742.27733766332 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:50 INFO 139919854057280] #progress_metric: host=algo-1, completed 44.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:50 INFO 139919854057280] #quality_metric: host=algo-1, epoch=178, train loss <loss>=3.972963051362471\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:50 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:51 INFO 139919854057280] Epoch[179] Batch[0] avg_epoch_loss=3.814553\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:51 INFO 139919854057280] #quality_metric: host=algo-1, epoch=179, batch=0 train loss <loss>=3.8145525455474854\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:51 INFO 139919854057280] Epoch[179] Batch[5] avg_epoch_loss=3.926377\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:51 INFO 139919854057280] #quality_metric: host=algo-1, epoch=179, batch=5 train loss <loss>=3.9263768990834556\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:51 INFO 139919854057280] Epoch[179] Batch [5]#011Speed: 3008.69 samples/sec#011loss=3.926377\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:51 INFO 139919854057280] Epoch[179] Batch[10] avg_epoch_loss=3.991085\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:51 INFO 139919854057280] #quality_metric: host=algo-1, epoch=179, batch=10 train loss <loss>=4.0687336921691895\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:51 INFO 139919854057280] Epoch[179] Batch [10]#011Speed: 2360.49 samples/sec#011loss=4.068734\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:51 INFO 139919854057280] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158890.8812525, \"EndTime\": 1681158891.2499883, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 368.3807849884033, \"count\": 1, \"min\": 368.3807849884033, \"max\": 368.3807849884033}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:51 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1783.0155910304989 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:51 INFO 139919854057280] #progress_metric: host=algo-1, completed 45.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:51 INFO 139919854057280] #quality_metric: host=algo-1, epoch=179, train loss <loss>=3.9910845323042436\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:51 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:51 INFO 139919854057280] Epoch[180] Batch[0] avg_epoch_loss=4.394410\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:51 INFO 139919854057280] #quality_metric: host=algo-1, epoch=180, batch=0 train loss <loss>=4.394409656524658\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:51 INFO 139919854057280] Epoch[180] Batch[5] avg_epoch_loss=4.240469\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:51 INFO 139919854057280] #quality_metric: host=algo-1, epoch=180, batch=5 train loss <loss>=4.240468660990397\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:51 INFO 139919854057280] Epoch[180] Batch [5]#011Speed: 2685.80 samples/sec#011loss=4.240469\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:51 INFO 139919854057280] Epoch[180] Batch[10] avg_epoch_loss=4.287674\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:51 INFO 139919854057280] #quality_metric: host=algo-1, epoch=180, batch=10 train loss <loss>=4.344320487976074\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:51 INFO 139919854057280] Epoch[180] Batch [10]#011Speed: 2721.94 samples/sec#011loss=4.344320\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:51 INFO 139919854057280] processed a total of 699 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158891.2500544, \"EndTime\": 1681158891.6211448, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 370.6965446472168, \"count\": 1, \"min\": 370.6965446472168, \"max\": 370.6965446472168}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:51 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1884.950768364299 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:51 INFO 139919854057280] #progress_metric: host=algo-1, completed 45.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:51 INFO 139919854057280] #quality_metric: host=algo-1, epoch=180, train loss <loss>=4.2876740368929775\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:51 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:51 INFO 139919854057280] Epoch[181] Batch[0] avg_epoch_loss=4.660143\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:51 INFO 139919854057280] #quality_metric: host=algo-1, epoch=181, batch=0 train loss <loss>=4.66014289855957\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:51 INFO 139919854057280] Epoch[181] Batch[5] avg_epoch_loss=4.178117\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:51 INFO 139919854057280] #quality_metric: host=algo-1, epoch=181, batch=5 train loss <loss>=4.17811659971873\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:51 INFO 139919854057280] Epoch[181] Batch [5]#011Speed: 2481.01 samples/sec#011loss=4.178117\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:51 INFO 139919854057280] Epoch[181] Batch[10] avg_epoch_loss=4.007938\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:51 INFO 139919854057280] #quality_metric: host=algo-1, epoch=181, batch=10 train loss <loss>=3.8037247180938722\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:51 INFO 139919854057280] Epoch[181] Batch [10]#011Speed: 2902.86 samples/sec#011loss=3.803725\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:51 INFO 139919854057280] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158891.6212006, \"EndTime\": 1681158891.993689, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 372.0395565032959, \"count\": 1, \"min\": 372.0395565032959, \"max\": 372.0395565032959}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:51 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1760.0906162468575 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:51 INFO 139919854057280] #progress_metric: host=algo-1, completed 45.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:51 INFO 139919854057280] #quality_metric: host=algo-1, epoch=181, train loss <loss>=4.007938471707431\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:51 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:52 INFO 139919854057280] Epoch[182] Batch[0] avg_epoch_loss=4.121133\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:52 INFO 139919854057280] #quality_metric: host=algo-1, epoch=182, batch=0 train loss <loss>=4.121133327484131\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:52 INFO 139919854057280] Epoch[182] Batch[5] avg_epoch_loss=3.960165\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:52 INFO 139919854057280] #quality_metric: host=algo-1, epoch=182, batch=5 train loss <loss>=3.960165103276571\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:52 INFO 139919854057280] Epoch[182] Batch [5]#011Speed: 2931.10 samples/sec#011loss=3.960165\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:52 INFO 139919854057280] Epoch[182] Batch[10] avg_epoch_loss=3.673630\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:52 INFO 139919854057280] #quality_metric: host=algo-1, epoch=182, batch=10 train loss <loss>=3.3297881126403808\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:52 INFO 139919854057280] Epoch[182] Batch [10]#011Speed: 2474.60 samples/sec#011loss=3.329788\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:52 INFO 139919854057280] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158891.9937577, \"EndTime\": 1681158892.3861992, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 392.0741081237793, \"count\": 1, \"min\": 392.0741081237793, \"max\": 392.0741081237793}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:52 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1639.5972661802273 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:52 INFO 139919854057280] #progress_metric: host=algo-1, completed 45.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:52 INFO 139919854057280] #quality_metric: host=algo-1, epoch=182, train loss <loss>=3.673630107532848\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:52 INFO 139919854057280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:52 INFO 139919854057280] Saved checkpoint to \"/opt/ml/model/state_7982d10c-69ef-4cb6-bb76-ca61ad63d105-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158892.386265, \"EndTime\": 1681158892.3962069, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 9.553194046020508, \"count\": 1, \"min\": 9.553194046020508, \"max\": 9.553194046020508}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:52 INFO 139919854057280] Epoch[183] Batch[0] avg_epoch_loss=3.981736\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:52 INFO 139919854057280] #quality_metric: host=algo-1, epoch=183, batch=0 train loss <loss>=3.9817357063293457\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:52 INFO 139919854057280] Epoch[183] Batch[5] avg_epoch_loss=3.855886\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:52 INFO 139919854057280] #quality_metric: host=algo-1, epoch=183, batch=5 train loss <loss>=3.855885863304138\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:52 INFO 139919854057280] Epoch[183] Batch [5]#011Speed: 2822.22 samples/sec#011loss=3.855886\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:52 INFO 139919854057280] Epoch[183] Batch[10] avg_epoch_loss=3.699275\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:52 INFO 139919854057280] #quality_metric: host=algo-1, epoch=183, batch=10 train loss <loss>=3.511340928077698\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:52 INFO 139919854057280] Epoch[183] Batch [10]#011Speed: 2943.42 samples/sec#011loss=3.511341\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:52 INFO 139919854057280] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158892.3963609, \"EndTime\": 1681158892.7472522, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 350.82030296325684, \"count\": 1, \"min\": 350.82030296325684, \"max\": 350.82030296325684}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:52 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1855.1226157815997 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:52 INFO 139919854057280] #progress_metric: host=algo-1, completed 46.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:52 INFO 139919854057280] #quality_metric: host=algo-1, epoch=183, train loss <loss>=3.6992745291103017\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:52 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:52 INFO 139919854057280] Epoch[184] Batch[0] avg_epoch_loss=4.071332\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:52 INFO 139919854057280] #quality_metric: host=algo-1, epoch=184, batch=0 train loss <loss>=4.0713324546813965\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:52 INFO 139919854057280] Epoch[184] Batch[5] avg_epoch_loss=3.943942\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:52 INFO 139919854057280] #quality_metric: host=algo-1, epoch=184, batch=5 train loss <loss>=3.943942348162333\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:52 INFO 139919854057280] Epoch[184] Batch [5]#011Speed: 3019.67 samples/sec#011loss=3.943942\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:53 INFO 139919854057280] Epoch[184] Batch[10] avg_epoch_loss=3.725467\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:53 INFO 139919854057280] #quality_metric: host=algo-1, epoch=184, batch=10 train loss <loss>=3.4632959842681883\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:53 INFO 139919854057280] Epoch[184] Batch [10]#011Speed: 2402.24 samples/sec#011loss=3.463296\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:53 INFO 139919854057280] processed a total of 670 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158892.7473216, \"EndTime\": 1681158893.128517, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 380.68103790283203, \"count\": 1, \"min\": 380.68103790283203, \"max\": 380.68103790283203}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:53 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1759.570517536637 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:53 INFO 139919854057280] #progress_metric: host=algo-1, completed 46.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:53 INFO 139919854057280] #quality_metric: host=algo-1, epoch=184, train loss <loss>=3.725466728210449\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:53 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:53 INFO 139919854057280] Epoch[185] Batch[0] avg_epoch_loss=3.895427\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:53 INFO 139919854057280] #quality_metric: host=algo-1, epoch=185, batch=0 train loss <loss>=3.8954265117645264\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:53 INFO 139919854057280] Epoch[185] Batch[5] avg_epoch_loss=3.838504\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:53 INFO 139919854057280] #quality_metric: host=algo-1, epoch=185, batch=5 train loss <loss>=3.8385035594304404\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:53 INFO 139919854057280] Epoch[185] Batch [5]#011Speed: 2990.96 samples/sec#011loss=3.838504\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:53 INFO 139919854057280] Epoch[185] Batch[10] avg_epoch_loss=3.827802\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:53 INFO 139919854057280] #quality_metric: host=algo-1, epoch=185, batch=10 train loss <loss>=3.814960241317749\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:53 INFO 139919854057280] Epoch[185] Batch [10]#011Speed: 2758.30 samples/sec#011loss=3.814960\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:53 INFO 139919854057280] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158893.1285825, \"EndTime\": 1681158893.486836, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 357.8460216522217, \"count\": 1, \"min\": 357.8460216522217, \"max\": 357.8460216522217}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:53 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1854.9806450495646 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:53 INFO 139919854057280] #progress_metric: host=algo-1, completed 46.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:53 INFO 139919854057280] #quality_metric: host=algo-1, epoch=185, train loss <loss>=3.827802051197399\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:53 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:53 INFO 139919854057280] Epoch[186] Batch[0] avg_epoch_loss=3.947553\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:53 INFO 139919854057280] #quality_metric: host=algo-1, epoch=186, batch=0 train loss <loss>=3.9475533962249756\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:53 INFO 139919854057280] Epoch[186] Batch[5] avg_epoch_loss=3.936226\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:53 INFO 139919854057280] #quality_metric: host=algo-1, epoch=186, batch=5 train loss <loss>=3.9362263679504395\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:53 INFO 139919854057280] Epoch[186] Batch [5]#011Speed: 2991.10 samples/sec#011loss=3.936226\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:53 INFO 139919854057280] Epoch[186] Batch[10] avg_epoch_loss=3.893162\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:53 INFO 139919854057280] #quality_metric: host=algo-1, epoch=186, batch=10 train loss <loss>=3.8414836883544923\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:53 INFO 139919854057280] Epoch[186] Batch [10]#011Speed: 2684.38 samples/sec#011loss=3.841484\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:53 INFO 139919854057280] processed a total of 681 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158893.4869132, \"EndTime\": 1681158893.8694701, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 382.1103572845459, \"count\": 1, \"min\": 382.1103572845459, \"max\": 382.1103572845459}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:53 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1781.7573938364112 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:53 INFO 139919854057280] #progress_metric: host=algo-1, completed 46.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:53 INFO 139919854057280] #quality_metric: host=algo-1, epoch=186, train loss <loss>=3.893161513588645\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:53 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:54 INFO 139919854057280] Epoch[187] Batch[0] avg_epoch_loss=3.747020\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:54 INFO 139919854057280] #quality_metric: host=algo-1, epoch=187, batch=0 train loss <loss>=3.7470202445983887\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:54 INFO 139919854057280] Epoch[187] Batch[5] avg_epoch_loss=3.981108\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:54 INFO 139919854057280] #quality_metric: host=algo-1, epoch=187, batch=5 train loss <loss>=3.9811084270477295\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:54 INFO 139919854057280] Epoch[187] Batch [5]#011Speed: 3046.10 samples/sec#011loss=3.981108\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:54 INFO 139919854057280] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158893.869539, \"EndTime\": 1681158894.2230668, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 353.1484603881836, \"count\": 1, \"min\": 353.1484603881836, \"max\": 353.1484603881836}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:54 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1811.7809330519278 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:54 INFO 139919854057280] #progress_metric: host=algo-1, completed 47.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:54 INFO 139919854057280] #quality_metric: host=algo-1, epoch=187, train loss <loss>=4.10758056640625\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:54 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:54 INFO 139919854057280] Epoch[188] Batch[0] avg_epoch_loss=4.872830\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:54 INFO 139919854057280] #quality_metric: host=algo-1, epoch=188, batch=0 train loss <loss>=4.872830390930176\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:54 INFO 139919854057280] Epoch[188] Batch[5] avg_epoch_loss=4.259609\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:54 INFO 139919854057280] #quality_metric: host=algo-1, epoch=188, batch=5 train loss <loss>=4.25960906346639\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:54 INFO 139919854057280] Epoch[188] Batch [5]#011Speed: 2636.78 samples/sec#011loss=4.259609\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:54 INFO 139919854057280] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158894.223132, \"EndTime\": 1681158894.5771742, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 353.61480712890625, \"count\": 1, \"min\": 353.61480712890625, \"max\": 353.61480712890625}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:54 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1777.8918122625348 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:54 INFO 139919854057280] #progress_metric: host=algo-1, completed 47.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:54 INFO 139919854057280] #quality_metric: host=algo-1, epoch=188, train loss <loss>=4.158091974258423\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:54 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:54 INFO 139919854057280] Epoch[189] Batch[0] avg_epoch_loss=3.838330\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:54 INFO 139919854057280] #quality_metric: host=algo-1, epoch=189, batch=0 train loss <loss>=3.838329792022705\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:54 INFO 139919854057280] Epoch[189] Batch[5] avg_epoch_loss=3.835159\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:54 INFO 139919854057280] #quality_metric: host=algo-1, epoch=189, batch=5 train loss <loss>=3.8351587454477944\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:54 INFO 139919854057280] Epoch[189] Batch [5]#011Speed: 2546.84 samples/sec#011loss=3.835159\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:54 INFO 139919854057280] Epoch[189] Batch[10] avg_epoch_loss=3.868488\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:54 INFO 139919854057280] #quality_metric: host=algo-1, epoch=189, batch=10 train loss <loss>=3.908483552932739\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:54 INFO 139919854057280] Epoch[189] Batch [10]#011Speed: 2738.51 samples/sec#011loss=3.908484\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:54 INFO 139919854057280] processed a total of 690 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158894.5772989, \"EndTime\": 1681158894.946237, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 368.46423149108887, \"count\": 1, \"min\": 368.46423149108887, \"max\": 368.46423149108887}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:54 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1872.1470563881758 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:54 INFO 139919854057280] #progress_metric: host=algo-1, completed 47.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:54 INFO 139919854057280] #quality_metric: host=algo-1, epoch=189, train loss <loss>=3.8684882033954966\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:54 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:55 INFO 139919854057280] Epoch[190] Batch[0] avg_epoch_loss=3.802589\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:55 INFO 139919854057280] #quality_metric: host=algo-1, epoch=190, batch=0 train loss <loss>=3.802588939666748\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:55 INFO 139919854057280] Epoch[190] Batch[5] avg_epoch_loss=3.956048\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:55 INFO 139919854057280] #quality_metric: host=algo-1, epoch=190, batch=5 train loss <loss>=3.956048011779785\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:55 INFO 139919854057280] Epoch[190] Batch [5]#011Speed: 2990.50 samples/sec#011loss=3.956048\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:55 INFO 139919854057280] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158894.9463043, \"EndTime\": 1681158895.3000498, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 353.4409999847412, \"count\": 1, \"min\": 353.4409999847412, \"max\": 353.4409999847412}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:55 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1807.2325816459534 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:55 INFO 139919854057280] #progress_metric: host=algo-1, completed 47.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:55 INFO 139919854057280] #quality_metric: host=algo-1, epoch=190, train loss <loss>=3.9613953828811646\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:55 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:55 INFO 139919854057280] Epoch[191] Batch[0] avg_epoch_loss=3.689225\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:55 INFO 139919854057280] #quality_metric: host=algo-1, epoch=191, batch=0 train loss <loss>=3.6892247200012207\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:55 INFO 139919854057280] Epoch[191] Batch[5] avg_epoch_loss=4.014322\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:55 INFO 139919854057280] #quality_metric: host=algo-1, epoch=191, batch=5 train loss <loss>=4.01432204246521\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:55 INFO 139919854057280] Epoch[191] Batch [5]#011Speed: 2606.31 samples/sec#011loss=4.014322\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:55 INFO 139919854057280] Epoch[191] Batch[10] avg_epoch_loss=4.064301\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:55 INFO 139919854057280] #quality_metric: host=algo-1, epoch=191, batch=10 train loss <loss>=4.124275732040405\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:55 INFO 139919854057280] Epoch[191] Batch [10]#011Speed: 2954.74 samples/sec#011loss=4.124276\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:55 INFO 139919854057280] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158895.3001585, \"EndTime\": 1681158895.6625996, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 361.9043827056885, \"count\": 1, \"min\": 361.9043827056885, \"max\": 361.9043827056885}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:55 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1809.3827889031882 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:55 INFO 139919854057280] #progress_metric: host=algo-1, completed 48.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:55 INFO 139919854057280] #quality_metric: host=algo-1, epoch=191, train loss <loss>=4.064300992272117\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:55 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:55 INFO 139919854057280] Epoch[192] Batch[0] avg_epoch_loss=3.826227\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:55 INFO 139919854057280] #quality_metric: host=algo-1, epoch=192, batch=0 train loss <loss>=3.8262267112731934\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:55 INFO 139919854057280] Epoch[192] Batch[5] avg_epoch_loss=4.145011\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:55 INFO 139919854057280] #quality_metric: host=algo-1, epoch=192, batch=5 train loss <loss>=4.145011266072591\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:55 INFO 139919854057280] Epoch[192] Batch [5]#011Speed: 2923.47 samples/sec#011loss=4.145011\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:56 INFO 139919854057280] Epoch[192] Batch[10] avg_epoch_loss=4.302413\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:56 INFO 139919854057280] #quality_metric: host=algo-1, epoch=192, batch=10 train loss <loss>=4.491295337677002\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:56 INFO 139919854057280] Epoch[192] Batch [10]#011Speed: 2920.85 samples/sec#011loss=4.491295\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:56 INFO 139919854057280] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158895.6626663, \"EndTime\": 1681158896.0107794, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 347.6531505584717, \"count\": 1, \"min\": 347.6531505584717, \"max\": 347.6531505584717}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:56 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1874.8988453137233 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:56 INFO 139919854057280] #progress_metric: host=algo-1, completed 48.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:56 INFO 139919854057280] #quality_metric: host=algo-1, epoch=192, train loss <loss>=4.302413116801869\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:56 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:56 INFO 139919854057280] Epoch[193] Batch[0] avg_epoch_loss=4.108495\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:56 INFO 139919854057280] #quality_metric: host=algo-1, epoch=193, batch=0 train loss <loss>=4.108494758605957\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:56 INFO 139919854057280] Epoch[193] Batch[5] avg_epoch_loss=4.478021\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:56 INFO 139919854057280] #quality_metric: host=algo-1, epoch=193, batch=5 train loss <loss>=4.478020588556926\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:56 INFO 139919854057280] Epoch[193] Batch [5]#011Speed: 2956.42 samples/sec#011loss=4.478021\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:56 INFO 139919854057280] processed a total of 591 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158896.0108492, \"EndTime\": 1681158896.327095, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 315.8853054046631, \"count\": 1, \"min\": 315.8853054046631, \"max\": 315.8853054046631}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:56 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1870.0543129182727 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:56 INFO 139919854057280] #progress_metric: host=algo-1, completed 48.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:56 INFO 139919854057280] #quality_metric: host=algo-1, epoch=193, train loss <loss>=4.4329692125320435\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:56 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:56 INFO 139919854057280] Epoch[194] Batch[0] avg_epoch_loss=3.957698\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:56 INFO 139919854057280] #quality_metric: host=algo-1, epoch=194, batch=0 train loss <loss>=3.957697868347168\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:56 INFO 139919854057280] Epoch[194] Batch[5] avg_epoch_loss=4.081656\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:56 INFO 139919854057280] #quality_metric: host=algo-1, epoch=194, batch=5 train loss <loss>=4.081655859947205\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:56 INFO 139919854057280] Epoch[194] Batch [5]#011Speed: 3010.15 samples/sec#011loss=4.081656\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:56 INFO 139919854057280] Epoch[194] Batch[10] avg_epoch_loss=4.060449\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:56 INFO 139919854057280] #quality_metric: host=algo-1, epoch=194, batch=10 train loss <loss>=4.035001420974732\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:56 INFO 139919854057280] Epoch[194] Batch [10]#011Speed: 2812.08 samples/sec#011loss=4.035001\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:56 INFO 139919854057280] processed a total of 680 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158896.3272095, \"EndTime\": 1681158896.676904, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 349.2777347564697, \"count\": 1, \"min\": 349.2777347564697, \"max\": 349.2777347564697}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:56 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1946.3778439431917 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:56 INFO 139919854057280] #progress_metric: host=algo-1, completed 48.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:56 INFO 139919854057280] #quality_metric: host=algo-1, epoch=194, train loss <loss>=4.0604492967778985\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:56 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:56 INFO 139919854057280] Epoch[195] Batch[0] avg_epoch_loss=3.922212\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:56 INFO 139919854057280] #quality_metric: host=algo-1, epoch=195, batch=0 train loss <loss>=3.9222116470336914\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:56 INFO 139919854057280] Epoch[195] Batch[5] avg_epoch_loss=4.003010\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:56 INFO 139919854057280] #quality_metric: host=algo-1, epoch=195, batch=5 train loss <loss>=4.003010471661885\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:56 INFO 139919854057280] Epoch[195] Batch [5]#011Speed: 2496.66 samples/sec#011loss=4.003010\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:57 INFO 139919854057280] Epoch[195] Batch[10] avg_epoch_loss=3.974097\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:57 INFO 139919854057280] #quality_metric: host=algo-1, epoch=195, batch=10 train loss <loss>=3.9394001007080077\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:57 INFO 139919854057280] Epoch[195] Batch [10]#011Speed: 2350.53 samples/sec#011loss=3.939400\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:57 INFO 139919854057280] processed a total of 695 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158896.6769633, \"EndTime\": 1681158897.0762887, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 398.91576766967773, \"count\": 1, \"min\": 398.91576766967773, \"max\": 398.91576766967773}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:57 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1741.8008232655884 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:57 INFO 139919854057280] #progress_metric: host=algo-1, completed 49.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:57 INFO 139919854057280] #quality_metric: host=algo-1, epoch=195, train loss <loss>=3.97409666668285\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:57 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:57 INFO 139919854057280] Epoch[196] Batch[0] avg_epoch_loss=4.344056\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:57 INFO 139919854057280] #quality_metric: host=algo-1, epoch=196, batch=0 train loss <loss>=4.344055652618408\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:57 INFO 139919854057280] Epoch[196] Batch[5] avg_epoch_loss=4.137505\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:57 INFO 139919854057280] #quality_metric: host=algo-1, epoch=196, batch=5 train loss <loss>=4.137505253156026\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:57 INFO 139919854057280] Epoch[196] Batch [5]#011Speed: 2654.11 samples/sec#011loss=4.137505\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:57 INFO 139919854057280] Epoch[196] Batch[10] avg_epoch_loss=4.109125\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:57 INFO 139919854057280] #quality_metric: host=algo-1, epoch=196, batch=10 train loss <loss>=4.075069713592529\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:57 INFO 139919854057280] Epoch[196] Batch [10]#011Speed: 2705.51 samples/sec#011loss=4.075070\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:57 INFO 139919854057280] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158897.076357, \"EndTime\": 1681158897.4419458, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 365.217924118042, \"count\": 1, \"min\": 365.217924118042, \"max\": 365.217924118042}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:57 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1817.553905301154 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:57 INFO 139919854057280] #progress_metric: host=algo-1, completed 49.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:57 INFO 139919854057280] #quality_metric: host=algo-1, epoch=196, train loss <loss>=4.109125462445346\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:57 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:57 INFO 139919854057280] Epoch[197] Batch[0] avg_epoch_loss=4.026960\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:57 INFO 139919854057280] #quality_metric: host=algo-1, epoch=197, batch=0 train loss <loss>=4.0269598960876465\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:57 INFO 139919854057280] Epoch[197] Batch[5] avg_epoch_loss=4.027370\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:57 INFO 139919854057280] #quality_metric: host=algo-1, epoch=197, batch=5 train loss <loss>=4.027369658152263\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:57 INFO 139919854057280] Epoch[197] Batch [5]#011Speed: 2942.64 samples/sec#011loss=4.027370\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:57 INFO 139919854057280] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158897.4420214, \"EndTime\": 1681158897.7754915, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 333.0700397491455, \"count\": 1, \"min\": 333.0700397491455, \"max\": 333.0700397491455}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:57 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1884.9442014139106 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:57 INFO 139919854057280] #progress_metric: host=algo-1, completed 49.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:57 INFO 139919854057280] #quality_metric: host=algo-1, epoch=197, train loss <loss>=4.020822191238404\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:57 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:57 INFO 139919854057280] Epoch[198] Batch[0] avg_epoch_loss=3.843295\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:57 INFO 139919854057280] #quality_metric: host=algo-1, epoch=198, batch=0 train loss <loss>=3.843295097351074\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:58 INFO 139919854057280] Epoch[198] Batch[5] avg_epoch_loss=3.874171\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:58 INFO 139919854057280] #quality_metric: host=algo-1, epoch=198, batch=5 train loss <loss>=3.874170740445455\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:58 INFO 139919854057280] Epoch[198] Batch [5]#011Speed: 2733.88 samples/sec#011loss=3.874171\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:58 INFO 139919854057280] Epoch[198] Batch[10] avg_epoch_loss=3.748010\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:58 INFO 139919854057280] #quality_metric: host=algo-1, epoch=198, batch=10 train loss <loss>=3.596616792678833\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:58 INFO 139919854057280] Epoch[198] Batch [10]#011Speed: 2658.51 samples/sec#011loss=3.596617\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:58 INFO 139919854057280] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158897.7755551, \"EndTime\": 1681158898.1383574, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 362.3201847076416, \"count\": 1, \"min\": 362.3201847076416, \"max\": 362.3201847076416}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:58 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1826.5975804382579 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:58 INFO 139919854057280] #progress_metric: host=algo-1, completed 49.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:58 INFO 139919854057280] #quality_metric: host=algo-1, epoch=198, train loss <loss>=3.7480098550969903\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:58 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:58 INFO 139919854057280] Epoch[199] Batch[0] avg_epoch_loss=4.094999\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:58 INFO 139919854057280] #quality_metric: host=algo-1, epoch=199, batch=0 train loss <loss>=4.094998836517334\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:58 INFO 139919854057280] Epoch[199] Batch[5] avg_epoch_loss=3.906862\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:58 INFO 139919854057280] #quality_metric: host=algo-1, epoch=199, batch=5 train loss <loss>=3.9068623781204224\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:58 INFO 139919854057280] Epoch[199] Batch [5]#011Speed: 3037.56 samples/sec#011loss=3.906862\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:58 INFO 139919854057280] Epoch[199] Batch[10] avg_epoch_loss=3.948765\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:58 INFO 139919854057280] #quality_metric: host=algo-1, epoch=199, batch=10 train loss <loss>=3.9990474700927736\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:58 INFO 139919854057280] Epoch[199] Batch [10]#011Speed: 2886.33 samples/sec#011loss=3.999047\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:58 INFO 139919854057280] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158898.1384292, \"EndTime\": 1681158898.5008063, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 361.95993423461914, \"count\": 1, \"min\": 361.95993423461914, \"max\": 361.95993423461914}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:58 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1783.2138833624335 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:58 INFO 139919854057280] #progress_metric: host=algo-1, completed 50.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:58 INFO 139919854057280] #quality_metric: host=algo-1, epoch=199, train loss <loss>=3.948764692653309\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:58 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:58 INFO 139919854057280] Epoch[200] Batch[0] avg_epoch_loss=4.772834\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:58 INFO 139919854057280] #quality_metric: host=algo-1, epoch=200, batch=0 train loss <loss>=4.772834300994873\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:58 INFO 139919854057280] Epoch[200] Batch[5] avg_epoch_loss=4.435782\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:58 INFO 139919854057280] #quality_metric: host=algo-1, epoch=200, batch=5 train loss <loss>=4.435781518618266\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:58 INFO 139919854057280] Epoch[200] Batch [5]#011Speed: 2955.84 samples/sec#011loss=4.435782\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:58 INFO 139919854057280] Epoch[200] Batch[10] avg_epoch_loss=4.623544\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:58 INFO 139919854057280] #quality_metric: host=algo-1, epoch=200, batch=10 train loss <loss>=4.848858070373535\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:58 INFO 139919854057280] Epoch[200] Batch [10]#011Speed: 2821.84 samples/sec#011loss=4.848858\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:58 INFO 139919854057280] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158898.5010796, \"EndTime\": 1681158898.8807523, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 379.3008327484131, \"count\": 1, \"min\": 379.3008327484131, \"max\": 379.3008327484131}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:58 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1731.6347430691446 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:58 INFO 139919854057280] #progress_metric: host=algo-1, completed 50.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:58 INFO 139919854057280] #quality_metric: host=algo-1, epoch=200, train loss <loss>=4.623543587597934\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:58 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:59 INFO 139919854057280] Epoch[201] Batch[0] avg_epoch_loss=4.413523\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:59 INFO 139919854057280] #quality_metric: host=algo-1, epoch=201, batch=0 train loss <loss>=4.413522720336914\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:59 INFO 139919854057280] Epoch[201] Batch[5] avg_epoch_loss=4.210006\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:59 INFO 139919854057280] #quality_metric: host=algo-1, epoch=201, batch=5 train loss <loss>=4.21000607808431\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:59 INFO 139919854057280] Epoch[201] Batch [5]#011Speed: 2956.44 samples/sec#011loss=4.210006\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:59 INFO 139919854057280] Epoch[201] Batch[10] avg_epoch_loss=4.246195\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:59 INFO 139919854057280] #quality_metric: host=algo-1, epoch=201, batch=10 train loss <loss>=4.289620685577392\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:59 INFO 139919854057280] Epoch[201] Batch [10]#011Speed: 2927.45 samples/sec#011loss=4.289621\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:59 INFO 139919854057280] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158898.88083, \"EndTime\": 1681158899.2256143, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 344.3713188171387, \"count\": 1, \"min\": 344.3713188171387, \"max\": 344.3713188171387}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:59 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1860.8514453607606 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:59 INFO 139919854057280] #progress_metric: host=algo-1, completed 50.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:59 INFO 139919854057280] #quality_metric: host=algo-1, epoch=201, train loss <loss>=4.246194536035711\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:59 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:59 INFO 139919854057280] Epoch[202] Batch[0] avg_epoch_loss=3.954352\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:59 INFO 139919854057280] #quality_metric: host=algo-1, epoch=202, batch=0 train loss <loss>=3.9543519020080566\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:59 INFO 139919854057280] Epoch[202] Batch[5] avg_epoch_loss=4.164786\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:59 INFO 139919854057280] #quality_metric: host=algo-1, epoch=202, batch=5 train loss <loss>=4.164786338806152\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:59 INFO 139919854057280] Epoch[202] Batch [5]#011Speed: 2900.29 samples/sec#011loss=4.164786\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:59 INFO 139919854057280] Epoch[202] Batch[10] avg_epoch_loss=4.117166\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:59 INFO 139919854057280] #quality_metric: host=algo-1, epoch=202, batch=10 train loss <loss>=4.060022211074829\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:59 INFO 139919854057280] Epoch[202] Batch [10]#011Speed: 2731.44 samples/sec#011loss=4.060022\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:59 INFO 139919854057280] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158899.2256806, \"EndTime\": 1681158899.613236, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 387.1803283691406, \"count\": 1, \"min\": 387.1803283691406, \"max\": 387.1803283691406}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:59 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1719.7083040651978 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:59 INFO 139919854057280] #progress_metric: host=algo-1, completed 50.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:59 INFO 139919854057280] #quality_metric: host=algo-1, epoch=202, train loss <loss>=4.11716628074646\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:59 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:59 INFO 139919854057280] Epoch[203] Batch[0] avg_epoch_loss=4.063484\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:59 INFO 139919854057280] #quality_metric: host=algo-1, epoch=203, batch=0 train loss <loss>=4.063483715057373\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:59 INFO 139919854057280] Epoch[203] Batch[5] avg_epoch_loss=3.909980\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:59 INFO 139919854057280] #quality_metric: host=algo-1, epoch=203, batch=5 train loss <loss>=3.9099795818328857\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:59 INFO 139919854057280] Epoch[203] Batch [5]#011Speed: 2836.50 samples/sec#011loss=3.909980\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:59 INFO 139919854057280] Epoch[203] Batch[10] avg_epoch_loss=3.826124\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:59 INFO 139919854057280] #quality_metric: host=algo-1, epoch=203, batch=10 train loss <loss>=3.725497341156006\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:59 INFO 139919854057280] Epoch[203] Batch [10]#011Speed: 2622.88 samples/sec#011loss=3.725497\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:59 INFO 139919854057280] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158899.6133022, \"EndTime\": 1681158899.9877753, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 373.7633228302002, \"count\": 1, \"min\": 373.7633228302002, \"max\": 373.7633228302002}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:59 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1717.2055588433361 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:59 INFO 139919854057280] #progress_metric: host=algo-1, completed 51.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:59 INFO 139919854057280] #quality_metric: host=algo-1, epoch=203, train loss <loss>=3.8261240178888496\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:34:59 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:00 INFO 139919854057280] Epoch[204] Batch[0] avg_epoch_loss=3.511895\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:00 INFO 139919854057280] #quality_metric: host=algo-1, epoch=204, batch=0 train loss <loss>=3.511894941329956\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:00 INFO 139919854057280] Epoch[204] Batch[5] avg_epoch_loss=3.793946\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:00 INFO 139919854057280] #quality_metric: host=algo-1, epoch=204, batch=5 train loss <loss>=3.7939459880193076\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:00 INFO 139919854057280] Epoch[204] Batch [5]#011Speed: 3006.65 samples/sec#011loss=3.793946\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:00 INFO 139919854057280] Epoch[204] Batch[10] avg_epoch_loss=3.632772\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:00 INFO 139919854057280] #quality_metric: host=algo-1, epoch=204, batch=10 train loss <loss>=3.4393637418746947\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:00 INFO 139919854057280] Epoch[204] Batch [10]#011Speed: 2505.11 samples/sec#011loss=3.439364\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:00 INFO 139919854057280] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158899.987846, \"EndTime\": 1681158900.3576126, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 369.40956115722656, \"count\": 1, \"min\": 369.40956115722656, \"max\": 369.40956115722656}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:00 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1756.306718747379 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:00 INFO 139919854057280] #progress_metric: host=algo-1, completed 51.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:00 INFO 139919854057280] #quality_metric: host=algo-1, epoch=204, train loss <loss>=3.6327722397717563\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:00 INFO 139919854057280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:00 INFO 139919854057280] Saved checkpoint to \"/opt/ml/model/state_e385cb72-03f3-483e-bcd7-1df6f7bc7f69-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158900.3576934, \"EndTime\": 1681158900.3653188, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 7.205963134765625, \"count\": 1, \"min\": 7.205963134765625, \"max\": 7.205963134765625}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:00 INFO 139919854057280] Epoch[205] Batch[0] avg_epoch_loss=3.727603\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:00 INFO 139919854057280] #quality_metric: host=algo-1, epoch=205, batch=0 train loss <loss>=3.727602958679199\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:00 INFO 139919854057280] Epoch[205] Batch[5] avg_epoch_loss=3.849590\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:00 INFO 139919854057280] #quality_metric: host=algo-1, epoch=205, batch=5 train loss <loss>=3.8495901823043823\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:00 INFO 139919854057280] Epoch[205] Batch [5]#011Speed: 2830.46 samples/sec#011loss=3.849590\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:00 INFO 139919854057280] Epoch[205] Batch[10] avg_epoch_loss=3.704360\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:00 INFO 139919854057280] #quality_metric: host=algo-1, epoch=205, batch=10 train loss <loss>=3.5300830364227296\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:00 INFO 139919854057280] Epoch[205] Batch [10]#011Speed: 2806.77 samples/sec#011loss=3.530083\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:00 INFO 139919854057280] processed a total of 683 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158900.365373, \"EndTime\": 1681158900.7182362, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 352.80776023864746, \"count\": 1, \"min\": 352.80776023864746, \"max\": 352.80776023864746}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:00 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1935.4079983353129 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:00 INFO 139919854057280] #progress_metric: host=algo-1, completed 51.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:00 INFO 139919854057280] #quality_metric: host=algo-1, epoch=205, train loss <loss>=3.7043596614490855\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:00 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:00 INFO 139919854057280] Epoch[206] Batch[0] avg_epoch_loss=3.801782\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:00 INFO 139919854057280] #quality_metric: host=algo-1, epoch=206, batch=0 train loss <loss>=3.8017823696136475\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:00 INFO 139919854057280] Epoch[206] Batch[5] avg_epoch_loss=3.966009\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:00 INFO 139919854057280] #quality_metric: host=algo-1, epoch=206, batch=5 train loss <loss>=3.9660091400146484\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:00 INFO 139919854057280] Epoch[206] Batch [5]#011Speed: 2537.29 samples/sec#011loss=3.966009\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:01 INFO 139919854057280] Epoch[206] Batch[10] avg_epoch_loss=3.948130\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:01 INFO 139919854057280] #quality_metric: host=algo-1, epoch=206, batch=10 train loss <loss>=3.9266740322113036\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:01 INFO 139919854057280] Epoch[206] Batch [10]#011Speed: 2888.50 samples/sec#011loss=3.926674\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:01 INFO 139919854057280] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158900.7182949, \"EndTime\": 1681158901.0790894, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 360.4297637939453, \"count\": 1, \"min\": 360.4297637939453, \"max\": 360.4297637939453}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:01 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1827.932735319248 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:01 INFO 139919854057280] #progress_metric: host=algo-1, completed 51.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:01 INFO 139919854057280] #quality_metric: host=algo-1, epoch=206, train loss <loss>=3.9481295455585825\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:01 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:01 INFO 139919854057280] Epoch[207] Batch[0] avg_epoch_loss=3.845702\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:01 INFO 139919854057280] #quality_metric: host=algo-1, epoch=207, batch=0 train loss <loss>=3.8457024097442627\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:01 INFO 139919854057280] Epoch[207] Batch[5] avg_epoch_loss=3.731491\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:01 INFO 139919854057280] #quality_metric: host=algo-1, epoch=207, batch=5 train loss <loss>=3.7314913670221963\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:01 INFO 139919854057280] Epoch[207] Batch [5]#011Speed: 2667.43 samples/sec#011loss=3.731491\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:01 INFO 139919854057280] Epoch[207] Batch[10] avg_epoch_loss=3.647019\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:01 INFO 139919854057280] #quality_metric: host=algo-1, epoch=207, batch=10 train loss <loss>=3.5456519603729246\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:01 INFO 139919854057280] Epoch[207] Batch [10]#011Speed: 2892.44 samples/sec#011loss=3.545652\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:01 INFO 139919854057280] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158901.0791485, \"EndTime\": 1681158901.4411175, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 361.53554916381836, \"count\": 1, \"min\": 361.53554916381836, \"max\": 361.53554916381836}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:01 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1816.7550728440494 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:01 INFO 139919854057280] #progress_metric: host=algo-1, completed 52.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:01 INFO 139919854057280] #quality_metric: host=algo-1, epoch=207, train loss <loss>=3.6470189094543457\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:01 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:01 INFO 139919854057280] Epoch[208] Batch[0] avg_epoch_loss=3.703293\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:01 INFO 139919854057280] #quality_metric: host=algo-1, epoch=208, batch=0 train loss <loss>=3.7032930850982666\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:01 INFO 139919854057280] Epoch[208] Batch[5] avg_epoch_loss=3.826717\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:01 INFO 139919854057280] #quality_metric: host=algo-1, epoch=208, batch=5 train loss <loss>=3.826716661453247\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:01 INFO 139919854057280] Epoch[208] Batch [5]#011Speed: 2940.23 samples/sec#011loss=3.826717\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:01 INFO 139919854057280] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158901.441185, \"EndTime\": 1681158901.7790377, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 337.475061416626, \"count\": 1, \"min\": 337.475061416626, \"max\": 337.475061416626}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:01 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1886.8403866375988 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:01 INFO 139919854057280] #progress_metric: host=algo-1, completed 52.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:01 INFO 139919854057280] #quality_metric: host=algo-1, epoch=208, train loss <loss>=3.7553855180740356\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:01 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:01 INFO 139919854057280] Epoch[209] Batch[0] avg_epoch_loss=4.212323\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:01 INFO 139919854057280] #quality_metric: host=algo-1, epoch=209, batch=0 train loss <loss>=4.212323188781738\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:02 INFO 139919854057280] Epoch[209] Batch[5] avg_epoch_loss=4.045102\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:02 INFO 139919854057280] #quality_metric: host=algo-1, epoch=209, batch=5 train loss <loss>=4.045101523399353\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:02 INFO 139919854057280] Epoch[209] Batch [5]#011Speed: 2662.76 samples/sec#011loss=4.045102\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:02 INFO 139919854057280] Epoch[209] Batch[10] avg_epoch_loss=3.988896\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:02 INFO 139919854057280] #quality_metric: host=algo-1, epoch=209, batch=10 train loss <loss>=3.9214494228363037\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:02 INFO 139919854057280] Epoch[209] Batch [10]#011Speed: 2352.74 samples/sec#011loss=3.921449\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:02 INFO 139919854057280] processed a total of 714 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158901.7791247, \"EndTime\": 1681158902.2332397, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 453.5946846008301, \"count\": 1, \"min\": 453.5946846008301, \"max\": 453.5946846008301}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:02 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1573.455698676603 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:02 INFO 139919854057280] #progress_metric: host=algo-1, completed 52.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:02 INFO 139919854057280] #quality_metric: host=algo-1, epoch=209, train loss <loss>=4.032290816307068\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:02 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:02 INFO 139919854057280] Epoch[210] Batch[0] avg_epoch_loss=4.066561\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:02 INFO 139919854057280] #quality_metric: host=algo-1, epoch=210, batch=0 train loss <loss>=4.066561222076416\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:02 INFO 139919854057280] Epoch[210] Batch[5] avg_epoch_loss=3.935326\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:02 INFO 139919854057280] #quality_metric: host=algo-1, epoch=210, batch=5 train loss <loss>=3.9353256225585938\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:02 INFO 139919854057280] Epoch[210] Batch [5]#011Speed: 2444.50 samples/sec#011loss=3.935326\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:02 INFO 139919854057280] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158902.233388, \"EndTime\": 1681158902.601892, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 367.7976131439209, \"count\": 1, \"min\": 367.7976131439209, \"max\": 367.7976131439209}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:02 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1698.7249065404512 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:02 INFO 139919854057280] #progress_metric: host=algo-1, completed 52.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:02 INFO 139919854057280] #quality_metric: host=algo-1, epoch=210, train loss <loss>=3.930019474029541\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:02 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:02 INFO 139919854057280] Epoch[211] Batch[0] avg_epoch_loss=5.032680\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:02 INFO 139919854057280] #quality_metric: host=algo-1, epoch=211, batch=0 train loss <loss>=5.032679557800293\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:02 INFO 139919854057280] Epoch[211] Batch[5] avg_epoch_loss=4.227582\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:02 INFO 139919854057280] #quality_metric: host=algo-1, epoch=211, batch=5 train loss <loss>=4.227582375208537\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:02 INFO 139919854057280] Epoch[211] Batch [5]#011Speed: 2630.89 samples/sec#011loss=4.227582\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:02 INFO 139919854057280] Epoch[211] Batch[10] avg_epoch_loss=4.212535\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:02 INFO 139919854057280] #quality_metric: host=algo-1, epoch=211, batch=10 train loss <loss>=4.194478321075439\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:02 INFO 139919854057280] Epoch[211] Batch [10]#011Speed: 2442.48 samples/sec#011loss=4.194478\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:03 INFO 139919854057280] processed a total of 711 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158902.6019545, \"EndTime\": 1681158903.0134878, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 410.6283187866211, \"count\": 1, \"min\": 410.6283187866211, \"max\": 410.6283187866211}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:03 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1731.0566483276002 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:03 INFO 139919854057280] #progress_metric: host=algo-1, completed 53.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:03 INFO 139919854057280] #quality_metric: host=algo-1, epoch=211, train loss <loss>=4.132166802883148\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:03 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:03 INFO 139919854057280] Epoch[212] Batch[0] avg_epoch_loss=3.967883\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:03 INFO 139919854057280] #quality_metric: host=algo-1, epoch=212, batch=0 train loss <loss>=3.9678828716278076\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:03 INFO 139919854057280] Epoch[212] Batch[5] avg_epoch_loss=3.926278\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:03 INFO 139919854057280] #quality_metric: host=algo-1, epoch=212, batch=5 train loss <loss>=3.926278273264567\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:03 INFO 139919854057280] Epoch[212] Batch [5]#011Speed: 2063.16 samples/sec#011loss=3.926278\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:03 INFO 139919854057280] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158903.0135586, \"EndTime\": 1681158903.488985, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 474.95532035827637, \"count\": 1, \"min\": 474.95532035827637, \"max\": 474.95532035827637}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:03 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1298.716215449886 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:03 INFO 139919854057280] #progress_metric: host=algo-1, completed 53.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:03 INFO 139919854057280] #quality_metric: host=algo-1, epoch=212, train loss <loss>=3.9418476104736326\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:03 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:03 INFO 139919854057280] Epoch[213] Batch[0] avg_epoch_loss=3.905655\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:03 INFO 139919854057280] #quality_metric: host=algo-1, epoch=213, batch=0 train loss <loss>=3.9056553840637207\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:03 INFO 139919854057280] Epoch[213] Batch[5] avg_epoch_loss=3.985510\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:03 INFO 139919854057280] #quality_metric: host=algo-1, epoch=213, batch=5 train loss <loss>=3.9855098724365234\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:03 INFO 139919854057280] Epoch[213] Batch [5]#011Speed: 2339.01 samples/sec#011loss=3.985510\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:03 INFO 139919854057280] Epoch[213] Batch[10] avg_epoch_loss=4.040815\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:03 INFO 139919854057280] #quality_metric: host=algo-1, epoch=213, batch=10 train loss <loss>=4.107182121276855\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:03 INFO 139919854057280] Epoch[213] Batch [10]#011Speed: 1913.36 samples/sec#011loss=4.107182\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:03 INFO 139919854057280] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158903.4890819, \"EndTime\": 1681158903.9844446, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 494.464635848999, \"count\": 1, \"min\": 494.464635848999, \"max\": 494.464635848999}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:03 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1322.336570069152 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:03 INFO 139919854057280] #progress_metric: host=algo-1, completed 53.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:03 INFO 139919854057280] #quality_metric: host=algo-1, epoch=213, train loss <loss>=4.04081544009122\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:03 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:04 INFO 139919854057280] Epoch[214] Batch[0] avg_epoch_loss=3.878027\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:04 INFO 139919854057280] #quality_metric: host=algo-1, epoch=214, batch=0 train loss <loss>=3.8780269622802734\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:04 INFO 139919854057280] Epoch[214] Batch[5] avg_epoch_loss=4.081215\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:04 INFO 139919854057280] #quality_metric: host=algo-1, epoch=214, batch=5 train loss <loss>=4.081214586893718\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:04 INFO 139919854057280] Epoch[214] Batch [5]#011Speed: 2683.98 samples/sec#011loss=4.081215\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:04 INFO 139919854057280] Epoch[214] Batch[10] avg_epoch_loss=4.062262\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:04 INFO 139919854057280] #quality_metric: host=algo-1, epoch=214, batch=10 train loss <loss>=4.039518022537232\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:04 INFO 139919854057280] Epoch[214] Batch [10]#011Speed: 2842.83 samples/sec#011loss=4.039518\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:04 INFO 139919854057280] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158903.9845285, \"EndTime\": 1681158904.416401, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 431.4706325531006, \"count\": 1, \"min\": 431.4706325531006, \"max\": 431.4706325531006}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:04 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1517.676475446378 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:04 INFO 139919854057280] #progress_metric: host=algo-1, completed 53.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:04 INFO 139919854057280] #quality_metric: host=algo-1, epoch=214, train loss <loss>=4.0622616030953145\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:04 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:04 INFO 139919854057280] Epoch[215] Batch[0] avg_epoch_loss=4.013767\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:04 INFO 139919854057280] #quality_metric: host=algo-1, epoch=215, batch=0 train loss <loss>=4.013766765594482\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:04 INFO 139919854057280] Epoch[215] Batch[5] avg_epoch_loss=3.999650\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:04 INFO 139919854057280] #quality_metric: host=algo-1, epoch=215, batch=5 train loss <loss>=3.999650478363037\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:04 INFO 139919854057280] Epoch[215] Batch [5]#011Speed: 3033.28 samples/sec#011loss=3.999650\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:04 INFO 139919854057280] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158904.4164793, \"EndTime\": 1681158904.739161, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 322.314977645874, \"count\": 1, \"min\": 322.314977645874, \"max\": 322.314977645874}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:04 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1916.6928838314338 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:04 INFO 139919854057280] #progress_metric: host=algo-1, completed 54.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:04 INFO 139919854057280] #quality_metric: host=algo-1, epoch=215, train loss <loss>=3.9847537755966185\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:04 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:04 INFO 139919854057280] Epoch[216] Batch[0] avg_epoch_loss=3.784105\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:04 INFO 139919854057280] #quality_metric: host=algo-1, epoch=216, batch=0 train loss <loss>=3.784105062484741\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:04 INFO 139919854057280] Epoch[216] Batch[5] avg_epoch_loss=3.735528\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:04 INFO 139919854057280] #quality_metric: host=algo-1, epoch=216, batch=5 train loss <loss>=3.7355278730392456\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:04 INFO 139919854057280] Epoch[216] Batch [5]#011Speed: 2897.61 samples/sec#011loss=3.735528\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:05 INFO 139919854057280] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158904.7392473, \"EndTime\": 1681158905.060983, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 321.382999420166, \"count\": 1, \"min\": 321.382999420166, \"max\": 321.382999420166}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:05 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1978.2310744208605 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:05 INFO 139919854057280] #progress_metric: host=algo-1, completed 54.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:05 INFO 139919854057280] #quality_metric: host=algo-1, epoch=216, train loss <loss>=3.8396175622940065\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:05 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:05 INFO 139919854057280] Epoch[217] Batch[0] avg_epoch_loss=3.922381\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:05 INFO 139919854057280] #quality_metric: host=algo-1, epoch=217, batch=0 train loss <loss>=3.9223814010620117\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:05 INFO 139919854057280] Epoch[217] Batch[5] avg_epoch_loss=3.896463\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:05 INFO 139919854057280] #quality_metric: host=algo-1, epoch=217, batch=5 train loss <loss>=3.8964630762736\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:05 INFO 139919854057280] Epoch[217] Batch [5]#011Speed: 3032.89 samples/sec#011loss=3.896463\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:05 INFO 139919854057280] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158905.0610716, \"EndTime\": 1681158905.4071183, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 345.69406509399414, \"count\": 1, \"min\": 345.69406509399414, \"max\": 345.69406509399414}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:05 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1842.1666393167957 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:05 INFO 139919854057280] #progress_metric: host=algo-1, completed 54.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:05 INFO 139919854057280] #quality_metric: host=algo-1, epoch=217, train loss <loss>=3.8652509689331054\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:05 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:05 INFO 139919854057280] Epoch[218] Batch[0] avg_epoch_loss=4.126391\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:05 INFO 139919854057280] #quality_metric: host=algo-1, epoch=218, batch=0 train loss <loss>=4.1263909339904785\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:05 INFO 139919854057280] Epoch[218] Batch[5] avg_epoch_loss=4.046181\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:05 INFO 139919854057280] #quality_metric: host=algo-1, epoch=218, batch=5 train loss <loss>=4.046181241671245\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:05 INFO 139919854057280] Epoch[218] Batch [5]#011Speed: 2650.72 samples/sec#011loss=4.046181\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:05 INFO 139919854057280] Epoch[218] Batch[10] avg_epoch_loss=4.075083\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:05 INFO 139919854057280] #quality_metric: host=algo-1, epoch=218, batch=10 train loss <loss>=4.10976619720459\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:05 INFO 139919854057280] Epoch[218] Batch [10]#011Speed: 2904.34 samples/sec#011loss=4.109766\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:05 INFO 139919854057280] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158905.407183, \"EndTime\": 1681158905.768896, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 361.2861633300781, \"count\": 1, \"min\": 361.2861633300781, \"max\": 361.2861633300781}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:05 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1848.5082674344496 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:05 INFO 139919854057280] #progress_metric: host=algo-1, completed 54.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:05 INFO 139919854057280] #quality_metric: host=algo-1, epoch=218, train loss <loss>=4.075083494186401\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:05 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:05 INFO 139919854057280] Epoch[219] Batch[0] avg_epoch_loss=4.065186\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:05 INFO 139919854057280] #quality_metric: host=algo-1, epoch=219, batch=0 train loss <loss>=4.065185546875\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:06 INFO 139919854057280] Epoch[219] Batch[5] avg_epoch_loss=3.958707\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:06 INFO 139919854057280] #quality_metric: host=algo-1, epoch=219, batch=5 train loss <loss>=3.9587069749832153\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:06 INFO 139919854057280] Epoch[219] Batch [5]#011Speed: 2907.25 samples/sec#011loss=3.958707\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:06 INFO 139919854057280] Epoch[219] Batch[10] avg_epoch_loss=3.941907\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:06 INFO 139919854057280] #quality_metric: host=algo-1, epoch=219, batch=10 train loss <loss>=3.921747159957886\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:06 INFO 139919854057280] Epoch[219] Batch [10]#011Speed: 2624.26 samples/sec#011loss=3.921747\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:06 INFO 139919854057280] processed a total of 677 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158905.768955, \"EndTime\": 1681158906.1247087, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 355.334997177124, \"count\": 1, \"min\": 355.334997177124, \"max\": 355.334997177124}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:06 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1904.6628862425025 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:06 INFO 139919854057280] #progress_metric: host=algo-1, completed 55.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:06 INFO 139919854057280] #quality_metric: host=algo-1, epoch=219, train loss <loss>=3.941907059062611\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:06 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:06 INFO 139919854057280] Epoch[220] Batch[0] avg_epoch_loss=4.294509\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:06 INFO 139919854057280] #quality_metric: host=algo-1, epoch=220, batch=0 train loss <loss>=4.294508934020996\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:06 INFO 139919854057280] Epoch[220] Batch[5] avg_epoch_loss=4.034534\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:06 INFO 139919854057280] #quality_metric: host=algo-1, epoch=220, batch=5 train loss <loss>=4.0345338980356855\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:06 INFO 139919854057280] Epoch[220] Batch [5]#011Speed: 3013.05 samples/sec#011loss=4.034534\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:06 INFO 139919854057280] Epoch[220] Batch[10] avg_epoch_loss=3.837448\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:06 INFO 139919854057280] #quality_metric: host=algo-1, epoch=220, batch=10 train loss <loss>=3.600945806503296\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:06 INFO 139919854057280] Epoch[220] Batch [10]#011Speed: 2845.01 samples/sec#011loss=3.600946\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:06 INFO 139919854057280] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158906.124783, \"EndTime\": 1681158906.468915, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 343.71304512023926, \"count\": 1, \"min\": 343.71304512023926, \"max\": 343.71304512023926}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:06 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1907.9745757187852 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:06 INFO 139919854057280] #progress_metric: host=algo-1, completed 55.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:06 INFO 139919854057280] #quality_metric: host=algo-1, epoch=220, train loss <loss>=3.837448401884599\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:06 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:06 INFO 139919854057280] Epoch[221] Batch[0] avg_epoch_loss=3.857601\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:06 INFO 139919854057280] #quality_metric: host=algo-1, epoch=221, batch=0 train loss <loss>=3.8576009273529053\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:06 INFO 139919854057280] Epoch[221] Batch[5] avg_epoch_loss=3.862893\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:06 INFO 139919854057280] #quality_metric: host=algo-1, epoch=221, batch=5 train loss <loss>=3.862893263498942\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:06 INFO 139919854057280] Epoch[221] Batch [5]#011Speed: 2804.74 samples/sec#011loss=3.862893\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:06 INFO 139919854057280] Epoch[221] Batch[10] avg_epoch_loss=3.876602\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:06 INFO 139919854057280] #quality_metric: host=algo-1, epoch=221, batch=10 train loss <loss>=3.8930514335632322\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:06 INFO 139919854057280] Epoch[221] Batch [10]#011Speed: 2682.94 samples/sec#011loss=3.893051\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:06 INFO 139919854057280] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158906.468988, \"EndTime\": 1681158906.8260615, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 356.6327095031738, \"count\": 1, \"min\": 356.6327095031738, \"max\": 356.6327095031738}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:06 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1889.3614818894143 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:06 INFO 139919854057280] #progress_metric: host=algo-1, completed 55.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:06 INFO 139919854057280] #quality_metric: host=algo-1, epoch=221, train loss <loss>=3.876601522619074\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:06 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:06 INFO 139919854057280] Epoch[222] Batch[0] avg_epoch_loss=3.728480\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:06 INFO 139919854057280] #quality_metric: host=algo-1, epoch=222, batch=0 train loss <loss>=3.7284798622131348\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:07 INFO 139919854057280] Epoch[222] Batch[5] avg_epoch_loss=3.734538\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:07 INFO 139919854057280] #quality_metric: host=algo-1, epoch=222, batch=5 train loss <loss>=3.734537680943807\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:07 INFO 139919854057280] Epoch[222] Batch [5]#011Speed: 2990.14 samples/sec#011loss=3.734538\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:07 INFO 139919854057280] Epoch[222] Batch[10] avg_epoch_loss=3.950512\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:07 INFO 139919854057280] #quality_metric: host=algo-1, epoch=222, batch=10 train loss <loss>=4.2096803188323975\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:07 INFO 139919854057280] Epoch[222] Batch [10]#011Speed: 2658.33 samples/sec#011loss=4.209680\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:07 INFO 139919854057280] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158906.8261342, \"EndTime\": 1681158907.1761112, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 349.5621681213379, \"count\": 1, \"min\": 349.5621681213379, \"max\": 349.5621681213379}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:07 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1919.0284119888393 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:07 INFO 139919854057280] #progress_metric: host=algo-1, completed 55.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:07 INFO 139919854057280] #quality_metric: host=algo-1, epoch=222, train loss <loss>=3.9505116072568027\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:07 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:07 INFO 139919854057280] Epoch[223] Batch[0] avg_epoch_loss=4.474018\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:07 INFO 139919854057280] #quality_metric: host=algo-1, epoch=223, batch=0 train loss <loss>=4.47401762008667\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:07 INFO 139919854057280] Epoch[223] Batch[5] avg_epoch_loss=4.072984\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:07 INFO 139919854057280] #quality_metric: host=algo-1, epoch=223, batch=5 train loss <loss>=4.072983503341675\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:07 INFO 139919854057280] Epoch[223] Batch [5]#011Speed: 2356.42 samples/sec#011loss=4.072984\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:07 INFO 139919854057280] Epoch[223] Batch[10] avg_epoch_loss=3.995630\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:07 INFO 139919854057280] #quality_metric: host=algo-1, epoch=223, batch=10 train loss <loss>=3.9028059005737306\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:07 INFO 139919854057280] Epoch[223] Batch [10]#011Speed: 2482.53 samples/sec#011loss=3.902806\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:07 INFO 139919854057280] processed a total of 700 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158907.1761758, \"EndTime\": 1681158907.5695033, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 393.0187225341797, \"count\": 1, \"min\": 393.0187225341797, \"max\": 393.0187225341797}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:07 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1780.6427510082785 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:07 INFO 139919854057280] #progress_metric: host=algo-1, completed 56.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:07 INFO 139919854057280] #quality_metric: host=algo-1, epoch=223, train loss <loss>=3.995630047538064\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:07 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:07 INFO 139919854057280] Epoch[224] Batch[0] avg_epoch_loss=4.028965\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:07 INFO 139919854057280] #quality_metric: host=algo-1, epoch=224, batch=0 train loss <loss>=4.028965473175049\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:07 INFO 139919854057280] Epoch[224] Batch[5] avg_epoch_loss=4.073668\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:07 INFO 139919854057280] #quality_metric: host=algo-1, epoch=224, batch=5 train loss <loss>=4.073667764663696\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:07 INFO 139919854057280] Epoch[224] Batch [5]#011Speed: 2967.48 samples/sec#011loss=4.073668\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:07 INFO 139919854057280] Epoch[224] Batch[10] avg_epoch_loss=4.094115\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:07 INFO 139919854057280] #quality_metric: host=algo-1, epoch=224, batch=10 train loss <loss>=4.1186511516571045\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:07 INFO 139919854057280] Epoch[224] Batch [10]#011Speed: 2775.73 samples/sec#011loss=4.118651\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:07 INFO 139919854057280] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158907.5695717, \"EndTime\": 1681158907.9224987, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 352.39601135253906, \"count\": 1, \"min\": 352.39601135253906, \"max\": 352.39601135253906}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:07 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1889.3670181969687 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:07 INFO 139919854057280] #progress_metric: host=algo-1, completed 56.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:07 INFO 139919854057280] #quality_metric: host=algo-1, epoch=224, train loss <loss>=4.094114758751609\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:07 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:08 INFO 139919854057280] Epoch[225] Batch[0] avg_epoch_loss=3.961618\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:08 INFO 139919854057280] #quality_metric: host=algo-1, epoch=225, batch=0 train loss <loss>=3.961618423461914\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:08 INFO 139919854057280] Epoch[225] Batch[5] avg_epoch_loss=4.019730\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:08 INFO 139919854057280] #quality_metric: host=algo-1, epoch=225, batch=5 train loss <loss>=4.019729892412822\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:08 INFO 139919854057280] Epoch[225] Batch [5]#011Speed: 2588.17 samples/sec#011loss=4.019730\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:08 INFO 139919854057280] Epoch[225] Batch[10] avg_epoch_loss=4.027704\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:08 INFO 139919854057280] #quality_metric: host=algo-1, epoch=225, batch=10 train loss <loss>=4.0372724533081055\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:08 INFO 139919854057280] Epoch[225] Batch [10]#011Speed: 2357.31 samples/sec#011loss=4.037272\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:08 INFO 139919854057280] processed a total of 670 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158907.9225705, \"EndTime\": 1681158908.305108, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 382.16447830200195, \"count\": 1, \"min\": 382.16447830200195, \"max\": 382.16447830200195}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:08 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1752.6874426593704 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:08 INFO 139919854057280] #progress_metric: host=algo-1, completed 56.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:08 INFO 139919854057280] #quality_metric: host=algo-1, epoch=225, train loss <loss>=4.027703783728859\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:08 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:08 INFO 139919854057280] Epoch[226] Batch[0] avg_epoch_loss=4.002307\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:08 INFO 139919854057280] #quality_metric: host=algo-1, epoch=226, batch=0 train loss <loss>=4.002306938171387\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:08 INFO 139919854057280] Epoch[226] Batch[5] avg_epoch_loss=3.828870\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:08 INFO 139919854057280] #quality_metric: host=algo-1, epoch=226, batch=5 train loss <loss>=3.828869620958964\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:08 INFO 139919854057280] Epoch[226] Batch [5]#011Speed: 2366.86 samples/sec#011loss=3.828870\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:08 INFO 139919854057280] Epoch[226] Batch[10] avg_epoch_loss=3.839817\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:08 INFO 139919854057280] #quality_metric: host=algo-1, epoch=226, batch=10 train loss <loss>=3.8529531002044677\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:08 INFO 139919854057280] Epoch[226] Batch [10]#011Speed: 2495.83 samples/sec#011loss=3.852953\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:08 INFO 139919854057280] processed a total of 678 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158908.3051798, \"EndTime\": 1681158908.7032094, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 397.6116180419922, \"count\": 1, \"min\": 397.6116180419922, \"max\": 397.6116180419922}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:08 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1704.7706272862756 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:08 INFO 139919854057280] #progress_metric: host=algo-1, completed 56.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:08 INFO 139919854057280] #quality_metric: host=algo-1, epoch=226, train loss <loss>=3.8398166569796475\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:08 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:08 INFO 139919854057280] Epoch[227] Batch[0] avg_epoch_loss=3.940042\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:08 INFO 139919854057280] #quality_metric: host=algo-1, epoch=227, batch=0 train loss <loss>=3.9400415420532227\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:08 INFO 139919854057280] Epoch[227] Batch[5] avg_epoch_loss=3.952801\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:08 INFO 139919854057280] #quality_metric: host=algo-1, epoch=227, batch=5 train loss <loss>=3.952800909678141\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:08 INFO 139919854057280] Epoch[227] Batch [5]#011Speed: 2952.46 samples/sec#011loss=3.952801\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:09 INFO 139919854057280] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158908.7032752, \"EndTime\": 1681158909.025777, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 322.1242427825928, \"count\": 1, \"min\": 322.1242427825928, \"max\": 322.1242427825928}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:09 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1976.8920984890133 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:09 INFO 139919854057280] #progress_metric: host=algo-1, completed 57.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:09 INFO 139919854057280] #quality_metric: host=algo-1, epoch=227, train loss <loss>=4.025950384140015\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:09 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:09 INFO 139919854057280] Epoch[228] Batch[0] avg_epoch_loss=3.848021\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:09 INFO 139919854057280] #quality_metric: host=algo-1, epoch=228, batch=0 train loss <loss>=3.8480207920074463\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:09 INFO 139919854057280] Epoch[228] Batch[5] avg_epoch_loss=4.027801\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:09 INFO 139919854057280] #quality_metric: host=algo-1, epoch=228, batch=5 train loss <loss>=4.027801473935445\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:09 INFO 139919854057280] Epoch[228] Batch [5]#011Speed: 2957.01 samples/sec#011loss=4.027801\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:09 INFO 139919854057280] Epoch[228] Batch[10] avg_epoch_loss=3.848073\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:09 INFO 139919854057280] #quality_metric: host=algo-1, epoch=228, batch=10 train loss <loss>=3.6323984622955323\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:09 INFO 139919854057280] Epoch[228] Batch [10]#011Speed: 2823.59 samples/sec#011loss=3.632398\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:09 INFO 139919854057280] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158909.0258424, \"EndTime\": 1681158909.3788052, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 352.4794578552246, \"count\": 1, \"min\": 352.4794578552246, \"max\": 352.4794578552246}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:09 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1871.9000297530497 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:09 INFO 139919854057280] #progress_metric: host=algo-1, completed 57.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:09 INFO 139919854057280] #quality_metric: host=algo-1, epoch=228, train loss <loss>=3.8480728322809394\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:09 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:09 INFO 139919854057280] Epoch[229] Batch[0] avg_epoch_loss=3.838454\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:09 INFO 139919854057280] #quality_metric: host=algo-1, epoch=229, batch=0 train loss <loss>=3.838454246520996\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:09 INFO 139919854057280] Epoch[229] Batch[5] avg_epoch_loss=3.777423\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:09 INFO 139919854057280] #quality_metric: host=algo-1, epoch=229, batch=5 train loss <loss>=3.777422626813253\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:09 INFO 139919854057280] Epoch[229] Batch [5]#011Speed: 2987.93 samples/sec#011loss=3.777423\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:09 INFO 139919854057280] Epoch[229] Batch[10] avg_epoch_loss=3.868215\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:09 INFO 139919854057280] #quality_metric: host=algo-1, epoch=229, batch=10 train loss <loss>=3.977166700363159\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:09 INFO 139919854057280] Epoch[229] Batch [10]#011Speed: 2904.27 samples/sec#011loss=3.977167\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:09 INFO 139919854057280] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158909.3788798, \"EndTime\": 1681158909.7276418, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 348.38414192199707, \"count\": 1, \"min\": 348.38414192199707, \"max\": 348.38414192199707}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:09 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1865.2089613302155 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:09 INFO 139919854057280] #progress_metric: host=algo-1, completed 57.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:09 INFO 139919854057280] #quality_metric: host=algo-1, epoch=229, train loss <loss>=3.868215387517756\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:09 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:09 INFO 139919854057280] Epoch[230] Batch[0] avg_epoch_loss=4.025937\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:09 INFO 139919854057280] #quality_metric: host=algo-1, epoch=230, batch=0 train loss <loss>=4.025937080383301\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:09 INFO 139919854057280] Epoch[230] Batch[5] avg_epoch_loss=3.995159\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:09 INFO 139919854057280] #quality_metric: host=algo-1, epoch=230, batch=5 train loss <loss>=3.995158553123474\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:09 INFO 139919854057280] Epoch[230] Batch [5]#011Speed: 2938.65 samples/sec#011loss=3.995159\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:10 INFO 139919854057280] Epoch[230] Batch[10] avg_epoch_loss=4.053426\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:10 INFO 139919854057280] #quality_metric: host=algo-1, epoch=230, batch=10 train loss <loss>=4.123346996307373\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:10 INFO 139919854057280] Epoch[230] Batch [10]#011Speed: 2754.46 samples/sec#011loss=4.123347\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:10 INFO 139919854057280] processed a total of 684 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158909.7277114, \"EndTime\": 1681158910.0801911, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 352.02860832214355, \"count\": 1, \"min\": 352.02860832214355, \"max\": 352.02860832214355}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:10 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1942.4542610843441 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:10 INFO 139919854057280] #progress_metric: host=algo-1, completed 57.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:10 INFO 139919854057280] #quality_metric: host=algo-1, epoch=230, train loss <loss>=4.053426027297974\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:10 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:10 INFO 139919854057280] Epoch[231] Batch[0] avg_epoch_loss=3.839628\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:10 INFO 139919854057280] #quality_metric: host=algo-1, epoch=231, batch=0 train loss <loss>=3.839628219604492\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:10 INFO 139919854057280] Epoch[231] Batch[5] avg_epoch_loss=3.910575\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:10 INFO 139919854057280] #quality_metric: host=algo-1, epoch=231, batch=5 train loss <loss>=3.910575032234192\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:10 INFO 139919854057280] Epoch[231] Batch [5]#011Speed: 2665.82 samples/sec#011loss=3.910575\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:10 INFO 139919854057280] Epoch[231] Batch[10] avg_epoch_loss=3.933280\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:10 INFO 139919854057280] #quality_metric: host=algo-1, epoch=231, batch=10 train loss <loss>=3.9605266571044924\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:10 INFO 139919854057280] Epoch[231] Batch [10]#011Speed: 2340.01 samples/sec#011loss=3.960527\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:10 INFO 139919854057280] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158910.0802498, \"EndTime\": 1681158910.4610617, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 380.29980659484863, \"count\": 1, \"min\": 380.29980659484863, \"max\": 380.29980659484863}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:10 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1748.1831059008757 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:10 INFO 139919854057280] #progress_metric: host=algo-1, completed 58.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:10 INFO 139919854057280] #quality_metric: host=algo-1, epoch=231, train loss <loss>=3.9332803162661465\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:10 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:10 INFO 139919854057280] Epoch[232] Batch[0] avg_epoch_loss=3.963146\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:10 INFO 139919854057280] #quality_metric: host=algo-1, epoch=232, batch=0 train loss <loss>=3.963146448135376\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:10 INFO 139919854057280] Epoch[232] Batch[5] avg_epoch_loss=3.934828\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:10 INFO 139919854057280] #quality_metric: host=algo-1, epoch=232, batch=5 train loss <loss>=3.93482768535614\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:10 INFO 139919854057280] Epoch[232] Batch [5]#011Speed: 3059.68 samples/sec#011loss=3.934828\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:10 INFO 139919854057280] Epoch[232] Batch[10] avg_epoch_loss=3.639892\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:10 INFO 139919854057280] #quality_metric: host=algo-1, epoch=232, batch=10 train loss <loss>=3.285969650745392\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:10 INFO 139919854057280] Epoch[232] Batch [10]#011Speed: 2725.22 samples/sec#011loss=3.285970\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:10 INFO 139919854057280] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158910.4611266, \"EndTime\": 1681158910.814091, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 352.59127616882324, \"count\": 1, \"min\": 352.59127616882324, \"max\": 352.59127616882324}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:10 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1828.785521820426 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:10 INFO 139919854057280] #progress_metric: host=algo-1, completed 58.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:10 INFO 139919854057280] #quality_metric: host=algo-1, epoch=232, train loss <loss>=3.639892215078527\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:10 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:10 INFO 139919854057280] Epoch[233] Batch[0] avg_epoch_loss=5.035585\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:10 INFO 139919854057280] #quality_metric: host=algo-1, epoch=233, batch=0 train loss <loss>=5.035584926605225\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:11 INFO 139919854057280] Epoch[233] Batch[5] avg_epoch_loss=4.345704\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:11 INFO 139919854057280] #quality_metric: host=algo-1, epoch=233, batch=5 train loss <loss>=4.345704078674316\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:11 INFO 139919854057280] Epoch[233] Batch [5]#011Speed: 3022.05 samples/sec#011loss=4.345704\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:11 INFO 139919854057280] Epoch[233] Batch[10] avg_epoch_loss=4.273118\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:11 INFO 139919854057280] #quality_metric: host=algo-1, epoch=233, batch=10 train loss <loss>=4.186015558242798\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:11 INFO 139919854057280] Epoch[233] Batch [10]#011Speed: 2936.44 samples/sec#011loss=4.186016\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:11 INFO 139919854057280] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158910.8141608, \"EndTime\": 1681158911.1538272, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 339.27011489868164, \"count\": 1, \"min\": 339.27011489868164, \"max\": 339.27011489868164}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:11 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1968.1207651927627 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:11 INFO 139919854057280] #progress_metric: host=algo-1, completed 58.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:11 INFO 139919854057280] #quality_metric: host=algo-1, epoch=233, train loss <loss>=4.273118387569081\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:11 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:11 INFO 139919854057280] Epoch[234] Batch[0] avg_epoch_loss=3.735688\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:11 INFO 139919854057280] #quality_metric: host=algo-1, epoch=234, batch=0 train loss <loss>=3.7356884479522705\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:11 INFO 139919854057280] Epoch[234] Batch[5] avg_epoch_loss=3.798636\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:11 INFO 139919854057280] #quality_metric: host=algo-1, epoch=234, batch=5 train loss <loss>=3.7986358801523843\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:11 INFO 139919854057280] Epoch[234] Batch [5]#011Speed: 2610.60 samples/sec#011loss=3.798636\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:11 INFO 139919854057280] Epoch[234] Batch[10] avg_epoch_loss=3.979336\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:11 INFO 139919854057280] #quality_metric: host=algo-1, epoch=234, batch=10 train loss <loss>=4.196175670623779\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:11 INFO 139919854057280] Epoch[234] Batch [10]#011Speed: 2718.51 samples/sec#011loss=4.196176\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:11 INFO 139919854057280] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158911.1539333, \"EndTime\": 1681158911.517036, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 362.6985549926758, \"count\": 1, \"min\": 362.6985549926758, \"max\": 362.6985549926758}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:11 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1838.4005656615573 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:11 INFO 139919854057280] #progress_metric: host=algo-1, completed 58.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:11 INFO 139919854057280] #quality_metric: host=algo-1, epoch=234, train loss <loss>=3.9793357849121094\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:11 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:11 INFO 139919854057280] Epoch[235] Batch[0] avg_epoch_loss=3.830034\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:11 INFO 139919854057280] #quality_metric: host=algo-1, epoch=235, batch=0 train loss <loss>=3.8300342559814453\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:11 INFO 139919854057280] Epoch[235] Batch[5] avg_epoch_loss=3.941480\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:11 INFO 139919854057280] #quality_metric: host=algo-1, epoch=235, batch=5 train loss <loss>=3.9414804379145303\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:11 INFO 139919854057280] Epoch[235] Batch [5]#011Speed: 2872.04 samples/sec#011loss=3.941480\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:11 INFO 139919854057280] Epoch[235] Batch[10] avg_epoch_loss=3.987316\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:11 INFO 139919854057280] #quality_metric: host=algo-1, epoch=235, batch=10 train loss <loss>=4.042319107055664\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:11 INFO 139919854057280] Epoch[235] Batch [10]#011Speed: 2707.11 samples/sec#011loss=4.042319\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:11 INFO 139919854057280] processed a total of 699 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158911.5171049, \"EndTime\": 1681158911.9023693, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 384.95326042175293, \"count\": 1, \"min\": 384.95326042175293, \"max\": 384.95326042175293}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:11 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1815.3540010848217 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:11 INFO 139919854057280] #progress_metric: host=algo-1, completed 59.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:11 INFO 139919854057280] #quality_metric: host=algo-1, epoch=235, train loss <loss>=3.987316196615046\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:11 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:12 INFO 139919854057280] Epoch[236] Batch[0] avg_epoch_loss=4.163662\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:12 INFO 139919854057280] #quality_metric: host=algo-1, epoch=236, batch=0 train loss <loss>=4.163661956787109\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:12 INFO 139919854057280] Epoch[236] Batch[5] avg_epoch_loss=4.095266\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:12 INFO 139919854057280] #quality_metric: host=algo-1, epoch=236, batch=5 train loss <loss>=4.095266342163086\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:12 INFO 139919854057280] Epoch[236] Batch [5]#011Speed: 2367.87 samples/sec#011loss=4.095266\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:12 INFO 139919854057280] Epoch[236] Batch[10] avg_epoch_loss=4.031420\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:12 INFO 139919854057280] #quality_metric: host=algo-1, epoch=236, batch=10 train loss <loss>=3.9548036098480224\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:12 INFO 139919854057280] Epoch[236] Batch [10]#011Speed: 2239.32 samples/sec#011loss=3.954804\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:12 INFO 139919854057280] processed a total of 711 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158911.9024365, \"EndTime\": 1681158912.3446095, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 441.85495376586914, \"count\": 1, \"min\": 441.85495376586914, \"max\": 441.85495376586914}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:12 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1608.7590111863003 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:12 INFO 139919854057280] #progress_metric: host=algo-1, completed 59.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:12 INFO 139919854057280] #quality_metric: host=algo-1, epoch=236, train loss <loss>=3.9434013962745667\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:12 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:12 INFO 139919854057280] Epoch[237] Batch[0] avg_epoch_loss=4.634669\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:12 INFO 139919854057280] #quality_metric: host=algo-1, epoch=237, batch=0 train loss <loss>=4.634669303894043\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:12 INFO 139919854057280] Epoch[237] Batch[5] avg_epoch_loss=4.165550\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:12 INFO 139919854057280] #quality_metric: host=algo-1, epoch=237, batch=5 train loss <loss>=4.1655503908793134\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:12 INFO 139919854057280] Epoch[237] Batch [5]#011Speed: 2959.86 samples/sec#011loss=4.165550\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:12 INFO 139919854057280] Epoch[237] Batch[10] avg_epoch_loss=4.037872\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:12 INFO 139919854057280] #quality_metric: host=algo-1, epoch=237, batch=10 train loss <loss>=3.884657955169678\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:12 INFO 139919854057280] Epoch[237] Batch [10]#011Speed: 2734.86 samples/sec#011loss=3.884658\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:12 INFO 139919854057280] processed a total of 680 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158912.344679, \"EndTime\": 1681158912.6978822, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 352.7183532714844, \"count\": 1, \"min\": 352.7183532714844, \"max\": 352.7183532714844}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:12 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1927.3587775736225 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:12 INFO 139919854057280] #progress_metric: host=algo-1, completed 59.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:12 INFO 139919854057280] #quality_metric: host=algo-1, epoch=237, train loss <loss>=4.037872011011297\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:12 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:12 INFO 139919854057280] Epoch[238] Batch[0] avg_epoch_loss=4.494628\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:12 INFO 139919854057280] #quality_metric: host=algo-1, epoch=238, batch=0 train loss <loss>=4.494627952575684\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:12 INFO 139919854057280] Epoch[238] Batch[5] avg_epoch_loss=4.014020\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:12 INFO 139919854057280] #quality_metric: host=algo-1, epoch=238, batch=5 train loss <loss>=4.014019687970479\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:12 INFO 139919854057280] Epoch[238] Batch [5]#011Speed: 2763.36 samples/sec#011loss=4.014020\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:13 INFO 139919854057280] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158912.6979492, \"EndTime\": 1681158913.0594685, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 361.2082004547119, \"count\": 1, \"min\": 361.2082004547119, \"max\": 361.2082004547119}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:13 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1702.1416590011363 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:13 INFO 139919854057280] #progress_metric: host=algo-1, completed 59.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:13 INFO 139919854057280] #quality_metric: host=algo-1, epoch=238, train loss <loss>=3.9089043378829955\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:13 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:13 INFO 139919854057280] Epoch[239] Batch[0] avg_epoch_loss=3.831304\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:13 INFO 139919854057280] #quality_metric: host=algo-1, epoch=239, batch=0 train loss <loss>=3.831303834915161\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:13 INFO 139919854057280] Epoch[239] Batch[5] avg_epoch_loss=3.805494\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:13 INFO 139919854057280] #quality_metric: host=algo-1, epoch=239, batch=5 train loss <loss>=3.8054941097895303\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:13 INFO 139919854057280] Epoch[239] Batch [5]#011Speed: 2365.11 samples/sec#011loss=3.805494\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:13 INFO 139919854057280] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158913.0595405, \"EndTime\": 1681158913.4200108, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 360.06975173950195, \"count\": 1, \"min\": 360.06975173950195, \"max\": 360.06975173950195}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:13 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1751.207947049665 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:13 INFO 139919854057280] #progress_metric: host=algo-1, completed 60.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:13 INFO 139919854057280] #quality_metric: host=algo-1, epoch=239, train loss <loss>=3.8141897678375245\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:13 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:13 INFO 139919854057280] Epoch[240] Batch[0] avg_epoch_loss=3.604888\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:13 INFO 139919854057280] #quality_metric: host=algo-1, epoch=240, batch=0 train loss <loss>=3.6048882007598877\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:13 INFO 139919854057280] Epoch[240] Batch[5] avg_epoch_loss=3.893027\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:13 INFO 139919854057280] #quality_metric: host=algo-1, epoch=240, batch=5 train loss <loss>=3.893027186393738\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:13 INFO 139919854057280] Epoch[240] Batch [5]#011Speed: 2768.96 samples/sec#011loss=3.893027\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:13 INFO 139919854057280] Epoch[240] Batch[10] avg_epoch_loss=3.979645\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:13 INFO 139919854057280] #quality_metric: host=algo-1, epoch=240, batch=10 train loss <loss>=4.083587408065796\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:13 INFO 139919854057280] Epoch[240] Batch [10]#011Speed: 2546.56 samples/sec#011loss=4.083587\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:13 INFO 139919854057280] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158913.420231, \"EndTime\": 1681158913.8020647, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 381.40249252319336, \"count\": 1, \"min\": 381.40249252319336, \"max\": 381.40249252319336}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:13 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1740.453412391699 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:13 INFO 139919854057280] #progress_metric: host=algo-1, completed 60.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:13 INFO 139919854057280] #quality_metric: host=algo-1, epoch=240, train loss <loss>=3.979645468971946\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:13 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:13 INFO 139919854057280] Epoch[241] Batch[0] avg_epoch_loss=3.956539\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:13 INFO 139919854057280] #quality_metric: host=algo-1, epoch=241, batch=0 train loss <loss>=3.9565393924713135\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:14 INFO 139919854057280] Epoch[241] Batch[5] avg_epoch_loss=3.861982\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:14 INFO 139919854057280] #quality_metric: host=algo-1, epoch=241, batch=5 train loss <loss>=3.861981987953186\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:14 INFO 139919854057280] Epoch[241] Batch [5]#011Speed: 2878.97 samples/sec#011loss=3.861982\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:14 INFO 139919854057280] Epoch[241] Batch[10] avg_epoch_loss=3.919147\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:14 INFO 139919854057280] #quality_metric: host=algo-1, epoch=241, batch=10 train loss <loss>=3.9877460956573487\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:14 INFO 139919854057280] Epoch[241] Batch [10]#011Speed: 2586.88 samples/sec#011loss=3.987746\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:14 INFO 139919854057280] processed a total of 697 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158913.8021388, \"EndTime\": 1681158914.1714034, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 368.8089847564697, \"count\": 1, \"min\": 368.8089847564697, \"max\": 368.8089847564697}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:14 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1889.3456625761637 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:14 INFO 139919854057280] #progress_metric: host=algo-1, completed 60.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:14 INFO 139919854057280] #quality_metric: host=algo-1, epoch=241, train loss <loss>=3.919147491455078\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:14 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:14 INFO 139919854057280] Epoch[242] Batch[0] avg_epoch_loss=3.710442\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:14 INFO 139919854057280] #quality_metric: host=algo-1, epoch=242, batch=0 train loss <loss>=3.710442066192627\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:14 INFO 139919854057280] Epoch[242] Batch[5] avg_epoch_loss=3.796867\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:14 INFO 139919854057280] #quality_metric: host=algo-1, epoch=242, batch=5 train loss <loss>=3.7968674500783286\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:14 INFO 139919854057280] Epoch[242] Batch [5]#011Speed: 2771.02 samples/sec#011loss=3.796867\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:14 INFO 139919854057280] Epoch[242] Batch[10] avg_epoch_loss=3.747916\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:14 INFO 139919854057280] #quality_metric: host=algo-1, epoch=242, batch=10 train loss <loss>=3.68917498588562\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:14 INFO 139919854057280] Epoch[242] Batch [10]#011Speed: 2719.46 samples/sec#011loss=3.689175\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:14 INFO 139919854057280] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158914.1714706, \"EndTime\": 1681158914.5391722, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 367.3088550567627, \"count\": 1, \"min\": 367.3088550567627, \"max\": 367.3088550567627}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:14 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1793.5867603849788 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:14 INFO 139919854057280] #progress_metric: host=algo-1, completed 60.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:14 INFO 139919854057280] #quality_metric: host=algo-1, epoch=242, train loss <loss>=3.747916329990734\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:14 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:14 INFO 139919854057280] Epoch[243] Batch[0] avg_epoch_loss=4.262271\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:14 INFO 139919854057280] #quality_metric: host=algo-1, epoch=243, batch=0 train loss <loss>=4.262270927429199\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:14 INFO 139919854057280] Epoch[243] Batch[5] avg_epoch_loss=4.226412\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:14 INFO 139919854057280] #quality_metric: host=algo-1, epoch=243, batch=5 train loss <loss>=4.226411779721578\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:14 INFO 139919854057280] Epoch[243] Batch [5]#011Speed: 2681.70 samples/sec#011loss=4.226412\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:14 INFO 139919854057280] Epoch[243] Batch[10] avg_epoch_loss=4.113231\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:14 INFO 139919854057280] #quality_metric: host=algo-1, epoch=243, batch=10 train loss <loss>=3.977414035797119\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:14 INFO 139919854057280] Epoch[243] Batch [10]#011Speed: 2555.60 samples/sec#011loss=3.977414\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:14 INFO 139919854057280] processed a total of 724 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158914.5392516, \"EndTime\": 1681158914.9640763, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 424.3805408477783, \"count\": 1, \"min\": 424.3805408477783, \"max\": 424.3805408477783}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:14 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1705.0073698761505 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:14 INFO 139919854057280] #progress_metric: host=algo-1, completed 61.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:14 INFO 139919854057280] #quality_metric: host=algo-1, epoch=243, train loss <loss>=3.9025068283081055\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:14 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:15 INFO 139919854057280] Epoch[244] Batch[0] avg_epoch_loss=4.103031\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:15 INFO 139919854057280] #quality_metric: host=algo-1, epoch=244, batch=0 train loss <loss>=4.103030681610107\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:15 INFO 139919854057280] Epoch[244] Batch[5] avg_epoch_loss=3.861059\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:15 INFO 139919854057280] #quality_metric: host=algo-1, epoch=244, batch=5 train loss <loss>=3.8610588709513345\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:15 INFO 139919854057280] Epoch[244] Batch [5]#011Speed: 2988.47 samples/sec#011loss=3.861059\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:15 INFO 139919854057280] Epoch[244] Batch[10] avg_epoch_loss=3.904044\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:15 INFO 139919854057280] #quality_metric: host=algo-1, epoch=244, batch=10 train loss <loss>=3.95562686920166\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:15 INFO 139919854057280] Epoch[244] Batch [10]#011Speed: 2735.34 samples/sec#011loss=3.955627\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:15 INFO 139919854057280] processed a total of 685 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158914.9642956, \"EndTime\": 1681158915.3453, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 380.62167167663574, \"count\": 1, \"min\": 380.62167167663574, \"max\": 380.62167167663574}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:15 INFO 139919854057280] #throughput_metric: host=algo-1, train throughput=1799.2320115828254 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:15 INFO 139919854057280] #progress_metric: host=algo-1, completed 61.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:15 INFO 139919854057280] #quality_metric: host=algo-1, epoch=244, train loss <loss>=3.9040443247014824\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:15 INFO 139919854057280] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:15 INFO 139919854057280] Loading parameters from best epoch (204)\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158915.3453681, \"EndTime\": 1681158915.350585, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.deserialize.time\": {\"sum\": 4.855632781982422, \"count\": 1, \"min\": 4.855632781982422, \"max\": 4.855632781982422}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:15 INFO 139919854057280] stopping training now\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:15 INFO 139919854057280] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:15 INFO 139919854057280] Final loss: 3.6327722397717563 (occurred at epoch 204)\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:15 INFO 139919854057280] #quality_metric: host=algo-1, train final_loss <loss>=3.6327722397717563\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:15 INFO 139919854057280] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:15 WARNING 139919854057280] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:15 INFO 139919854057280] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158915.350653, \"EndTime\": 1681158915.3917942, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 40.58551788330078, \"count\": 1, \"min\": 40.58551788330078, \"max\": 40.58551788330078}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:15 INFO 139919854057280] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158915.3918557, \"EndTime\": 1681158915.4073236, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 56.15067481994629, \"count\": 1, \"min\": 56.15067481994629, \"max\": 56.15067481994629}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:15 INFO 139919854057280] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:15 INFO 139919854057280] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158915.4073818, \"EndTime\": 1681158915.4100418, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.serialize.time\": {\"sum\": 2.6259422302246094, \"count\": 1, \"min\": 2.6259422302246094, \"max\": 2.6259422302246094}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:15 INFO 139919854057280] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:15 INFO 139919854057280] #memory_usage::<batchbuffer> = 0.40283203125 mb\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:15 INFO 139919854057280] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158915.4100852, \"EndTime\": 1681158915.412068, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.bind.time\": {\"sum\": 0.031232833862304688, \"count\": 1, \"min\": 0.031232833862304688, \"max\": 0.031232833862304688}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158915.4121177, \"EndTime\": 1681158915.5614822, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.score.time\": {\"sum\": 149.4455337524414, \"count\": 1, \"min\": 149.4455337524414, \"max\": 149.4455337524414}}}\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:15 INFO 139919854057280] #test_score (algo-1, RMSE): 45.477259748704064\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:15 INFO 139919854057280] #test_score (algo-1, mean_absolute_QuantileLoss): 148.26761067708333\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:15 INFO 139919854057280] #test_score (algo-1, mean_wQuantileLoss): 0.0038727882552436247\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:15 INFO 139919854057280] #test_score (algo-1, wQuantileLoss[0.1]): 0.0005557084905735457\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:15 INFO 139919854057280] #test_score (algo-1, wQuantileLoss[0.2]): 0.0018071494411387129\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:15 INFO 139919854057280] #test_score (algo-1, wQuantileLoss[0.3]): 0.00405915952763962\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:15 INFO 139919854057280] #test_score (algo-1, wQuantileLoss[0.4]): 0.005325099262082302\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:15 INFO 139919854057280] #test_score (algo-1, wQuantileLoss[0.5]): 0.0057106179461273755\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:15 INFO 139919854057280] #test_score (algo-1, wQuantileLoss[0.6]): 0.005311554479818122\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:15 INFO 139919854057280] #test_score (algo-1, wQuantileLoss[0.7]): 0.005257059050761253\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:15 INFO 139919854057280] #test_score (algo-1, wQuantileLoss[0.8]): 0.004185679527696757\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:15 INFO 139919854057280] #test_score (algo-1, wQuantileLoss[0.9]): 0.002643066571354933\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:15 INFO 139919854057280] #quality_metric: host=algo-1, test RMSE <loss>=45.477259748704064\u001b[0m\n",
      "\u001b[34m[04/10/2023 20:35:15 INFO 139919854057280] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.0038727882552436247\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681158915.561548, \"EndTime\": 1681158915.566677, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 5.681514739990234, \"count\": 1, \"min\": 5.681514739990234, \"max\": 5.681514739990234}, \"totaltime\": {\"sum\": 91354.74729537964, \"count\": 1, \"min\": 91354.74729537964, \"max\": 91354.74729537964}}}\u001b[0m\n",
      "\n",
      "2023-04-10 20:35:30 Uploading - Uploading generated training model\n",
      "2023-04-10 20:35:30 Completed - Training job completed\n",
      "Training seconds: 207\n",
      "Billable seconds: 207\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(inputs=data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2b37a553-dfaa-4e10-9d83-4f77f2def0b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.serializers import IdentitySerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cd818ebe-fbfa-436d-a6be-32df4677856e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Esta clase permiti recibir \n",
    "class DeepARPredictor(sagemaker.predictor.Predictor):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(\n",
    "            *args,\n",
    "            # serializer=JSONSerializer(),\n",
    "            serializer=IdentitySerializer(content_type=\"application/json\"),\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        ts,\n",
    "        cat=None,\n",
    "        dynamic_feat=None,\n",
    "        num_samples=100,\n",
    "        return_samples=False,\n",
    "        quantiles=[\"0.1\", \"0.5\", \"0.9\"],\n",
    "    ):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "\n",
    "        ts -- `pandas.Series` object, the time series to predict\n",
    "        cat -- integer, the group associated to the time series (default: None)\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        return_samples -- boolean indicating whether to include samples in the response (default: False)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "\n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        prediction_time = ts.index[-1] + ts.index.freq\n",
    "        quantiles = [str(q) for q in quantiles]\n",
    "        req = self.__encode_request(ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, ts.index.freq, prediction_time, return_samples)\n",
    "\n",
    "    def __encode_request(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles):\n",
    "        instance = series_to_dict(\n",
    "            ts, cat if cat is not None else None, dynamic_feat if dynamic_feat else None\n",
    "        )\n",
    "\n",
    "        configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\", \"samples\"] if return_samples else [\"quantiles\"],\n",
    "            \"quantiles\": quantiles,\n",
    "        }\n",
    "\n",
    "        http_request_data = {\"instances\": [instance], \"configuration\": configuration}\n",
    "\n",
    "        return json.dumps(http_request_data).encode(\"utf-8\")\n",
    "\n",
    "    def __decode_response(self, response, freq, prediction_time, return_samples):\n",
    "        # we only sent one time series so we only receive one in return\n",
    "        # however, if possible one will pass multiple time series as predictions will then be faster\n",
    "        predictions = json.loads(response.decode(\"utf-8\"))[\"predictions\"][0]\n",
    "        prediction_length = len(next(iter(predictions[\"quantiles\"].values())))\n",
    "        prediction_index = pd.date_range(\n",
    "            start=prediction_time, freq=freq, periods=prediction_length\n",
    "        )\n",
    "        if return_samples:\n",
    "            dict_of_samples = {\"sample_\" + str(i): s for i, s in enumerate(predictions[\"samples\"])}\n",
    "        else:\n",
    "            dict_of_samples = {}\n",
    "        return pd.DataFrame(\n",
    "            data={**predictions[\"quantiles\"], **dict_of_samples}, index=prediction_index\n",
    "        )\n",
    "\n",
    "    def set_frequency(self, freq):\n",
    "        self.freq = freq\n",
    "\n",
    "\n",
    "def encode_target(ts):\n",
    "    return [x if np.isfinite(x) else \"NaN\" for x in ts]\n",
    "\n",
    "\n",
    "def series_to_dict(ts, cat=None, dynamic_feat=None):\n",
    "    \"\"\"Given a pandas.Series object, returns a dictionary encoding the time series.\n",
    "\n",
    "    ts -- a pands.Series object with the target time series\n",
    "    cat -- an integer indicating the time series category\n",
    "\n",
    "    Return value: a dictionary\n",
    "    \"\"\"\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": encode_target(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    if dynamic_feat is not None:\n",
    "        obj[\"dynamic_feat\"] = dynamic_feat\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "27b1f444-7f9f-4721-9d26-7db9088a9d70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: dollar-predictor-2023-04-10-20-35-49-202\n",
      "INFO:sagemaker:Creating endpoint-config with name dollar-predictor-2023-04-10-20-35-49-202\n",
      "INFO:sagemaker:Creating endpoint with name dollar-predictor-2023-04-10-20-35-49-202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------!"
     ]
    }
   ],
   "source": [
    "# 2) Desplegar en Sagemaker un modelo que prediga el precio del dólar(\n",
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1, instance_type=\"ml.m4.xlarge\", predictor_cls=DeepARPredictor\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
